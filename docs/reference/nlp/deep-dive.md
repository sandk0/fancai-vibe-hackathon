# NLP Technical Deep-Dive: Advanced Algorithmic Analysis

**–î–∞—Ç–∞:** 2025-11-05
**–ê–≤—Ç–æ—Ä:** Claude Code
**–í–µ—Ä—Å–∏—è:** 1.0 - Comprehensive Technical Analysis
**–¶–µ–ª—å:** –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ advanced –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

---

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –°–∏—Å—Ç–µ–º—ã](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-—Å–∏—Å—Ç–µ–º—ã)
2. [–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑ –¢–µ–∫—É—â–µ–π –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏](#–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π-–∞–Ω–∞–ª–∏–∑)
3. [–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑](#–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π-–∞–Ω–∞–ª–∏–∑)
4. [–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ú–æ–¥–µ–ª–∏](#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ-–º–æ–¥–µ–ª–∏)
5. [Advanced NLP –¢–µ—Ö–Ω–∏–∫–∏](#advanced-nlp-—Ç–µ—Ö–Ω–∏–∫–∏)
6. [–ì—Ä–∞—Ñ-–ê–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –û–ø–∏—Å–∞–Ω–∏–π](#–≥—Ä–∞—Ñ-–∞–ª–≥–æ—Ä–∏—Ç–º—ã)
7. [Machine Learning –ü–æ–¥—Ö–æ–¥—ã](#machine-learning)
8. [Implementation Roadmap](#implementation-roadmap)

---

## üèóÔ∏è –ß–ê–°–¢–¨ 1: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –°–∏—Å—Ç–µ–º—ã

### 1.1 –û–±–∑–æ—Ä –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BOOKREADER AI NLP SYSTEM                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 1: FILE PARSING (book_parser.py - 835 lines)            ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  ‚Ä¢ BookParser (main coordinator)                                ‚îÇ
‚îÇ  ‚Ä¢ EPUBParser (EPUB files)                                      ‚îÇ
‚îÇ  ‚Ä¢ FB2Parser (FB2 files)                                        ‚îÇ
‚îÇ  ‚Ä¢ ChapterNumberExtractor (chapter detection)                   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Input:  EPUB/FB2 file                                          ‚îÇ
‚îÇ  Output: ParsedBook { metadata, chapters[] }                    ‚îÇ
‚îÇ          BookChapter { number, title, content, html_content }   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 2: NLP PROCESSING (multiple files)                      ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  2.1 BASIC NLP (nlp_processor.py - 572 lines)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ BaseNLPProcessor                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ SpacyProcessor (ru_core_news_lg)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ NatashaProcessor (Russian NER)                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ NLPProcessor (coordinator)                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                         ‚îÇ                                        ‚îÇ
‚îÇ                         ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  2.2 ENHANCED PROCESSORS (3 files, ~1749 lines total)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ EnhancedSpacyProcessor (692 lines)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ EnhancedNatashaProcessor (516 lines)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ EnhancedStanzaProcessor (541 lines)                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                         ‚îÇ                                        ‚îÇ
‚îÇ                         ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  2.3 MULTI-NLP MANAGER (280 lines)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ProcessorRegistry                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ConfigLoader                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ EnsembleVoter                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ProcessingMode: SINGLE/PARALLEL/ENSEMBLE/ADAPTIVE    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Input:  Chapter.content (text)                                 ‚îÇ
‚îÇ  Output: List[Description] with confidence scores               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 3: DATABASE STORAGE (models)                             ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  ‚Ä¢ Book model                                                    ‚îÇ
‚îÇ  ‚Ä¢ Chapter model (117 lines)                                    ‚îÇ
‚îÇ  ‚Ä¢ Description model (181 lines)                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Data Flow

```
1. USER UPLOADS FILE
   ‚îÇ
   ‚ñº
2. BookParser.parse_book()
   ‚îú‚îÄ‚ñ∫ EPUBParser.parse() OR FB2Parser.parse()
   ‚îÇ   ‚îú‚îÄ‚ñ∫ _extract_metadata()
   ‚îÇ   ‚îî‚îÄ‚ñ∫ _extract_chapters()
   ‚îÇ       ‚îú‚îÄ‚ñ∫ _extract_chapters_from_toc() [preferred]
   ‚îÇ       ‚îî‚îÄ‚ñ∫ _extract_chapters_from_spine() [fallback]
   ‚îÇ
   ‚ñº
3. ParsedBook { metadata, chapters[] } ‚Üí Database
   ‚îÇ
   ‚ñº
4. FOR EACH Chapter:
   ‚îÇ
   ‚îú‚îÄ‚ñ∫ NLPProcessor.extract_descriptions()
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ‚ñ∫ BASIC MODE (nlp_processor.py)
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ SpacyProcessor.extract_descriptions()
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ _analyze_sentence_spacy()
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ NER extraction
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Pattern matching
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ _filter_and_prioritize()
   ‚îÇ   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ NatashaProcessor.extract_descriptions()
   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ _analyze_sentence_natasha()
   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚ñ∫ _filter_and_prioritize()
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îî‚îÄ‚ñ∫ ENHANCED MODE (multi_nlp_manager.py)
   ‚îÇ       ‚îú‚îÄ‚ñ∫ ProcessorRegistry.get_processor()
   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ EnhancedSpacyProcessor
   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ EnhancedNatashaProcessor
   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚ñ∫ EnhancedStanzaProcessor
   ‚îÇ       ‚îÇ
   ‚îÇ       ‚îú‚îÄ‚ñ∫ ProcessingMode selection
   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ SINGLE: One processor
   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ PARALLEL: All processors concurrently
   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ SEQUENTIAL: One after another
   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ ENSEMBLE: Voting algorithm
   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚ñ∫ ADAPTIVE: Auto-select based on text
   ‚îÇ       ‚îÇ
   ‚îÇ       ‚îî‚îÄ‚ñ∫ EnsembleVoter.vote() [if ENSEMBLE mode]
   ‚îÇ           ‚îú‚îÄ‚ñ∫ Weighted consensus
   ‚îÇ           ‚îú‚îÄ‚ñ∫ Deduplication
   ‚îÇ           ‚îî‚îÄ‚ñ∫ Context enrichment
   ‚îÇ
   ‚ñº
5. List[Description] ‚Üí Database (descriptions table)
```

### 1.3 Critical Observations

#### ‚úÖ –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:

1. **Modular Design:**
   - –ß–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ layers (parsing ‚Üí NLP ‚Üí storage)
   - Pluggable processors (SpaCy, Natasha, Stanza)
   - Easy to extend —Å –Ω–æ–≤—ã–º–∏ processors

2. **Robust File Parsing:**
   - –ù–∞–¥–µ–∂–Ω—ã–π EPUB parser —Å TOC support
   - FB2 parser —Å XML handling
   - Graceful degradation (TOC ‚Üí spine fallback)

3. **Multi-NLP Ensemble:**
   - 3 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ strengths
   - Voting mechanism –¥–ª—è consensus
   - Configurable weights

#### ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:

1. **SENTENCE-LEVEL PROCESSING** (—Ñ–∞—Ç–∞–ª—å–Ω–æ!)
   ```python
   # enhanced_nlp_system.py:350
   for sent in doc.sents:
       description = sent.text.strip()
       # ‚Üê –ü–†–û–ë–õ–ï–ú–ê: –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å 5-20 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π!
   ```

2. **NO PARAGRAPH AWARENESS**
   - –ü–∞—Ä—Å–µ—Ä—ã —Ä–∞–∑–±–∏–≤–∞—é—Ç –Ω–∞ sentences, –Ω–µ paragraphs
   - –ù–µ—Ç –ø–æ–Ω—è—Ç–∏—è "–∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è"
   - Result: —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π

3. **SHALLOW PATTERN MATCHING**
   ```python
   # nlp_processor.py:216-220
   location_patterns = [
       r"(?:–≤|–Ω–∞|–æ–∫–æ–ª–æ)\s+([^,.!?]{10,100})",  # –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–æ–π!
   ]
   ```

4. **NO CONTEXT TRACKING**
   - –ö–∞–∂–¥–∞—è –≥–ª–∞–≤–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
   - –ù–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π/–ª–æ–∫–∞—Ü–∏–π
   - –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è

5. **NAIVE CONFIDENCE SCORING**
   ```python
   # nlp_processor.py:177-182
   if ent.label_ in ["LOC", "GPE", "FAC"]:
       confidence = 0.8  # Hardcoded!
   elif ent.label_ in ["PERSON"]:
       confidence = 0.7
   ```
   - –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
   - –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
   - –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ—Å—Ç—å

---

## üî¨ –ß–ê–°–¢–¨ 2: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑ –¢–µ–∫—É—â–µ–π –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### 2.1 Sentence-Level Processing Problem

#### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (–ü–õ–û–•–û):

**File: `nlp_processor.py:126-159`**
```python
def extract_descriptions(self, text: str, chapter_id: str = None) -> List[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è spaCy."""

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    cleaned_text = self._clean_text(text)

    # –ü–†–û–ë–õ–ï–ú–ê #1: –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø
    doc = self.nlp(cleaned_text)
    sentences = [
        sent.text.strip()
        for sent in doc.sents
        if len(sent.text.strip()) >= self.min_sentence_length
    ]

    descriptions = []

    # –ü–†–û–ë–õ–ï–ú–ê #2: –ê–Ω–∞–ª–∏–∑ –ö–ê–ñ–î–û–ì–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –û–¢–î–ï–õ–¨–ù–û
    for i, sentence in enumerate(sentences):
        sentence_descriptions = self._analyze_sentence_spacy(
            sentence, i, cleaned_text
        )
        descriptions.extend(sentence_descriptions)

    return descriptions
```

#### –ü–æ—á–µ–º—É —ç—Ç–æ —Ñ–∞—Ç–∞–ª—å–Ω–æ?

**–ü—Ä–∏–º–µ—Ä –∏–∑ "–í–µ–¥—å–º–∞–∫":**
```
–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–ö–°–¢ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –û–î–ù–û –æ–ø–∏—Å–∞–Ω–∏–µ, 347 chars):

"–ó–∞–º–æ–∫ –≤–æ–∑–≤—ã—à–∞–ª—Å—è –Ω–∞ —Å–∫–∞–ª–µ, –µ–≥–æ —Å—Ç–µ–Ω—ã –∏–∑ —Ç–µ–º–Ω–æ–≥–æ –∫–∞–º–Ω—è —É—Ö–æ–¥–∏–ª–∏ –≤ –Ω–µ–±–æ.
–ß–µ—Ç—ã—Ä–µ –±–∞—à–Ω–∏ —Å –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω—ã–º–∏ –∫—Ä—ã—à–∞–º–∏ —Å—Ç–æ—è–ª–∏ –ø–æ —É–≥–ª–∞–º. –ì–ª–∞–≤–Ω—ã–µ –≤–æ—Ä–æ—Ç–∞
–±—ã–ª–∏ –æ—Ç–∫—Ä—ã—Ç—ã, –Ω–∞–¥ –Ω–∏–º–∏ —Ä–∞–∑–≤–µ–≤–∞–ª—Å—è —Ñ–ª–∞–≥ —Å –≥–µ—Ä–±–æ–º. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–≤–æ—Ä –±—ã–ª
–≤—ã–º–æ—â–µ–Ω —Å–µ—Ä—ã–º –±—É–ª—ã–∂–Ω–∏–∫–æ–º."

–¢–ï–ö–£–©–ê–Ø –°–ò–°–¢–ï–ú–ê –∏–∑–≤–ª–µ–∫–∞–µ—Ç:

1. "–ó–∞–º–æ–∫ –≤–æ–∑–≤—ã—à–∞–ª—Å—è –Ω–∞ —Å–∫–∞–ª–µ" (31 chars) ‚Üê fragment
2. "–µ–≥–æ —Å—Ç–µ–Ω—ã –∏–∑ —Ç–µ–º–Ω–æ–≥–æ –∫–∞–º–Ω—è" (26 chars) ‚Üê fragment
3. "–ß–µ—Ç—ã—Ä–µ –±–∞—à–Ω–∏ —Å –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω—ã–º–∏ –∫—Ä—ã—à–∞–º–∏" (40 chars) ‚Üê fragment
4. "–ì–ª–∞–≤–Ω—ã–µ –≤–æ—Ä–æ—Ç–∞ –±—ã–ª–∏ –æ—Ç–∫—Ä—ã—Ç—ã" (30 chars) ‚Üê fragment
5. "–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–≤–æ—Ä –±—ã–ª –≤—ã–º–æ—â–µ–Ω" (29 chars) ‚Üê fragment

–†–ï–ó–£–õ–¨–¢–ê–¢: 5 –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ 1 –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è!
```

### 2.2 Pattern Matching Analysis

#### –¢–µ–∫—É—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–ü–†–ò–ú–ò–¢–ò–í–ù–´–ï):

**File: `nlp_processor.py:209-241`**
```python
location_patterns = [
    r"(?:–≤|–Ω–∞|–æ–∫–æ–ª–æ|–≤–æ–∑–ª–µ|—Ä—è–¥–æ–º —Å|–ø–µ—Ä–µ–¥|–∑–∞|–Ω–∞–¥|–ø–æ–¥)\s+([^,.!?]{10,100})",
    r"([^,.!?]{5,50})\s+(?:—Å—Ç–æ—è–ª|—Å—Ç–æ—è–ª–∞|—Å—Ç–æ—è–ª–æ|–Ω–∞—Ö–æ–¥–∏–ª—Å—è|–Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å|–Ω–∞—Ö–æ–¥–∏–ª–æ—Å—å)",
    r"(?:–¥–æ–º|–∑–¥–∞–Ω–∏–µ|–∑–∞–º–æ–∫|—Ö—Ä–∞–º|–¥–≤–æ—Ä–µ—Ü|–±–∞—à–Ω—è|–º–æ—Å—Ç|–ª–µ—Å|–ø–æ–ª–µ|–≥–æ—Ä—ã?|—Ä–µ–∫–∞|–º–æ—Ä–µ|–æ–∑–µ—Ä–æ)\s+([^,.!?]{10,100})",
]

character_patterns = [
    r"(?:–æ–Ω|–æ–Ω–∞|–æ–Ω–æ|–æ–Ω–∏)\s+(?:–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏)\s+([^,.!?]{10,100})",
    r"(?:–º—É–∂—á–∏–Ω–∞|–∂–µ–Ω—â–∏–Ω–∞|–¥–µ–≤—É—à–∫–∞|–ø–∞—Ä–µ–Ω—å|—Å—Ç–∞—Ä–∏–∫|—Å—Ç–∞—Ä—É—Ö–∞)\s+([^,.!?]{10,100})",
]

atmosphere_patterns = [
    r"(?:–±—ã–ª–æ|—Å—Ç–∞–ª–æ)\s+(?:—Ç–µ–º–Ω–æ|—Å–≤–µ—Ç–ª–æ|—Ö–æ–ª–æ–¥–Ω–æ|–∂–∞—Ä–∫–æ|—Ç–∏—Ö–æ|—à—É–º–Ω–æ|—Ç—É–º–∞–Ω–Ω–æ|—è—Å–Ω–æ)\s*([^,.!?]{0,50})",
    r"(?:–Ω–∞—Å—Ç—É–ø–∏–ª|–Ω–∞—Å—Ç—É–ø–∏–ª–∞|–Ω–∞—Å—Ç—É–ø–∏–ª–æ)\s+(?:–≤–µ—á–µ—Ä|—É—Ç—Ä–æ|–Ω–æ—á—å|–¥–µ–Ω—å|—Ä–∞—Å—Å–≤–µ—Ç|–∑–∞–∫–∞—Ç)\s*([^,.!?]{0,50})",
]
```

#### –ü—Ä–æ–±–ª–µ–º—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:

1. **–°–ª–∏—à–∫–æ–º —É–∑–∫–∏–µ:**
   - –õ–æ–≤—è—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
   - –ü—Ä–æ–ø—É—Å–∫–∞—é—Ç —Å–ª–æ–∂–Ω—ã–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
   - –ù–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –ø—Ä–∏—á–∞—Å—Ç–Ω—ã–º–∏ –æ–±–æ—Ä–æ—Ç–∞–º–∏

2. **–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è:**
   - Hardcoded keywords
   - –ù–µ—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
   - –ù–µ–ª—å–∑—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∂–∞–Ω—Ä

3. **–ò–≥–Ω–æ—Ä–∏—Ä—É—é—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å:**
   - –ù–µ —É—á–∏—Ç—ã–≤–∞—é—Ç dependency parsing
   - –ù–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
   - –ù–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç POS sequences

### 2.3 Confidence Scoring Analysis

#### –¢–µ–∫—É—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (–ù–ê–ò–í–ù–´–ô):

**File: `enhanced_nlp_system.py:400-442`**
```python
def _calculate_general_descriptive_score(self, sent) -> float:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â—É—é –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    """
    # –ü–æ–¥—Å—á—ë—Ç –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
    adj_count = sum(1 for token in sent if token.pos_ == "ADJ")
    noun_count = sum(1 for token in sent if token.pos_ == "NOUN")

    if adj_count > 0 and noun_count > 0:
        # –ü–†–û–ë–õ–ï–ú–ê: –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞
        adj_ratio = adj_count / (noun_count + adj_count)
        score = 0.5 + adj_ratio * 0.3  # ‚Üê –í—Å–µ –ø–æ–ª—É—á–∞—é—Ç 0.5-0.8!

        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if any(kw in sent.text.lower() for kw in ["–∫—Ä–∞—Å–∏–≤—ã–π", "–≤–µ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π"]):
            score += 0.1

        # –®—Ç—Ä–∞—Ñ—ã
        if any(kw in sent.text.lower() for kw in ["—Å–∫–∞–∑–∞–ª", "–ø–æ–¥—É–º–∞–ª"]):
            score -= 0.2

        return max(0.0, min(1.0, score))

    return 0.3  # Default –¥–ª—è –≤—Å–µ–≥–æ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ
```

#### –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:

1. **–ù–µ—Ç discrimination:**
   - –ü–æ—á—Ç–∏ –≤—Å—ë –ø–æ–ª—É—á–∞–µ—Ç 0.5-0.8
   - –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≥–ª–∞–≤: 0.9 (!)
   - –†–µ–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è: 0.3-0.4
   - **–°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ù–ê–û–ë–û–†–û–¢!**

2. **–ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç:**
   - –í–∏–∑—É–∞–ª—å–Ω—É—é –ª–µ–∫—Å–∏–∫—É (—Ü–≤–µ—Ç–∞, —Ä–∞–∑–º–µ—Ä—ã, —Ñ–æ—Ä–º—ã)
   - –°–µ–Ω—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ (–∑–∞–ø–∞—Ö–∏, –∑–≤—É–∫–∏, —Ç–µ–∫—Å—Ç—É—Ä—ã)
   - –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–≥–∏ (–Ω–∞–¥, –ø–æ–¥, –º–µ–∂–¥—É)
   - –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å
   - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å

3. **Hardcoded thresholds:**
   - –ù–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–æ–¥ –∂–∞–Ω—Ä
   - –ù–µ—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–∞ feedback
   - –ù–µ—Ç calibration

### 2.4 Enhanced Processors Analysis

#### EnhancedSpacyProcessor (692 lines)

**Strengths:**
- Multiple extraction strategies:
  - `_extract_entity_descriptions()` - NER-based
  - `_extract_pattern_descriptions()` - Pattern-based
  - `_extract_contextual_descriptions()` - Context-based
  - `_extract_fallback_descriptions()` - ADJ+NOUN fallback

**Weaknesses:**
```python
# enhanced_nlp_system.py:350-370
def _extract_entity_descriptions(self, doc):
    descriptions = []
    for sent in doc.sents:  # ‚Üê SENTENCE-LEVEL!
        # Extract entities from SINGLE sentence
        for ent in sent.ents:
            # Create description from entity
            description = {
                "content": sent.text,  # ‚Üê –¢–æ–ª—å–∫–æ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ!
                "type": self._guess_type(ent),
                "confidence_score": 0.5,
            }
            descriptions.append(description)
    return descriptions
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –í—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ sentence-level!

#### EnhancedNatashaProcessor (516 lines)

**Strengths:**
- Russian-specific patterns
- Yargy-parser integration
- Morphological analysis

**Weaknesses:**
```python
# natasha_processor.py:60-72
"person_patterns": [
    r"\b(?:—é–Ω–æ—à–∞|–¥–µ–≤—É—à–∫–∞|—Å—Ç–∞—Ä–∏–∫|–∂–µ–Ω—â–∏–Ω–∞|–º—É–∂—á–∏–Ω–∞|—Ä–µ–±—ë–Ω–æ–∫|–¥–∏—Ç—è)\b",
],
"location_patterns": [
    r"\b(?:–¥–≤–æ—Ä–µ—Ü|–∑–∞–º–æ–∫|–∫—Ä–µ–ø–æ—Å—Ç—å|—Ç–µ—Ä–µ–º|—Ö–∏–∂–∏–Ω–∞|–∏–∑–±–∞)\b",
],
```

- –°–ª–∏—à–∫–æ–º —É–∑–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –ù–µ –ø–æ–∫—Ä—ã–≤–∞—é—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
- –ù–µ –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç multi-sentence descriptions

#### EnhancedStanzaProcessor (541 lines)

**Strengths:**
- Deep dependency parsing
- Universal Dependencies
- Participial constructions handling

**Weaknesses:**
```python
# stanza_processor.py:150-180
def _extract_dependency_descriptions(self, doc):
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç dependency relations
    for sent in doc.sentences:  # ‚Üê SENTENCE-LEVEL AGAIN!
        for word in sent.words:
            if word.deprel in ["amod", "nmod"]:
                # Extract based on dependency
```

- –¢–∞–∫–∂–µ sentence-level
- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω—É—é —Å–∏–ª—É dependency parsing –¥–ª—è multi-sentence grouping

### 2.5 Multi-NLP Manager Analysis

#### Ensemble Voting Algorithm

**File: `multi_nlp_manager.py:150-220`**
```python
class EnsembleVoter:
    def vote(self, results: List[ProcessingResult]) -> ProcessingResult:
        """
        Ensemble voting —Å weighted consensus.
        """
        # Weights –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        weights = {
            "spacy": 1.0,
            "natasha": 1.2,  # –í—ã—à–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
            "stanza": 0.8,
        }

        # Group similar descriptions
        groups = self._group_similar_descriptions(all_descriptions)

        # Weighted voting
        voted_descriptions = []
        for group in groups:
            # Calculate consensus score
            total_weight = sum(weights[d.source] for d in group)
            consensus_score = total_weight / len(results)

            if consensus_score >= 0.6:  # Threshold
                # Take best description from group
                best = max(group, key=lambda d: d.confidence)
                voted_descriptions.append(best)

        return voted_descriptions
```

**Strengths:**
- Weighted consensus
- Deduplication
- Multiple processors combine strengths

**Weaknesses:**
- `_group_similar_descriptions()` - –∫–∞–∫ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å?
- Similarity metric - –∫–∞–∫–æ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?
- Threshold 0.6 - –æ—Ç–∫—É–¥–∞ –≤–∑—è–ª—Å—è?
- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –¥–ª–∏–Ω—É/–ø–æ–ª–Ω–æ—Ç—É –æ–ø–∏—Å–∞–Ω–∏–π

---

## üßÆ –ß–ê–°–¢–¨ 3: –ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑

### 3.1 –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ê–ª–≥–æ—Ä–∏—Ç–º—ã

–î–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π (500-3500 chars) –Ω—É–∂–Ω—ã:

#### A. **Paragraph Segmentation Algorithm**
- **–í—Ö–æ–¥:** Chapter.content (plain text)
- **–í—ã—Ö–æ–¥:** List[Paragraph] —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** O(n) –≥–¥–µ n = length(text)

#### B. **Description Boundary Detection Algorithm**
- **–í—Ö–æ–¥:** List[Paragraph]
- **–í—ã—Ö–æ–¥:** List[CompleteDescription] (multi-paragraph)
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** O(n * m) –≥–¥–µ m = max paragraphs in description

#### C. **Multi-Feature Confidence Scoring Algorithm**
- **–í—Ö–æ–¥:** CompleteDescription + NLP analysis
- **–í—ã—Ö–æ–¥:** Confidence score [0.0, 1.0]
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** O(k) –≥–¥–µ k = number of features

#### D. **Cross-Chapter Context Tracking Algorithm**
- **–í—Ö–æ–¥:** List[Description] across chapters
- **–í—ã—Ö–æ–¥:** Entity registry + context enrichment
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** O(n log n) —Å indexing

#### E. **Type Classification Algorithm**
- **–í—Ö–æ–¥:** CompleteDescription + visual/semantic features
- **–í—ã—Ö–æ–¥:** DescriptionType + confidence
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** O(k) –≥–¥–µ k = number of classifiers

### 3.2 Algorithm A: Paragraph Segmentation

#### –ü—Å–µ–≤–¥–æ–∫–æ–¥:

```python
ALGORITHM: ParagraphSegmentation
INPUT: text (string), config (ParagraphSegmentationConfig)
OUTPUT: paragraphs (List[Paragraph])

FUNCTION segment_into_paragraphs(text, config):
    lines = text.split('\n')
    paragraphs = []
    current_paragraph_lines = []

    FOR EACH line IN lines:
        stripped_line = line.strip()

        # Rule 1: Empty line = paragraph boundary
        IF stripped_line == "":
            IF current_paragraph_lines NOT EMPTY:
                paragraph = join(current_paragraph_lines, ' ')
                paragraphs.append(create_paragraph(paragraph))
                current_paragraph_lines = []
            CONTINUE

        # Rule 2: Dialog marker = separate paragraph
        IF is_dialog_start(stripped_line):
            IF current_paragraph_lines NOT EMPTY:
                paragraph = join(current_paragraph_lines, ' ')
                paragraphs.append(create_paragraph(paragraph))
                current_paragraph_lines = []

            paragraphs.append(create_paragraph(stripped_line, type=DIALOG))
            CONTINUE

        # Rule 3: Chapter header = separate paragraph
        IF is_chapter_header(stripped_line):
            IF current_paragraph_lines NOT EMPTY:
                paragraph = join(current_paragraph_lines, ' ')
                paragraphs.append(create_paragraph(paragraph))
                current_paragraph_lines = []

            paragraphs.append(create_paragraph(stripped_line, type=META))
            CONTINUE

        # Rule 4: Accumulate regular lines
        current_paragraph_lines.append(stripped_line)

    # Flush remaining
    IF current_paragraph_lines NOT EMPTY:
        paragraph = join(current_paragraph_lines, ' ')
        paragraphs.append(create_paragraph(paragraph))

    # Classify each paragraph
    FOR EACH paragraph IN paragraphs:
        paragraph.type = classify_paragraph(paragraph.text)
        paragraph.descriptiveness_score = score_descriptiveness(paragraph.text)

    RETURN paragraphs

FUNCTION classify_paragraph(text):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ: DESCRIPTION, NARRATIVE, DIALOG, META
    """
    # Fast classification based on patterns
    IF starts_with_dialog_marker(text):
        RETURN DIALOG

    IF is_chapter_header(text):
        RETURN META

    IF is_epigraph(text):
        RETURN META

    # Compute scores
    desc_score = score_descriptiveness(text)
    narr_score = score_narrativeness(text)

    IF desc_score > narr_score + 0.2:
        RETURN DESCRIPTION
    ELSE IF narr_score > desc_score + 0.2:
        RETURN NARRATIVE
    ELSE:
        RETURN MIXED

FUNCTION score_descriptiveness(text):
    """
    –û—Ü–µ–Ω–∫–∞ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞.
    """
    doc = nlp(text)

    # Feature 1: ADJ/NOUN ratio
    adj_count = count_pos(doc, "ADJ")
    noun_count = count_pos(doc, "NOUN")
    adj_ratio = adj_count / (noun_count + 1) –µ—Å–ª–∏ noun_count > 0 else 0
    score = min(0.3, adj_ratio * 0.6)

    # Feature 2: Visual vocabulary density
    visual_words = count_visual_words(text)
    score += min(0.25, visual_words / word_count(text) * 5)

    # Feature 3: Descriptive verbs
    descriptive_verbs = ["–±—ã–ª", "–∫–∞–∑–∞–ª—Å—è", "–≤—ã–≥–ª—è–¥–µ–ª", "–Ω–∞–ø–æ–º–∏–Ω–∞–ª"]
    verb_count = sum(1 for v in descriptive_verbs if v in text.lower())
    score += min(0.2, verb_count * 0.05)

    # Feature 4: Spatial prepositions
    spatial_preps = ["–Ω–∞–¥", "–ø–æ–¥", "–≤–æ–∫—Ä—É–≥", "–º–µ–∂–¥—É", "—Ä—è–¥–æ–º"]
    prep_count = sum(1 for p in spatial_preps if f" {p} " in text.lower())
    score += min(0.15, prep_count * 0.03)

    # Feature 5: Penalty for action verbs
    action_verbs = ["–ø–æ—à–µ–ª", "–ø–æ–±–µ–∂–∞–ª", "—Å—Ö–≤–∞—Ç–∏–ª", "–∑–∞–∫—Ä–∏—á–∞–ª"]
    action_count = sum(1 for v in action_verbs if v in text.lower())
    score -= min(0.2, action_count * 0.05)

    RETURN clamp(score, 0.0, 1.0)

FUNCTION score_narrativeness(text):
    """
    –û—Ü–µ–Ω–∫–∞ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏).
    """
    score = 0.0

    # Feature 1: Action verbs
    action_verbs = ["—Å–∫–∞–∑–∞–ª", "–ø–æ—à–µ–ª", "–≤–∑—è–ª", "–ø–æ—Å–º–æ—Ç—Ä–µ–ª", "–ø–æ–¥—É–º–∞–ª"]
    action_count = sum(1 for v in action_verbs if v in text.lower())
    score += min(0.4, action_count * 0.08)

    # Feature 2: Temporal markers
    temporal_markers = ["–∑–∞—Ç–µ–º", "–ø–æ—Ç–æ–º", "–≤–¥—Ä—É–≥", "–≤–Ω–µ–∑–∞–ø–Ω–æ", "—Å–ø—É—Å—Ç—è"]
    temporal_count = sum(1 for m in temporal_markers if m in text.lower())
    score += min(0.3, temporal_count * 0.1)

    # Feature 3: Verb/Adjective ratio (narratives have more verbs)
    doc = nlp(text)
    verb_count = count_pos(doc, "VERB")
    adj_count = count_pos(doc, "ADJ")
    verb_adj_ratio = verb_count / (adj_count + 1) –µ—Å–ª–∏ adj_count > 0 else 0
    IF verb_adj_ratio > 2.0:
        score += 0.3

    RETURN clamp(score, 0.0, 1.0)
```

**Complexity Analysis:**
- –í—Ä–µ–º—è: O(n) –≥–¥–µ n = length(text)
  - Split by lines: O(n)
  - Classify each paragraph: O(p) –≥–¥–µ p = number of paragraphs
  - NLP processing per paragraph: O(p * avg_paragraph_length)
  - **Total: O(n)** (linear –≤ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞)

- –ü–∞–º—è—Ç—å: O(p) –≥–¥–µ p = number of paragraphs
  - Store paragraphs: O(p)
  - NLP doc objects: O(p)

### 3.3 Algorithm B: Description Boundary Detection

–≠—Ç–æ **–∫–ª—é—á–µ–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º** –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ –¥–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è.

#### Mathematical Foundation

–ó–∞–¥–∞—á–∞: **–ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é** –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤.

**–î–∞–Ω–æ:**
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: P = [p‚ÇÅ, p‚ÇÇ, ..., p‚Çô]
- –ö–∞–∂–¥—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ p·µ¢ –∏–º–µ–µ—Ç:
  - `descriptiveness_score(p·µ¢)` ‚àà [0, 1]
  - `type(p·µ¢)` ‚àà {DESCRIPTION, NARRATIVE, DIALOG, META}
  - `length(p·µ¢)` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤

**–ù–∞–π—Ç–∏:**
- –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é S = {D‚ÇÅ, D‚ÇÇ, ..., D‚Çñ} –≥–¥–µ –∫–∞–∂–¥—ã–π D·µ¢ = [p‚±º, p‚±º‚Çä‚ÇÅ, ..., p‚±º‚Çä‚Çò]
- D·µ¢ - —ç—Ç–æ **CompleteDescription** (group of consecutive paragraphs)

**Constraints:**
1. ‚àÄD·µ¢: 500 ‚â§ length(D·µ¢) ‚â§ 4000 chars
2. ‚àÄD·µ¢: avg(descriptiveness_score(p‚±º ‚àà D·µ¢)) ‚â• 0.5
3. ‚àÄD·µ¢: type(p‚ÇÅ) ‚àà {DESCRIPTION, MIXED} (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞)

**Objective:**
–ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å: Œ£ quality_score(D·µ¢) –≥–¥–µ quality_score —É—á–∏—Ç—ã–≤–∞–µ—Ç:
- –î–ª–∏–Ω—É (prefer longer descriptions)
- Coherence (—Å–≤—è–∑–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤)
- Visual richness (–Ω–∞–ª–∏—á–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏)

#### Algorithm: Dynamic Programming Approach

```python
ALGORITHM: DescriptionBoundaryDetection
INPUT: paragraphs (List[Paragraph])
OUTPUT: descriptions (List[CompleteDescription])

FUNCTION detect_boundaries(paragraphs):
    n = length(paragraphs)
    descriptions = []
    i = 0

    WHILE i < n:
        paragraph = paragraphs[i]

        # Skip non-descriptive paragraphs
        IF paragraph.type IN [DIALOG, META]:
            i += 1
            CONTINUE

        IF paragraph.descriptiveness_score < 0.4:
            i += 1
            CONTINUE

        # Try to build a description starting from i
        description = extract_complete_description(paragraphs, i)

        IF description IS NOT NULL:
            descriptions.append(description)
            i = description.end_index + 1
        ELSE:
            i += 1

    RETURN descriptions

FUNCTION extract_complete_description(paragraphs, start_idx):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞—á–∏–Ω–∞—è —Å start_idx.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GREEDY ALGORITHM —Å lookahead.
    """
    current_desc_paras = [paragraphs[start_idx]]
    current_length = length(paragraphs[start_idx].text)

    # Lookahead window
    for i in range(start_idx + 1, min(start_idx + 20, len(paragraphs))):
        para = paragraphs[i]

        # Stopping conditions
        IF should_stop(para, current_desc_paras):
            BREAK

        # Length constraint
        IF current_length + length(para.text) > 4000:
            BREAK

        # Continuation signals
        IF should_continue(para, current_desc_paras):
            current_desc_paras.append(para)
            current_length += length(para.text)
        ELSE:
            # Try lookahead (maybe next paragraph continues)
            IF i + 1 < len(paragraphs):
                next_para = paragraphs[i + 1]
                IF has_strong_continuation_signal(next_para, current_desc_paras):
                    # Include both current and next
                    current_desc_paras.append(para)
                    current_desc_paras.append(next_para)
                    current_length += length(para.text) + length(next_para.text)
                    i += 1
                ELSE:
                    BREAK
            ELSE:
                BREAK

    # Validate minimum length
    IF current_length < 500:
        RETURN NULL

    # Create CompleteDescription
    description = CompleteDescription(
        paragraphs=current_desc_paras,
        text=join(p.text for p in current_desc_paras, '\n\n'),
        start_index=start_idx,
        end_index=start_idx + len(current_desc_paras) - 1,
        length=current_length
    )

    # Calculate quality score
    description.quality_score = calculate_quality_score(description)

    RETURN description

FUNCTION should_stop(para, current_description):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∂–µ—Å—Ç–∫–∏–µ stop signals.
    """
    # Stop Signal 1: Dialog
    IF para.type == DIALOG:
        RETURN TRUE

    # Stop Signal 2: Meta text (chapter header, etc.)
    IF para.type == META:
        RETURN TRUE

    # Stop Signal 3: Strong narrative shift
    IF starts_with(para.text, ["–ó–∞—Ç–µ–º", "–ü–æ—Ç–æ–º", "–í–¥—Ä—É–≥", "–û–¥–Ω–∞–∫–æ", "–ù–æ"]):
        RETURN TRUE

    # Stop Signal 4: Action verbs indicating scene change
    action_verbs = ["–ø–æ—à–µ–ª", "–ø–æ–≤–µ—Ä–Ω—É–ª—Å—è", "–±—Ä–æ—Å–∏–ª—Å—è", "–∑–∞–∫—Ä–∏—á–∞–ª"]
    IF any(verb in para.text[:100] for verb in action_verbs):
        RETURN TRUE

    RETURN FALSE

FUNCTION should_continue(para, current_description):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç continuation signals.
    """
    last_para = current_description[-1]

    # Continue Signal 1: Still descriptive
    IF para.descriptiveness_score >= 0.5:
        RETURN TRUE

    # Continue Signal 2: Entity continuity
    entities_prev = extract_entities(last_para.text)
    entities_curr = extract_entities(para.text)
    IF overlap(entities_prev, entities_curr) > 0:
        RETURN TRUE

    # Continue Signal 3: Spatial continuity
    spatial_words = ["–Ω–∞–¥", "–ø–æ–¥", "—Ä—è–¥–æ–º", "–≤–æ–∑–ª–µ", "–≤–Ω—É—Ç—Ä–∏", "—Å–Ω–∞—Ä—É–∂–∏"]
    IF any(word in para.text[:100] for word in spatial_words):
        RETURN TRUE

    # Continue Signal 4: Semantic similarity
    IF cosine_similarity(embed(last_para.text), embed(para.text)) > 0.7:
        RETURN TRUE

    RETURN FALSE

FUNCTION calculate_quality_score(description):
    """
    Multi-factor quality scoring.

    Factors:
    1. Length (30%) - prefer longer descriptions
    2. Coherence (25%) - semantic —Å–≤—è–∑–Ω–æ—Å—Ç—å
    3. Visual richness (25%) - visual vocabulary density
    4. Descriptiveness (20%) - avg descriptiveness_score
    """
    # Factor 1: Length score (prefer 1000-2500 chars)
    length = description.length
    IF 1000 <= length <= 2500:
        length_score = 1.0
    ELSE IF 500 <= length < 1000:
        length_score = 0.6 + (length - 500) / 500 * 0.4
    ELSE IF 2500 < length <= 3500:
        length_score = 0.9 - (length - 2500) / 1000 * 0.2
    ELSE:
        length_score = 0.5

    # Factor 2: Coherence score
    coherence = calculate_semantic_coherence(description.paragraphs)

    # Factor 3: Visual richness
    visual_richness = calculate_visual_richness(description.text)

    # Factor 4: Average descriptiveness
    avg_desc = mean(p.descriptiveness_score for p in description.paragraphs)

    # Weighted sum
    quality = (
        length_score * 0.30 +
        coherence * 0.25 +
        visual_richness * 0.25 +
        avg_desc * 0.20
    )

    RETURN quality

FUNCTION calculate_semantic_coherence(paragraphs):
    """
    –ò–∑–º–µ—Ä—è–µ—Ç semantic coherence –º–µ–∂–¥—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence embeddings –∏ cosine similarity.
    """
    IF len(paragraphs) == 1:
        RETURN 1.0

    embeddings = [embed_text(p.text) for p in paragraphs]

    # Pairwise cosine similarity
    similarities = []
    FOR i in range(len(embeddings) - 1):
        sim = cosine_similarity(embeddings[i], embeddings[i+1])
        similarities.append(sim)

    # Average similarity
    coherence = mean(similarities)

    RETURN coherence

FUNCTION calculate_visual_richness(text):
    """
    –ò–∑–º–µ—Ä—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞.
    """
    words = text.lower().split()
    total_words = len(words)

    # Category 1: Colors (15%)
    colors = ["–±–µ–ª—ã–π", "—á–µ—Ä–Ω—ã–π", "—Å–µ—Ä—ã–π", "–∫—Ä–∞—Å–Ω—ã–π", "—Å–∏–Ω–∏–π", "–∑–µ–ª–µ–Ω—ã–π",
              "–∂–µ–ª—Ç—ã–π", "–∑–æ–ª–æ—Ç–æ–π", "—Å–µ—Ä–µ–±—Ä—è–Ω—ã–π", "—Ç–µ–º–Ω—ã–π", "—Å–≤–µ—Ç–ª—ã–π"]
    color_count = sum(1 for w in words if w in colors)
    color_score = min(0.15, color_count / total_words * 10)

    # Category 2: Sizes (15%)
    sizes = ["–±–æ–ª—å—à–æ–π", "–º–∞–ª–µ–Ω—å–∫–∏–π", "–æ–≥—Ä–æ–º–Ω—ã–π", "–∫—Ä–æ—à–µ—á–Ω—ã–π", "–≤—ã—Å–æ–∫–∏–π",
             "–Ω–∏–∑–∫–∏–π", "—à–∏—Ä–æ–∫–∏–π", "—É–∑–∫–∏–π", "–¥–ª–∏–Ω–Ω—ã–π", "–∫–æ—Ä–æ—Ç–∫–∏–π"]
    size_count = sum(1 for w in words if w in sizes)
    size_score = min(0.15, size_count / total_words * 10)

    # Category 3: Shapes (10%)
    shapes = ["–∫—Ä—É–≥–ª—ã–π", "–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π", "—Ç—Ä–µ—É–≥–æ–ª—å–Ω—ã–π", "–æ—Å—Ç—Ä—ã–π", "—Ç—É–ø–æ–π",
              "–ø—Ä—è–º–æ–π", "–∫—Ä–∏–≤–æ–π", "–∏–∑–æ–≥–Ω—É—Ç—ã–π"]
    shape_count = sum(1 for w in words if w in shapes)
    shape_score = min(0.10, shape_count / total_words * 10)

    # Category 4: Textures (15%)
    textures = ["–≥–ª–∞–¥–∫–∏–π", "—à–µ—Ä—à–∞–≤—ã–π", "–º—è–≥–∫–∏–π", "—Ç–≤–µ—Ä–¥—ã–π", "—Ö–æ–ª–æ–¥–Ω—ã–π",
                "—Ç–µ–ø–ª—ã–π", "–≤–ª–∞–∂–Ω—ã–π", "—Å—É—Ö–æ–π"]
    texture_count = sum(1 for w in words if w in textures)
    texture_score = min(0.15, texture_count / total_words * 10)

    # Category 5: Lighting (20%)
    lighting = ["—Å–≤–µ—Ç", "—Ç–µ–Ω—å", "—Å—É–º—Ä–∞–∫", "—è—Ä–∫–∏–π", "—Ç—É—Å–∫–ª—ã–π", "–æ—Å–≤–µ—â–µ–Ω–Ω—ã–π",
                "—Ç–µ–º–Ω—ã–π", "—Å–æ–ª–Ω—Ü–µ", "–ª—É–Ω–∞", "–∑–≤–µ–∑–¥—ã", "–æ–≥–æ–Ω—å"]
    lighting_count = sum(1 for w in words if w in lighting)
    lighting_score = min(0.20, lighting_count / total_words * 10)

    # Category 6: Materials (15%)
    materials = ["–∫–∞–º–µ–Ω—å", "–¥–µ—Ä–µ–≤–æ", "–º–µ—Ç–∞–ª–ª", "—Ç–∫–∞–Ω—å", "–∫–æ–∂–∞", "—Å—Ç–µ–∫–ª–æ",
                 "–∑–æ–ª–æ—Ç–æ", "—Å–µ—Ä–µ–±—Ä–æ", "–∂–µ–ª–µ–∑–æ", "—Å—Ç–∞–ª—å"]
    material_count = sum(1 for w in words if w in materials)
    material_score = min(0.15, material_count / total_words * 10)

    # Category 7: Architecture (10%)
    architecture = ["–±–∞—à–Ω—è", "—Å—Ç–µ–Ω–∞", "–∫—Ä—ã—à–∞", "–æ–∫–Ω–æ", "–¥–≤–µ—Ä—å", "–≤–æ—Ä–æ—Ç–∞",
                    "–∞—Ä–∫–∞", "–∫–æ–ª–æ–Ω–Ω–∞", "–∫—É–ø–æ–ª"]
    arch_count = sum(1 for w in words if w in architecture)
    arch_score = min(0.10, arch_count / total_words * 10)

    visual_richness = (
        color_score + size_score + shape_score + texture_score +
        lighting_score + material_score + arch_score
    )

    RETURN visual_richness
```

**Complexity Analysis:**
- **Time:** O(n * w) –≥–¥–µ:
  - n = number of paragraphs
  - w = lookahead window size (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞, –æ–±—ã—á–Ω–æ 20)
  - –î–ª—è –∫–∞–∂–¥–æ–≥–æ paragraph: O(w) lookahead
  - **Total: O(n)** (linear)

- **Space:** O(d) –≥–¥–µ d = number of detected descriptions
  - Store descriptions: O(d)
  - Temporary paragraph groups: O(w) (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)

**Optimization:**
- Early stopping –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ length limit
- Lookahead –æ–≥—Ä–∞–Ω–∏—á–µ–Ω –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π (20 paragraphs)
- Semantic similarity –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (lazy)

---

## üî¢ –ß–ê–°–¢–¨ 4: –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ú–æ–¥–µ–ª–∏

### 4.1 Confidence Scoring Model

**–¢–µ–∫—É—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞:** Naive linear model —Å hardcoded weights.

**–†–µ—à–µ–Ω–∏–µ:** Multi-factor ensemble model —Å learned weights.

#### Mathematical Formulation

**–î–∞–Ω–æ:**
- Description D —Å text t
- NLP analysis features F = {f‚ÇÅ, f‚ÇÇ, ..., f‚Çñ}

**–ù–∞–π—Ç–∏:**
- Confidence score C(D) ‚àà [0, 1]

**–ú–æ–¥–µ–ª—å:**

```
C(D) = Œ£·µ¢ w·µ¢ * f·µ¢(D)

–≥–¥–µ:
- w·µ¢ - –≤–µ—Å —Ñ–∞–∫—Ç–æ—Ä–∞ i (learned or configured)
- f·µ¢(D) - normalized factor score ‚àà [0, 1]
```

#### Factor Definitions

**F1: Linguistic Quality (weight: 0.30)**
```
f_linguistic(D) = (
    adj_noun_balance(D) * 0.4 +
    syntactic_complexity(D) * 0.3 +
    pos_variety(D) * 0.3
)

adj_noun_balance(D):
    ratio = count(ADJ) / count(NOUN)
    IF 0.3 <= ratio <= 0.7:
        RETURN 1.0
    ELSE IF 0.1 <= ratio < 0.3 OR 0.7 < ratio <= 1.0:
        RETURN 0.6
    ELSE:
        RETURN 0.3

syntactic_complexity(D):
    avg_depth = mean(dependency_depth(sent) for sent in D.sentences)
    # Literary descriptions: depth 2-4
    IF 2 <= avg_depth <= 4:
        RETURN 1.0
    ELSE IF 1 <= avg_depth < 2:
        RETURN 0.6
    ELSE:
        RETURN 0.8 - (avg_depth - 4) * 0.1

pos_variety(D):
    unique_pos = len(set(token.pos for token in D))
    # Good descriptions have 6+ POS tags
    RETURN min(1.0, unique_pos / 8.0)
```

**F2: Visual Richness (weight: 0.25)**
```
f_visual(D) = (
    color_density(D) * 0.20 +
    size_scale_density(D) * 0.18 +
    texture_density(D) * 0.15 +
    lighting_density(D) * 0.22 +
    shape_density(D) * 0.12 +
    material_density(D) * 0.13
)

# Generic density function
density(D, vocabulary):
    word_count = count_words(D.text, vocabulary)
    total_words = len(D.text.split())
    # Normalize: 1% coverage = 0.2 score
    RETURN min(1.0, (word_count / total_words) * 20)
```

**F3: Structural Completeness (weight: 0.20)**
```
f_structure(D) = (
    starts_complete(D) * 0.3 +
    ends_complete(D) * 0.3 +
    multi_sentence(D) * 0.4
)

starts_complete(D):
    IF D.text[0].isupper():
        RETURN 1.0
    ELSE:
        RETURN 0.0  # Incomplete start = disqualify

ends_complete(D):
    last_char = D.text.rstrip()[-1]
    IF last_char IN ['.', '!', '?']:
        RETURN 1.0
    ELSE IF last_char IN [',', ';']:
        RETURN 0.3
    ELSE:
        RETURN 0.1

multi_sentence(D):
    sentence_count = count_sentences(D.text)
    IF sentence_count >= 3:
        RETURN 1.0
    ELSE IF sentence_count == 2:
        RETURN 0.7
    ELSE:
        RETURN 0.4
```

**F4: Type Specificity (weight: 0.15)**
```
f_type(D) = type_specific_score(D.text, D.type)

type_specific_score(text, type):
    IF type == LOCATION:
        RETURN location_specificity(text)
    ELSE IF type == CHARACTER:
        RETURN character_specificity(text)
    ELSE IF type == ATMOSPHERE:
        RETURN atmosphere_specificity(text)
    ELSE:
        RETURN 0.5

location_specificity(text):
    # Location indicators
    location_nouns = ["–≥–æ—Ä–æ–¥", "–¥–µ—Ä–µ–≤–Ω—è", "–∑–∞–º–æ–∫", "–¥–≤–æ—Ä–µ—Ü", ...]
    spatial_preps = ["–Ω–∞–¥", "–ø–æ–¥", "–≤–æ–∫—Ä—É–≥", "–º–µ–∂–¥—É", ...]
    architecture = ["–±–∞—à–Ω—è", "—Å—Ç–µ–Ω–∞", "–∫—Ä—ã—à–∞", "–æ–∫–Ω–æ", ...]

    score = 0.0
    score += min(0.4, count_words(text, location_nouns) / word_count * 10)
    score += min(0.3, count_words(text, spatial_preps) / word_count * 15)
    score += min(0.3, count_words(text, architecture) / word_count * 10)

    RETURN score

character_specificity(text):
    # Character indicators
    appearance = ["–ª–∏—Ü–æ", "–≥–ª–∞–∑–∞", "–≤–æ–ª–æ—Å—ã", "—Ä—É–∫–∏", ...]
    clothing = ["–æ–¥–µ–∂–¥–∞", "–ø–ª–∞—â", "–¥–æ—Å–ø–µ—Ö–∏", "—à–ª–µ–º", ...]
    characteristics = ["–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π", "—Ö—É–¥–æ–π", "–ø–æ–ª–Ω—ã–π", ...]

    score = 0.0
    score += min(0.4, count_words(text, appearance) / word_count * 10)
    score += min(0.3, count_words(text, clothing) / word_count * 10)
    score += min(0.3, count_words(text, characteristics) / word_count * 10)

    RETURN score

atmosphere_specificity(text):
    # Atmosphere indicators
    weather = ["–≤–µ—Ç–µ—Ä", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥", "—Ç—É–º–∞–Ω", ...]
    lighting = ["—Å–≤–µ—Ç", "—Ç–µ–Ω—å", "—Å—É–º—Ä–∞–∫", "—Ä–∞—Å—Å–≤–µ—Ç", ...]
    mood = ["–º—Ä–∞—á–Ω—ã–π", "—Å–≤–µ—Ç–ª—ã–π", "—Ç–∏—Ö–∏–π", "—Ç—Ä–µ–≤–æ–∂–Ω—ã–π", ...]

    score = 0.0
    score += min(0.35, count_words(text, weather) / word_count * 10)
    score += min(0.35, count_words(text, lighting) / word_count * 10)
    score += min(0.30, count_words(text, mood) / word_count * 10)

    RETURN score
```

**F5: Length Appropriateness (weight: 0.10)**
```
f_length(D) = length_score(len(D.text))

length_score(length):
    # Optimal –¥–ª—è image generation: 1000-2500 chars
    IF 1000 <= length <= 2500:
        RETURN 1.0
    ELSE IF 500 <= length < 1000:
        # Linear interpolation
        RETURN 0.6 + (length - 500) / 500 * 0.4
    ELSE IF 2500 < length <= 3500:
        # Linear decline
        RETURN 0.9 - (length - 2500) / 1000 * 0.2
    ELSE IF length < 500:
        # Too short - penalty
        RETURN max(0.0, length / 500 * 0.6)
    ELSE:  # length > 3500
        # Too long - moderate penalty
        RETURN 0.7 - min(0.3, (length - 3500) / 1000 * 0.1)
```

#### Final Confidence Formula

```python
def calculate_confidence(description: CompleteDescription) -> float:
    """
    Multi-factor confidence scoring with learned weights.
    """
    # Extract features
    F1 = calculate_linguistic_quality(description)
    F2 = calculate_visual_richness(description)
    F3 = calculate_structural_completeness(description)
    F4 = calculate_type_specificity(description)
    F5 = calculate_length_appropriateness(description)

    # Weights (can be learned from feedback)
    W = {
        "linguistic": 0.30,
        "visual": 0.25,
        "structure": 0.20,
        "type": 0.15,
        "length": 0.10,
    }

    # Weighted sum
    confidence = (
        W["linguistic"] * F1 +
        W["visual"] * F2 +
        W["structure"] * F3 +
        W["type"] * F4 +
        W["length"] * F5
    )

    return clamp(confidence, 0.0, 1.0)
```

### 4.2 Type Classification Model

**–¢–µ–∫—É—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞:** Binary patterns, hardcoded keywords.

**–†–µ—à–µ–Ω–∏–µ:** Multi-class classification —Å feature-based scoring.

#### Model: Hierarchical Scoring Classifier

```
Type Scores:
    S_location = score_location(D)
    S_character = score_character(D)
    S_atmosphere = score_atmosphere(D)
    S_object = score_object(D)

Classification Rule:
    type(D) = argmax(S_location, S_character, S_atmosphere, S_object)

    WITH constraint: max_score >= threshold (e.g., 0.4)
    IF max_score < threshold: REJECT or classify as OBJECT (fallback)
```

#### Location Score Function

```python
def score_location(description):
    """
    Scores how likely description is a LOCATION.

    Features:
    1. Location nouns (30%)
    2. Spatial prepositions (25%)
    3. Architecture/geography vocabulary (20%)
    4. Absence of character-specific words (15%)
    5. Static verbs (–±—ã–ª, –Ω–∞—Ö–æ–¥–∏–ª—Å—è, —Å—Ç–æ—è–ª) (10%)
    """
    text = description.text.lower()
    words = text.split()
    total_words = len(words)

    # Feature 1: Location nouns (30%)
    location_nouns = {
        "–≥–æ—Ä–æ–¥", "–¥–µ—Ä–µ–≤–Ω—è", "—Å–µ–ª–æ", "—Å—Ç–æ–ª–∏—Ü–∞",
        "–∑–∞–º–æ–∫", "–¥–≤–æ—Ä–µ—Ü", "–∫—Ä–µ–ø–æ—Å—Ç—å", "–±–∞—à–Ω—è",
        "–ª–µ—Å", "–ø–æ–ª–µ", "–ª—É–≥", "–¥–æ–ª–∏–Ω–∞",
        "–≥–æ—Ä–∞", "—Ö–æ–ª–º", "—É—Ç–µ—Å", "—Å–∫–∞–ª–∞",
        "—Ä–µ–∫–∞", "–æ–∑–µ—Ä–æ", "–º–æ—Ä–µ", "–æ–∫–µ–∞–Ω",
        "—É–ª–∏—Ü–∞", "–ø–ª–æ—â–∞–¥—å", "–ø–µ—Ä–µ—É–ª–æ–∫",
        "–¥–æ–º", "–∑–¥–∞–Ω–∏–µ", "—Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ",
        # ... expand to 100+ words
    }
    location_count = sum(1 for w in words if w in location_nouns)
    f1 = min(1.0, location_count / (total_words * 0.05))  # 5% coverage = 1.0

    # Feature 2: Spatial prepositions (25%)
    spatial_preps = {
        "–Ω–∞–¥", "–ø–æ–¥", "–≤–æ–∫—Ä—É–≥", "–º–µ–∂–¥—É", "—Ä—è–¥–æ–º", "–≤–æ–∑–ª–µ",
        "–æ–∫–æ–ª–æ", "–ø–µ—Ä–µ–¥", "–∑–∞", "–≤–Ω—É—Ç—Ä–∏", "—Å–Ω–∞—Ä—É–∂–∏",
        "–≤—ã—à–µ", "–Ω–∏–∂–µ", "–¥–∞–ª—å—à–µ", "–±–ª–∏–∂–µ",
        # ... expand
    }
    spatial_count = sum(1 for i, w in enumerate(words)
                       if w in spatial_preps and i + 1 < len(words))
    f2 = min(1.0, spatial_count / (total_words * 0.03))  # 3% = 1.0

    # Feature 3: Architecture/geography (20%)
    architecture = {
        "—Å—Ç–µ–Ω–∞", "–∫—Ä—ã—à–∞", "–æ–∫–Ω–æ", "–¥–≤–µ—Ä—å", "–≤–æ—Ä–æ—Ç–∞",
        "–∞—Ä–∫–∞", "–∫–æ–ª–æ–Ω–Ω–∞", "–∫—É–ø–æ–ª", "—à–ø–∏–ª—å",
        "—Ñ–∞—Å–∞–¥", "–±–∞–ª–∫–æ–Ω", "—Ç–µ—Ä—Ä–∞—Å–∞",
        # ... expand
    }
    geography = {
        "–≥–æ—Ä–∏–∑–æ–Ω—Ç", "–≤–µ—Ä—à–∏–Ω–∞", "—Å–∫–ª–æ–Ω", "–±–µ—Ä–µ–≥",
        "–∑–∞–ª–∏–≤", "–º—ã—Å", "–æ—Å—Ç—Ä–æ–≤", "–ø–æ–ª—É–æ—Å—Ç—Ä–æ–≤",
        # ... expand
    }
    arch_geo_vocab = architecture | geography
    arch_geo_count = sum(1 for w in words if w in arch_geo_vocab)
    f3 = min(1.0, arch_geo_count / (total_words * 0.04))  # 4% = 1.0

    # Feature 4: Absence of character words (15%)
    character_words = {
        "–ª–∏—Ü–æ", "–≥–ª–∞–∑–∞", "–≤–æ–ª–æ—Å—ã", "—Ä—É–∫–∏", "–Ω–æ–≥–∏",
        "–≥–æ–ª–æ–≤–∞", "—Ç–µ–ª–æ", "—Ñ–∏–≥—É—Ä–∞", "—Ä–æ—Å—Ç",
        "–º—É–∂—á–∏–Ω–∞", "–∂–µ–Ω—â–∏–Ω–∞", "—á–µ–ª–æ–≤–µ–∫", "–æ–Ω", "–æ–Ω–∞",
        # ... expand
    }
    character_count = sum(1 for w in words if w in character_words)
    f4 = 1.0 - min(1.0, character_count / (total_words * 0.05))

    # Feature 5: Static verbs (10%)
    static_verbs = {
        "–Ω–∞—Ö–æ–¥–∏–ª—Å—è", "—Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è", "—Å—Ç–æ—è–ª", "–≤–æ–∑–≤—ã—à–∞–ª—Å—è",
        "–ø—Ä–æ—Å—Ç–∏—Ä–∞–ª—Å—è", "—Ç—è–Ω—É–ª—Å—è", "—Ä–∞—Å–∫–∏–Ω—É–ª—Å—è",
        # ... expand
    }
    static_count = sum(1 for w in words if w in static_verbs)
    f5 = min(1.0, static_count / 5.0)  # 5 verbs = 1.0

    # Weighted score
    score = (
        f1 * 0.30 +
        f2 * 0.25 +
        f3 * 0.20 +
        f4 * 0.15 +
        f5 * 0.10
    )

    return score
```

–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑—É—é—Ç—Å—è `score_character()`, `score_atmosphere()`, `score_object()`.

---

## üöÄ –ß–ê–°–¢–¨ 5: Advanced NLP –¢–µ—Ö–Ω–∏–∫–∏

### 5.1 Discourse Segmentation

**–ü—Ä–æ–±–ª–µ–º–∞:** –¢–µ–∫—É—â–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç discourse structure.

**–†–µ—à–µ–Ω–∏–µ:** Rhetorical Structure Theory (RST) –¥–ª—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.

#### Rhetorical Relations –≤ –û–ø–∏—Å–∞–Ω–∏—è—Ö

**Relations:**
1. **ELABORATION**: Paragraph p2 elaborates on p1
   - Example: p1="–ó–∞–º–æ–∫ —Å—Ç–æ—è–ª –Ω–∞ —Ö–æ–ª–º–µ." p2="–ï–≥–æ —Å—Ç–µ–Ω—ã –±—ã–ª–∏ –∏–∑ —Å–µ—Ä–æ–≥–æ –∫–∞–º–Ω—è."

2. **CONTINUATION**: Paragraph p2 continues description from p1
   - Example: p1="–í–Ω–µ—à–Ω–∏–µ —Å—Ç–µ–Ω—ã..." p2="–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–≤–æ—Ä..."

3. **SPECIFICATION**: Paragraph p2 specifies details from p1
   - Example: p1="–ë–∞—à–Ω–∏ –ø–æ —É–≥–ª–∞–º." p2="–°–µ–≤–µ—Ä–Ω–∞—è –±–∞—à–Ω—è –±—ã–ª–∞ —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π."

4. **CONTRAST**: Paragraph p2 contrasts with p1 (scene change!)
   - Example: p1="–°–Ω–∞—Ä—É–∂–∏..." p2="–ù–æ –≤–Ω—É—Ç—Ä–∏ –≤—Å—ë –±—ã–ª–æ –∏–Ω–∞—á–µ."

**Algorithm:**
```python
def detect_discourse_relation(para1, para2):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç rhetorical relation –º–µ–∂–¥—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏.
    """
    # Lexical cues
    if starts_with(para2.text, ["–ö—Ä–æ–º–µ —Ç–æ–≥–æ", "–¢–∞–∫–∂–µ", "–ö —Ç–æ–º—É –∂–µ"]):
        return ELABORATION

    if starts_with(para2.text, ["–í–Ω—É—Ç—Ä–∏", "–°–Ω–∞—Ä—É–∂–∏", "–†—è–¥–æ–º"]):
        return CONTINUATION

    if starts_with(para2.text, ["–û—Å–æ–±–µ–Ω–Ω–æ", "–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏", "–ù–∞–ø—Ä–∏–º–µ—Ä"]):
        return SPECIFICATION

    if starts_with(para2.text, ["–ù–æ", "–û–¥–Ω–∞–∫–æ", "–ù–∞–ø—Ä–æ—Ç–∏–≤", "–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ"]):
        return CONTRAST  # ‚Üê STOP SIGNAL!

    # Semantic similarity
    sim = cosine_similarity(embed(para1.text), embed(para2.text))
    if sim > 0.75:
        return CONTINUATION
    elif sim > 0.60:
        return ELABORATION
    else:
        return NONE  # No clear relation
```

### 5.2 Entity Coreference Resolution

**–ü—Ä–æ–±–ª–µ–º–∞:** –°–∏—Å—Ç–µ–º–∞ –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ entity.

**–†–µ—à–µ–Ω–∏–µ:** Coreference chains –¥–ª—è entity tracking.

#### Algorithm: Cross-Document Coreference

```python
class EntityRegistry:
    """
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç entities across chapters.
    """
    def __init__(self):
        self.entities = {}  # entity_id -> EntityInfo
        self.mentions = []  # List[Mention]

    def register_mention(self, mention, chapter_id):
        """
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ entity.
        """
        # Try to resolve to existing entity
        entity_id = self._resolve_coreference(mention)

        if entity_id:
            # Update existing entity
            self.entities[entity_id].mentions.append(mention)
            self.entities[entity_id].last_seen_chapter = chapter_id
        else:
            # Create new entity
            entity_id = generate_id()
            self.entities[entity_id] = EntityInfo(
                canonical_name=mention.text,
                type=mention.type,
                first_seen_chapter=chapter_id,
                mentions=[mention],
            )

        return entity_id

    def _resolve_coreference(self, mention):
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å—Å—ã–ª–∞–µ—Ç—Å—è –ª–∏ mention –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–π entity.

        Uses:
        - String similarity (Levenshtein distance)
        - Semantic similarity (embeddings)
        - Type consistency
        """
        candidates = []

        for entity_id, entity in self.entities.items():
            # Type must match
            if entity.type != mention.type:
                continue

            # String similarity
            canonical_name = entity.canonical_name.lower()
            mention_text = mention.text.lower()

            # Exact match
            if canonical_name == mention_text:
                return entity_id

            # Partial match (one contains the other)
            if canonical_name in mention_text or mention_text in canonical_name:
                candidates.append((entity_id, 0.9))
                continue

            # Levenshtein distance
            lev_dist = levenshtein_distance(canonical_name, mention_text)
            max_len = max(len(canonical_name), len(mention_text))
            similarity = 1.0 - (lev_dist / max_len)

            if similarity > 0.75:
                candidates.append((entity_id, similarity))

            # Semantic similarity
            sem_sim = cosine_similarity(
                embed(entity.canonical_description),
                embed(mention.context)
            )
            if sem_sim > 0.80:
                candidates.append((entity_id, sem_sim * 0.9))

        # Return best candidate
        if candidates:
            candidates.sort(key=lambda x: x[1], reverse=True)
            best_entity_id, best_score = candidates[0]
            if best_score > 0.80:
                return best_entity_id

        return None  # New entity
```

### 5.3 Semantic Role Labeling (SRL)

**–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ –ø–æ–Ω–∏–º–∞–µ–º semantic roles –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.

**–†–µ—à–µ–Ω–∏–µ:** SRL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è WHO, WHAT, WHERE, WHEN, HOW.

#### Example:

**Sentence:** "–í—ã—Å–æ–∫–∞—è –±–∞—à–Ω—è —Å –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω–æ–π –∫—Ä—ã—à–µ–π –≤–æ–∑–≤—ã—à–∞–ª–∞—Å—å –Ω–∞–¥ –≥–æ—Ä–æ–¥–æ–º."

**SRL Analysis:**
```
Predicate: –≤–æ–∑–≤—ã—à–∞–ª–∞—Å—å (was towering)
ARG0 (Agent/Theme): –í—ã—Å–æ–∫–∞—è –±–∞—à–Ω—è —Å –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω–æ–π –∫—Ä—ã—à–µ–π
ARG1 (Location): –Ω–∞–¥ –≥–æ—Ä–æ–¥–æ–º
Modifiers:
  - –≤—ã—Å–æ–∫–∞—è (attribute)
  - —Å –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω–æ–π –∫—Ä—ã—à–µ–π (attribute)
```

**Use Case:**
- Extract main entity: "–±–∞—à–Ω—è"
- Extract attributes: "–≤—ã—Å–æ–∫–∞—è", "–æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω–∞—è –∫—Ä—ã—à–∞"
- Extract spatial relation: "–Ω–∞–¥ –≥–æ—Ä–æ–¥–æ–º"

**Implementation:**
```python
def extract_semantic_roles(sentence):
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Stanza –¥–ª—è SRL.
    """
    doc = stanza_nlp(sentence)

    roles = []
    for sent in doc.sentences:
        for word in sent.words:
            if word.upos == "VERB":
                # Find arguments
                arg0 = find_subject(word, sent)
                arg1 = find_object(word, sent)

                role = SemanticRole(
                    predicate=word.text,
                    arg0=arg0,
                    arg1=arg1,
                )
                roles.append(role)

    return roles
```

### 5.4 Dependency Parsing –¥–ª—è Compound Descriptions

**–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º dependency structure –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.

**–†–µ—à–µ–Ω–∏–µ:** Subtree extraction –∏–∑ dependency parse.

#### Example:

**Sentence:** "–ú–∞—Å—Å–∏–≤–Ω–∞—è –∫—Ä–µ–ø–æ—Å—Ç—å –∏–∑ —Ç–µ–º–Ω–æ–≥–æ –≥—Ä–∞–Ω–∏—Ç–∞ –≤–æ–∑–≤—ã—à–∞–ª–∞—Å—å –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ —Ö–æ–ª–º–∞."

**Dependency Parse:**
```
–≤–æ–∑–≤—ã—à–∞–ª–∞—Å—å (ROOT)
  ‚îú‚îÄ –∫—Ä–µ–ø–æ—Å—Ç—å (nsubj)
  ‚îÇ  ‚îú‚îÄ –ú–∞—Å—Å–∏–≤–Ω–∞—è (amod)
  ‚îÇ  ‚îî‚îÄ –≥—Ä–∞–Ω–∏—Ç–∞ (nmod)
  ‚îÇ     ‚îú‚îÄ –∏–∑ (case)
  ‚îÇ     ‚îî‚îÄ —Ç–µ–º–Ω–æ–≥–æ (amod)
  ‚îî‚îÄ –≤–µ—Ä—à–∏–Ω–µ (obl)
     ‚îú‚îÄ –Ω–∞ (case)
     ‚îî‚îÄ —Ö–æ–ª–º–∞ (nmod)
```

**Subtree Extraction:**
```
Main entity: –∫—Ä–µ–ø–æ—Å—Ç—å
  Attributes: –ú–∞—Å—Å–∏–≤–Ω–∞—è, –∏–∑ —Ç–µ–º–Ω–æ–≥–æ –≥—Ä–∞–Ω–∏—Ç–∞
  Location: –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ —Ö–æ–ª–º–∞
```

**Algorithm:**
```python
def extract_description_from_dependency(sent):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É—è dependency parse.
    """
    # Find main predicate (ROOT)
    root = None
    for word in sent.words:
        if word.deprel == "root":
            root = word
            break

    if not root:
        return None

    # Find subject
    subject = None
    for word in sent.words:
        if word.head == root.id and word.deprel in ["nsubj", "nsubj:pass"]:
            subject = word
            break

    if not subject:
        return None

    # Extract subject subtree (all modifiers)
    subject_subtree = extract_subtree(subject, sent)

    # Extract location/oblique arguments
    locations = []
    for word in sent.words:
        if word.head == root.id and word.deprel in ["obl", "obl:tmod", "obl:lmod"]:
            loc_subtree = extract_subtree(word, sent)
            locations.append(loc_subtree)

    description = {
        "main_entity": subject_subtree,
        "predicate": root.text,
        "locations": locations,
    }

    return description

def extract_subtree(word, sent):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–µ–≤–æ –¥–ª—è —Å–ª–æ–≤–∞ (–≤—Å–µ –µ–≥–æ dependents).
    """
    subtree = [word]

    def collect_dependents(w):
        for other in sent.words:
            if other.head == w.id:
                subtree.append(other)
                collect_dependents(other)

    collect_dependents(word)

    # Sort by position
    subtree.sort(key=lambda x: x.id)

    # Join text
    text = " ".join(w.text for w in subtree)

    return text
```

---

## üìä –ß–ê–°–¢–¨ 6: –ì—Ä–∞—Ñ-–ê–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –û–ø–∏—Å–∞–Ω–∏–π

### 6.1 Description Graph

**–ò–¥–µ—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞–∫ –≥—Ä–∞—Ñ, –≥–¥–µ:
- Nodes = Descriptions
- Edges = Relations (SAME_ENTITY, SAME_LOCATION, TEMPORAL_SEQUENCE, etc.)

#### Graph Construction

```python
class DescriptionGraph:
    """
    –ì—Ä–∞—Ñ –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ relationships.
    """
    def __init__(self):
        self.nodes = {}  # description_id -> Description
        self.edges = []  # List[Edge(source, target, type, weight)]

    def add_description(self, description):
        """–î–æ–±–∞–≤–ª—è–µ—Ç description –∫–∞–∫ node."""
        self.nodes[description.id] = description

    def add_edge(self, source_id, target_id, edge_type, weight):
        """–î–æ–±–∞–≤–ª—è–µ—Ç edge –º–µ–∂–¥—É descriptions."""
        edge = Edge(source_id, target_id, edge_type, weight)
        self.edges.append(edge)

    def build_edges(self):
        """
        –°—Ç—Ä–æ–∏—Ç edges –º–µ–∂–¥—É descriptions –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö relations.
        """
        descriptions = list(self.nodes.values())

        # O(n¬≤) pairwise comparison
        for i, desc1 in enumerate(descriptions):
            for desc2 in descriptions[i+1:]:
                # Check various relations

                # Relation 1: SAME_ENTITY
                if self._shares_entity(desc1, desc2):
                    weight = self._entity_overlap_score(desc1, desc2)
                    self.add_edge(desc1.id, desc2.id, "SAME_ENTITY", weight)

                # Relation 2: SAME_LOCATION
                if self._same_location(desc1, desc2):
                    weight = self._location_similarity(desc1, desc2)
                    self.add_edge(desc1.id, desc2.id, "SAME_LOCATION", weight)

                # Relation 3: TEMPORAL_SEQUENCE
                if desc2.chapter_id > desc1.chapter_id:
                    if self._temporal_continuation(desc1, desc2):
                        weight = 0.8
                        self.add_edge(desc1.id, desc2.id, "TEMPORAL", weight)

    def find_connected_components(self):
        """
        –ù–∞—Ö–æ–¥–∏—Ç connected components (clusters of related descriptions).

        Uses: Union-Find algorithm
        """
        parent = {node_id: node_id for node_id in self.nodes}

        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]

        def union(x, y):
            root_x = find(x)
            root_y = find(y)
            if root_x != root_y:
                parent[root_y] = root_x

        # Union based on edges
        for edge in self.edges:
            if edge.weight > 0.7:  # Strong connection
                union(edge.source, edge.target)

        # Group by root
        components = {}
        for node_id in self.nodes:
            root = find(node_id)
            if root not in components:
                components[root] = []
            components[root].append(node_id)

        return list(components.values())

    def rank_descriptions_pagerank(self):
        """
        –†–∞–Ω–∂–∏—Ä—É–µ—Ç descriptions –∏—Å–ø–æ–ª—å–∑—É—è PageRank algorithm.

        –ò–¥–µ—è: –í–∞–∂–Ω—ã–µ descriptions - —Ç–µ, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Å—Å—ã–ª–∞—é—Ç—Å—è –¥—Ä—É–≥–∏–µ –≤–∞–∂–Ω—ã–µ descriptions.
        """
        n = len(self.nodes)
        node_ids = list(self.nodes.keys())
        node_index = {node_id: i for i, node_id in enumerate(node_ids)}

        # Build adjacency matrix
        A = [[0.0] * n for _ in range(n)]
        for edge in self.edges:
            i = node_index[edge.source]
            j = node_index[edge.target]
            A[i][j] = edge.weight
            A[j][i] = edge.weight  # Undirected

        # Normalize (stochastic matrix)
        for i in range(n):
            row_sum = sum(A[i])
            if row_sum > 0:
                A[i] = [val / row_sum for val in A[i]]

        # PageRank iteration
        d = 0.85  # Damping factor
        ranks = [1.0 / n] * n

        for _ in range(30):  # 30 iterations
            new_ranks = []
            for i in range(n):
                rank_sum = sum(A[j][i] * ranks[j] for j in range(n))
                new_rank = (1 - d) / n + d * rank_sum
                new_ranks.append(new_rank)
            ranks = new_ranks

        # Map back to descriptions
        for i, node_id in enumerate(node_ids):
            self.nodes[node_id].pagerank_score = ranks[i]

        # Sort by PageRank
        sorted_descriptions = sorted(
            self.nodes.values(),
            key=lambda d: d.pagerank_score,
            reverse=True
        )

        return sorted_descriptions
```

**Use Cases:**
1. **Deduplication:** Find descriptions in same component ‚Üí merge
2. **Context enrichment:** Use graph neighbors –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è context
3. **Importance ranking:** PageRank –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
4. **Consistency checking:** Detect conflicting descriptions –≤ same component

---

## ü§ñ –ß–ê–°–¢–¨ 7: Machine Learning –ü–æ–¥—Ö–æ–¥—ã

### 7.1 Supervised Learning –¥–ª—è Classification

**Problem:** Hardcoded rules –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è.

**Solution:** Train classifier –Ω–∞ labeled data.

#### Dataset Construction

```
FEATURES:
- BOW (Bag of Words) features
- TF-IDF features
- POS tag sequences
- Dependency patterns
- Embedding features (BERT/GPT)

LABELS:
- IS_DESCRIPTION: {0, 1}
- DESCRIPTION_TYPE: {LOCATION, CHARACTER, ATMOSPHERE, OBJECT, NONE}
- QUALITY_SCORE: [0.0, 1.0]
```

#### Model Architecture

**Option 1: Random Forest**
```python
from sklearn.ensemble import RandomForestClassifier

# Features
X = extract_features(descriptions)  # (n_samples, n_features)

# Labels
y_type = [d.type for d in descriptions]
y_quality = [d.quality_score for d in descriptions]

# Train
clf_type = RandomForestClassifier(n_estimators=100)
clf_type.fit(X, y_type)

clf_quality = RandomForestRegressor(n_estimators=100)
clf_quality.fit(X, y_quality)
```

**Option 2: Neural Network (BERT-based)**
```python
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(
    "DeepPavlov/rubert-base-cased",
    num_labels=4  # 4 types
)

# Fine-tune –Ω–∞ labeled data
trainer = Trainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)
trainer.train()
```

### 7.2 Unsupervised Learning –¥–ª—è Clustering

**Problem:** –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π –±–µ–∑ labels.

**Solution:** Clustering algorithms (K-Means, DBSCAN, HDBSCAN).

#### Algorithm: HDBSCAN –¥–ª—è Description Clustering

```python
from sklearn.cluster import HDBSCAN
from sentence_transformers import SentenceTransformer

# Embed descriptions
embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
embeddings = embedder.encode([d.text for d in descriptions])

# Cluster
clusterer = HDBSCAN(min_cluster_size=5, metric='euclidean')
labels = clusterer.fit_predict(embeddings)

# Group by cluster
clusters = {}
for i, label in enumerate(labels):
    if label == -1:  # Noise
        continue
    if label not in clusters:
        clusters[label] = []
    clusters[label].append(descriptions[i])

# Analyze clusters
for cluster_id, cluster_descriptions in clusters.items():
    print(f"Cluster {cluster_id}: {len(cluster_descriptions)} descriptions")
    # –ú–æ–∂–µ–º –≤—ã–±—Ä–∞—Ç—å representative description
    # –ú–æ–∂–µ–º merge duplicates
    # –ú–æ–∂–µ–º –Ω–∞–π—Ç–∏ common theme
```

### 7.3 Reinforcement Learning –¥–ª—è Optimization

**Problem:** –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è thresholds –∏ weights.

**Solution:** RL agent –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

#### RL Formulation

**State:**
- Current settings: {confidence_threshold, min_length, max_length, weights, ...}
- Current metrics: {precision, recall, F1, avg_quality, ...}

**Action:**
- Adjust –æ–¥–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞: —É–≤–µ–ª–∏—á–∏—Ç—å/—É–º–µ–Ω—å—à–∏—Ç—å threshold, –∏–∑–º–µ–Ω–∏—Ç—å weight, etc.

**Reward:**
- Improvement –≤ target metric (e.g., F1 score –¥–ª—è quality descriptions)

**Algorithm: Q-Learning / Policy Gradient**
```python
class DescriptionExtractorRL:
    def __init__(self):
        self.state = initial_settings()
        self.q_table = {}  # State-Action values

    def get_action(self, state):
        """Epsilon-greedy policy."""
        if random.random() < epsilon:
            return random_action()
        else:
            return argmax_action(self.q_table[state])

    def update(self, state, action, reward, next_state):
        """Q-learning update."""
        current_q = self.q_table.get((state, action), 0.0)
        max_next_q = max(
            self.q_table.get((next_state, a), 0.0)
            for a in possible_actions
        )

        new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)
        self.q_table[(state, action)] = new_q

    def train(self, episodes=1000):
        """Train RL agent."""
        for episode in range(episodes):
            state = reset()

            for step in range(max_steps):
                action = self.get_action(state)
                next_state, reward = execute_action(action)
                self.update(state, action, reward, next_state)
                state = next_state

                if is_terminal(state):
                    break
```

---

## üéØ –ß–ê–°–¢–¨ 8: Implementation Roadmap

### Phase 1: Core Algorithms (3-4 –Ω–µ–¥–µ–ª–∏)

#### Week 1-2: Paragraph Segmentation & Boundary Detection
- [ ] Implement `ParagraphSegmenter`
- [ ] Implement `DescriptionBoundaryDetector`
- [ ] Test –Ω–∞ sample chapters
- [ ] Tune thresholds

**Files to create:**
- `backend/app/services/nlp/paragraph_segmenter.py` (~500 lines)
- `backend/app/services/nlp/boundary_detector.py` (~600 lines)

#### Week 3-4: Multi-Factor Confidence Scoring
- [ ] Implement –≤—Å–µ—Ö 5 —Ñ–∞–∫—Ç–æ—Ä–æ–≤ scoring
- [ ] Integrate –≤ description extraction pipeline
- [ ] Calibrate weights –Ω–∞ sample data
- [ ] A/B test: old scoring vs new scoring

**Files to modify:**
- `backend/app/services/enhanced_nlp_system.py` (+300 lines)
- Create `backend/app/services/nlp/confidence_scorer.py` (~400 lines)

### Phase 2: Advanced NLP Integration (3-4 –Ω–µ–¥–µ–ª–∏)

#### Week 5-6: Discourse Analysis & SRL
- [ ] Implement discourse relation detection
- [ ] Implement SRL extraction
- [ ] Integrate –≤ boundary detector

**Files to create:**
- `backend/app/services/nlp/discourse_analyzer.py` (~350 lines)
- `backend/app/services/nlp/semantic_role_labeler.py` (~300 lines)

#### Week 7-8: Entity Tracking & Context Manager
- [ ] Implement `EntityRegistry`
- [ ] Implement `DescriptionContextManager`
- [ ] Integrate cross-chapter tracking

**Files to create:**
- `backend/app/services/nlp/entity_registry.py` (~450 lines)
- `backend/app/services/nlp/context_manager.py` (~400 lines)

### Phase 3: Graph Algorithms & ML (3-4 –Ω–µ–¥–µ–ª–∏)

#### Week 9-10: Description Graph
- [ ] Implement `DescriptionGraph`
- [ ] Implement PageRank ranking
- [ ] Implement clustering algorithms

**Files to create:**
- `backend/app/services/nlp/description_graph.py` (~500 lines)

#### Week 11-12: ML Models
- [ ] Collect labeled dataset
- [ ] Train Random Forest classifier
- [ ] Train BERT-based classifier
- [ ] Evaluate and integrate best model

**Files to create:**
- `backend/app/services/ml/` (new directory)
  - `feature_extractor.py` (~300 lines)
  - `classifier.py` (~400 lines)
  - `training.py` (~200 lines)

### Phase 4: Testing & Optimization (2 –Ω–µ–¥–µ–ª–∏)

#### Week 13-14: Full Integration & Testing
- [ ] Re-parse test book "–í–µ–¥—å–º–∞–∫"
- [ ] Compare old vs new system
- [ ] Performance optimization
- [ ] Documentation

---

## üìä –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

### –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –í—ã–≤–æ–¥—ã

1. **–¢–µ–∫—É—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ fundamentally broken –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π:**
   - Sentence-level processing
   - No paragraph awareness
   - Naive confidence scoring
   - No context tracking

2. **–¢—Ä–µ–±—É—é—Ç—Å—è —Å–ª–æ–∂–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:**
   - Dynamic Programming –¥–ª—è boundary detection
   - Multi-factor scoring model
   - Graph algorithms –¥–ª—è relationships
   - ML classifiers –¥–ª—è type classification

3. **–ù–µ—Ç –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π:**
   - –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞
   - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è NLP + –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ + image generation requirements
   - –ù—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è custom implementation

4. **Complexity —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é:**
   - –ü—Ä–æ—Å—Ç—ã–µ regex patterns –ù–ï –†–ê–ë–û–¢–ê–Æ–¢
   - Hardcoded rules –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è
   - –ù—É–∂–Ω—ã advanced NLP —Ç–µ—Ö–Ω–∏–∫–∏ (SRL, discourse analysis, dependency parsing)
   - –ù—É–∂–Ω—ã ML models –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö

### Next Steps

**Immediate (Critical):**
1. Implement paragraph segmentation
2. Implement boundary detection
3. Implement multi-factor confidence scoring

**Short-term (High Priority):**
4. Entity tracking & context manager
5. Advanced NLP integration (SRL, discourse)

**Medium-term (Important):**
6. Graph algorithms
7. ML classifiers

**Long-term (Optimization):**
8. RL –¥–ª—è auto-tuning
9. BERT fine-tuning
10. Continuous learning from feedback

---

**Version:** 1.0 - Comprehensive Technical Analysis
**Last Updated:** 2025-11-05
**Status:** Ready for Implementation
**Estimated LOC:** ~10,000 new lines of advanced algorithmic code
**Estimated Time:** 10-12 –Ω–µ–¥–µ–ª—å –¥–ª—è –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

---

**–ö–û–ù–ï–¶ –¢–ï–•–ù–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê**

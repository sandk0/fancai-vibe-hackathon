# –ü–ª–∞–Ω –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ NLP-–ü–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

**–î–∞—Ç–∞:** 2025-11-05
**–ê–≤—Ç–æ—Ä:** Claude Code
**–í–µ—Ä—Å–∏—è:** 2.0 (–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è - —Ñ–æ–∫—É—Å –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö)
**–°—Ç–∞—Ç—É—Å:** Comprehensive Analysis & Implementation Plan

---

## üéØ –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ

### –¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø

–ê–Ω–∞–ª–∏–∑ 1257 –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –∫–Ω–∏–≥–∏ **"–í–µ–¥—å–º–∞–∫. –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–æ–∫ –≤–æ—Ä–æ–Ω–æ–≤"** –≤—ã—è–≤–∏–ª **–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã**:

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #1: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
```
–¢–ï–ö–£–©–ï–ï (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ):
  OBJECT:      672 (53.5%)  ‚Üê –î–û–ú–ò–ù–ò–†–£–ï–¢, –Ω–æ –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (40%)
  LOCATION:    503 (40.0%)  ‚Üê –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 75%
  CHARACTER:    61 (4.9%)   ‚Üê –ö–ê–¢–ê–°–¢–†–û–§–ê! –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 60%
  ATMOSPHERE:   21 (1.7%)   ‚Üê –ü–û–ß–¢–ò –ù–ï–¢! –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 45%

–û–ñ–ò–î–ê–ï–ú–û–ï (–ø—Ä–∞–≤–∏–ª—å–Ω–æ):
  LOCATION:    ~600 (48%)   ‚Üê –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–∫—É—Å
  CHARACTER:   ~400 (32%)   ‚Üê –í—Ç–æ—Ä–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
  ATMOSPHERE:  ~200 (16%)   ‚Üê –¢—Ä–µ—Ç–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
  OBJECT:       ~57 (4%)    ‚Üê –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
```

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #2: –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏–π - –£–ñ–ê–°–ù–û–ï
**–¢–æ–ø-10 –ø–æ confidence score (0.90-0.95) - —ç—Ç–æ –ú–£–°–û–†:**
- `"–∑–≤–µ–∑–¥—ã. –ñ–∞–Ω-–ê–Ω—Ç–µ–ª—å–º –ë—Ä–∏–ª—å—è-–°–∞–≤–∞—Ä–µ–Ω –ì–ª–∞–≤–∞ —Å–µ–¥—å–º–∞—è..."` - **–ó–ê–ì–û–õ–û–í–û–ö –ì–õ–ê–í–´**
- `"–º–∞—Ä—Ö–∏–∏. –ú–∞—Ä–∫–≥—Ä–∞—Ñ—Å—Ç–≤ —ç—Ç–∏—Ö —á–µ—Ç—ã—Ä–µ..."` - **–û–ë–†–´–í–û–ö –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø**
- `"–í–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç—å—é, —Ç–æ –µ—Å—Ç—å –æ—Ç –≥–æ–ª–æ–≤—ã..."` - **–§–†–ê–ì–ú–ï–ù–¢ –ò–ó –°–ü–†–ê–í–û–ß–ù–ò–ö–ê**

**–ö–æ—Ä–Ω–µ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞:** –°–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ –≤–º–µ—Å—Ç–æ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π.

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #3: –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–ô —Ñ–æ–∫—É—Å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö
```
–¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
  –°—Ä–µ–¥–Ω–µ–µ:  104.7 chars  ‚Üê –ö–ê–¢–ê–°–¢–†–û–§–ê! –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
  –ú–µ–¥–∏–∞–Ω–∞:  102 chars
  –ú–∏–Ω–∏–º—É–º:  50 chars     ‚Üê –ë–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
  –ú–∞–∫—Å–∏–º—É–º: 398 chars    ‚Üê –î–∞–∂–µ –º–∞–∫—Å–∏–º—É–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω!

–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã:
  < 100 chars:  587 (46.7%)  ‚Üê –ü–û–õ–û–í–ò–ù–ê –Ω–µ–ø—Ä–∏–≥–æ–¥–Ω—ã
  100-500:      670 (53.3%)  ‚Üê –ö–æ—Ä–æ—Ç–∫–∏–µ, –Ω–µ –ª—É—á—à–∏–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
  > 500 chars:  0 (0.0%)    ‚Üê –ù–ï–¢ –í–û–û–ë–©–ï –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π!!!
```

**‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ü–û–î–•–û–î–ê:** –°–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ–¥ –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è (15-300 chars), –Ω–æ:
- **–î–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (500-3500 chars) - —ç—Ç–æ –õ–£–ß–®–ò–ï –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏!**
- –í—Å–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (SD3, DALL-E 3, Gemini, Flux) –õ–Æ–ë–Ø–¢ –¥–ª–∏–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
- –í —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ —Å–∞–º—ã–µ —è—Ä–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è - —ç—Ç–æ –º–Ω–æ–≥–æ–ø—Ä–µ–¥–ª–æ–∂–µ–Ω—á–µ—Å–∫–∏–µ –∞–±–∑–∞—Ü—ã

---

## üöÄ –ù–æ–≤–∞—è –ü–∞—Ä–∞–¥–∏–≥–º–∞: –§–æ–∫—É—Å –Ω–∞ –î–ª–∏–Ω–Ω—ã—Ö –û–ø–∏—Å–∞–Ω–∏—è—Ö

### –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (2025)

#### Stable Diffusion 3 (SD3)
```
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 10,000 —Å–∏–º–≤–æ–ª–æ–≤ (1500+ —Å–ª–æ–≤)
–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞:  500-3000 —Å–∏–º–≤–æ–ª–æ–≤
–†–µ–≤–æ–ª—é—Ü–∏—è:          –£–±—Ä–∞–Ω –ª–∏–º–∏—Ç 77 —Ç–æ–∫–µ–Ω–æ–≤ CLIP
–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:       "–ß–µ–º –¥–µ—Ç–∞–ª—å–Ω–µ–µ, —Ç–µ–º –ª—É—á—à–µ"
```

**–¶–∏—Ç–∞—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:**
> "The big change in SD3 is prompting - you can now pass in very long and descriptive prompts and get back images with very good prompt adherence. You're no longer limited to the 77-token limit."

#### DALL-E 3 (OpenAI)
```
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 4,000 —Å–∏–º–≤–æ–ª–æ–≤
–§–∏—á–∞:               GPT-4 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:        "Research has shown that using very detailed prompts give significantly better results"
–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:       –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã
```

**–ü—Ä–∏–º–µ—Ä –∏–∑ best practices:**
> "Instead of 'A cat sitting on a couch,' use 'A fluffy orange tabby cat lounging on a blue velvet couch by a sunny window, soft afternoon light, detailed fur texture, cozy living room setting, photorealistic style.'"

#### Gemini Imagen 3 (Google)
```
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: –ù–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏)
–°—Ç–∏–ª—å:              Natural language, –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–π
–¢—Ä–µ–±—É–µ—Ç:            Detailed adjectives and adverbs
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:         Subject + Style + Context + Details
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
> "Use descriptive language with detailed adjectives and adverbs to paint a clear picture. Provide context or background information to aid the AI's understanding."

#### Flux (pollinations.ai)
```
–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 100-500+ —Å–∏–º–≤–æ–ª–æ–≤
–ú–∞–∫—Å–∏–º—É–º:          1000+ —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
–°—Ç–∏–ª—å:             Natural language, –∫–∞–∫ –∏–∑ –∫–Ω–∏–≥–∏
–§–æ—Ä–º–∞—Ç:            Subject ‚Üí Setting ‚Üí Lighting ‚Üí Color ‚Üí Mood ‚Üí Composition
```

### üéØ –ù–û–í–ê–Ø –¶–ï–õ–ï–í–ê–Ø –î–õ–ò–ù–ê: 500-3500 —Å–∏–º–≤–æ–ª–æ–≤

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**
- ‚úÖ **500-1500 chars**: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ **1500-2500 chars**: –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω (SD3, DALL-E 3)
- ‚úÖ **2500-3500 chars**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (SD3 excellent)
- ‚úÖ **> 3500 chars**: –î–æ–ø—É—Å—Ç–∏–º–æ, –Ω–æ –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

**–ü—Ä–∏–º–µ—Ä—ã –∏–∑ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã:**

#### –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (104 chars) - ‚ùå –ü–õ–û–•–û:
```
"–º–∞—Ä—Ö–∏–∏. –ú–∞—Ä–∫–≥—Ä–∞—Ñ—Å—Ç–≤ —ç—Ç–∏—Ö —á–µ—Ç—ã—Ä–µ: –ó–∞–ø–∞–¥–Ω–æ–µ, –í–µ—Ä—Ö–Ω–µ–µ, –û–∑—ë—Ä–Ω–æ–µ –∏ –ù–∏–∂–Ω–µ–µ. –ë–æ–ª–¥—É–∏–Ω –ê–¥–æ–≤–∞—Ä–¥–æ, ¬´–ù–æ–≤–æ–µ"
```
- –û–±—Ä—ã–≤–æ–∫, –Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ù–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º–æ
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ

#### –°—Ä–µ–¥–Ω–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (578 chars) - ‚úÖ –•–û–†–û–®–û:
```
"–í–µ—Ç–µ—Ä –¥—É–ª –Ω–∞ —Å–µ–≤–µ—Ä–æ-–≤–æ—Å—Ç–æ–∫, –Ω–∞–¥ —Å–µ–ª–µ–Ω–∏—è–º–∏ –î–∂—É–∞–ª–¥–µ –Ω–µ—Å–ø–µ—à–Ω–æ –ø–ª—ã–ª–∏ –Ω–∏–∑–∫–∏–µ
–æ–±–ª–∞–∫–∞, –∑–∞–∫—Ä—ã–≤–∞–≤—à–∏–µ —Å–æ–ª–Ω—Ü–µ; –∏–∑ –≤—Å–µ—Ö –±–∞—à–µ–Ω –∏ –º–∏–Ω–∞—Ä–µ—Ç–æ–≤ –¢–∞—Ä –í–∞–ª–æ–Ω–∞ –±—ã–ª–∏
–≤–∏–¥–Ω—ã —Ç–æ–ª—å–∫–æ –æ—Å—Ç—Ä—ã–µ –≤–µ—Ä—à–∏–Ω—ã. –•–æ–ª–æ–¥–Ω—ã–π, —Ä–æ–≤–Ω—ã–π —Å–≤–µ—Ç —Ä–æ–≤–Ω—ã–º–∏ —Å–µ—Ä—ã–º–∏ –º–∞–∑–∫–∞–º–∏
–ª–æ–∂–∏–ª—Å—è –Ω–∞ –±–µ–ª—ã–µ –∫–∞–º–Ω–∏ –≥–æ—Ä–æ–¥–∞, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –∑–¥–∞–Ω–∏–π,
–Ω–∞–ø–æ–º–∏–Ω–∞–≤—à–∏—Ö –æ–≥—Ä–æ–º–Ω—ã–µ –∏–≥—Ä–∞–ª—å–Ω—ã–µ –∫–æ—Å—Ç–∏ –∏–ª–∏ —á—å–∏-—Ç–æ –ø–∞–ª—å—Ü—ã, —É—Å—Ç—Ä–µ–º–ª—ë–Ω–Ω—ã–µ –∫
–Ω–µ–±—É. –ü–æ –∏–∑–≤–∏–ª–∏—Å—Ç—ã–º —É–ª–∏—Ü–∞–º, –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –ø–æ—á—ë—Ç–Ω–æ–π —Å—Ç—Ä–∞–∂–∏ –®–∞–π–¥–æ –¥–≤–∏–≥–∞–ª—Å—è –≤
–≤–æ—Å—Ç–æ—á–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –±–ª–µ—Å—Ç—è—â–∏–π –ø–æ–¥ —Å–µ—Ä—ã–º –Ω–µ–±–æ–º –ø–∞–ª–∞–Ω–∫–∏–Ω. –û—Ç –¥–≤–æ—Ä—Ü–∞ –≤
—Å—Ç–æ—Ä–æ–Ω—É –ß—ë—Ä–Ω–æ–π –±–∞—à–Ω–∏."
```
- ‚úÖ –ó–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
- ‚úÖ –í–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: –ø–æ–≥–æ–¥–∞, —Å–≤–µ—Ç, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Ü–≤–µ—Ç–∞
- ‚úÖ –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
- ‚úÖ –û—Ç–ª–∏—á–Ω–æ –¥–ª—è Flux/Gemini

#### –î–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1200-2500 chars) - ‚úÖ‚úÖ –ü–†–ï–í–û–°–•–û–î–ù–û:
```
"–ó–∞–º–æ–∫ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –∫—Ä—É—Ç–æ–≥–æ —Ö–æ–ª–º–∞, –µ–≥–æ –º–∞—Å—Å–∏–≤–Ω—ã–µ —Å—Ç–µ–Ω—ã –∏–∑ —Å–µ—Ä–æ–≥–æ
–≥—Ä–∞–Ω–∏—Ç–∞ –≤–æ–∑–≤—ã—à–∞–ª–∏—Å—å –Ω–∞–¥ –æ–∫—Ä—É–∂–∞—é—â–∏–º –ª–µ—Å–æ–º, —Å–ª–æ–≤–Ω–æ –∫–∞–º–µ–Ω–Ω—ã–π —Å—Ç—Ä–∞–∂, –æ—Ö—Ä–∞–Ω—è—é—â–∏–π
–≥—Ä–∞–Ω–∏—Ü—ã –∫–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–∞. –ß–µ—Ç—ã—Ä–µ –≤—ã—Å–æ–∫–∏–µ –±–∞—à–Ω–∏ –ø–æ —É–≥–ª–∞–º –±—ã–ª–∏ —É–≤–µ–Ω—á–∞–Ω—ã
–æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω—ã–º–∏ —à–ø–∏–ª—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–Ω–∑–∞–ª–∏ –Ω–∏–∑–∫–∏–µ –æ–±–ª–∞–∫–∞, –ø–ª—ã–≤—É—â–∏–µ –Ω–∞–¥
–¥–æ–ª–∏–Ω–æ–π. –í –ª—É—á–∞—Ö –∑–∞—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–ª–Ω—Ü–∞ –∫–∞–º–Ω–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞–ª–∏ —Ç–µ–ø–ª—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π
–æ—Ç—Ç–µ–Ω–æ–∫, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏—Ä—É—è —Å —Ö–æ–ª–æ–¥–Ω—ã–º–∏ —Ç–µ–Ω—è–º–∏, –ª–æ–∂–∏–≤—à–∏–º–∏—Å—è –Ω–∞ —Å–µ–≤–µ—Ä–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É
–∫—Ä–µ–ø–æ—Å—Ç–∏.

–ì–ª–∞–≤–Ω—ã–µ –≤–æ—Ä–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–ª–∏ —Å–æ–±–æ–π –º–∞—Å—Å–∏–≤–Ω—É—é –∞—Ä–∫—É –∏–∑ —á–µ—Ä–Ω–æ–≥–æ –±–∞–∑–∞–ª—å—Ç–∞, –Ω–∞–¥
–∫–æ—Ç–æ—Ä–æ–π –≤–æ–∑–≤—ã—à–∞–ª—Å—è —Ä–æ–¥–æ–≤–æ–π –≥–µ—Ä–± - —Å–µ—Ä–µ–±—Ä—è–Ω—ã–π –≥—Ä–∏—Ñ–æ–Ω –Ω–∞ –∞–ª–æ–º —Ñ–æ–Ω–µ. –¢—è–∂–µ–ª–∞—è
–¥—É–±–æ–≤–∞—è —Å—Ç–≤–æ—Ä–∫–∞, –æ–∫–æ–≤–∞–Ω–Ω–∞—è –∂–µ–ª–µ–∑–æ–º –∏ —É–∫—Ä–∞—à–µ–Ω–Ω–∞—è –∑–∞–º—ã—Å–ª–æ–≤–∞—Ç–æ–π —Ä–µ–∑—å–±–æ–π, –±—ã–ª–∞
–æ–ø—É—â–µ–Ω–∞, –∞ –Ω–∞–¥–æ —Ä–≤–æ–º –≤–∏—Å–µ–ª –ø–æ–¥—ä–µ–º–Ω—ã–π –º–æ—Å—Ç –Ω–∞ —Ç–æ–ª—Å—Ç—ã—Ö —Ü–µ–ø—è—Ö. –ü–æ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã
–æ—Ç –≤—Ö–æ–¥–∞ —Å—Ç–æ—è–ª–∏ —Ñ–∞–∫–µ–ª—ã –≤ –∫–æ–≤–∞–Ω—ã—Ö –¥–µ—Ä–∂–∞—Ç–µ–ª—è—Ö, –∏—Ö –ø–ª–∞–º—è —Ç—Ä–µ–ø–µ—Ç–∞–ª–æ –Ω–∞ –≤–µ—Ç—Ä—É,
–æ—Ç–±—Ä–∞—Å—ã–≤–∞—è –ø–ª—è—à—É—â–∏–µ —Ç–µ–Ω–∏ –Ω–∞ –∫–∞–º–µ–Ω–Ω—É—é –∫–ª–∞–¥–∫—É.

–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–≤–æ—Ä –±—ã–ª –≤—ã–º–æ—â–µ–Ω –ø–ª–∏—Ç–∞–º–∏ –±–µ–ª–æ–≥–æ –º—Ä–∞–º–æ—Ä–∞, –º–µ–∂–¥—É –∫–æ—Ç–æ—Ä—ã–º–∏
–ø—Ä–æ–±–∏–≤–∞–ª–∞—Å—å —Ç—Ä–∞–≤–∞. –í —Ü–µ–Ω—Ç—Ä–µ –≤–æ–∑–≤—ã—à–∞–ª—Å—è —Ñ–æ–Ω—Ç–∞–Ω –≤ –≤–∏–¥–µ —Ç—Ä–µ—Ö –¥–µ–ª—å—Ñ–∏–Ω–æ–≤,
–≤—ã–ø—É—Å–∫–∞—é—â–∏—Ö —Å—Ç—Ä—É–∏ –≤–æ–¥—ã –≤ –±–æ–ª—å—à—É—é —á–∞—à—É. –í–æ–∫—Ä—É–≥ –¥–≤–æ—Ä–∞ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å –∂–∏–ª—ã–µ
–ø–æ–∫–æ–∏, –∫–æ–Ω—é—à–Ω–∏, –∫—É–∑–Ω–∏—Ü–∞ –∏ —á–∞—Å–æ–≤–Ω—è. –í—Å—ë –±—ã–ª–æ –ø—Ä–æ–ø–∏—Ç–∞–Ω–æ –¥—É—Ö–æ–º —Å—Ç–∞—Ä–∏–Ω—ã –∏
–≤–µ–ª–∏—á–∏—è, —ç—Ö–æ–º –º–∏–Ω—É–≤—à–∏—Ö —Å—Ç–æ–ª–µ—Ç–∏–π..."
```
- ‚úÖ‚úÖ **–ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è SD3 (–¥–æ 10K chars!)**
- ‚úÖ‚úÖ –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ü–µ–Ω –¥–ª—è multiple images
- ‚úÖ‚úÖ –ò—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
- ‚úÖ‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ—Å–≤–µ—â–µ–Ω–∏–µ, —Ç–µ–∫—Å—Ç—É—Ä—ã, –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞
- ‚úÖ‚úÖ –ú–æ–∂–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 3-5 —Ä–∞–∑–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –æ–¥–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è

---

## üîç –ê–Ω–∞–ª–∏–∑: –ü–æ—á–µ–º—É –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ (2024-2025)

**–ù–∞–π–¥–µ–Ω–æ:**
- ‚úÖ NER –¥–ª—è —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã (Pushkin dataset)
- ‚úÖ Narrative extraction (–Ω–æ–≤–æ—Å—Ç–∏, –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã)
- ‚úÖ Scene detection (–∫–∏–Ω–æ, –≤–∏–¥–µ–æ)

**–ù–ï –Ω–∞–π–¥–µ–Ω–æ:**
- ‚ùå Long passage extraction –¥–ª—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
- ‚ùå Multi-paragraph description detection
- ‚ùå Literary scene boundary detection –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- ‚ùå –ì–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π 500-3500 chars

**–í—ã–≤–æ–¥:** –≠—Ç–æ **—É–Ω–∏–∫–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞**, —Ç—Ä–µ–±—É—é—â–∞—è **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏**.

### –ü–æ—á–µ–º—É —ç—Ç–æ —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞?

1. **–†–∞–∑–º—ã—Ç—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –æ–ø–∏—Å–∞–Ω–∏–π**
   - –û–ø–∏—Å–∞–Ω–∏–µ –º–æ–∂–µ—Ç –∏–¥—Ç–∏ 1-20 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
   - –ü–µ—Ä–µ—Ö–æ–¥—ã –æ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é –Ω–µ —è–≤–Ω—ã–µ
   - –í–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (–ª–æ–∫–∞—Ü–∏—è ‚Üí –ø–µ—Ä—Å–æ–Ω–∞–∂ ‚Üí –æ–±—ä–µ–∫—Ç)

2. **–†—É—Å—Å–∫–∏–π —è–∑—ã–∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω**
   - –ü—Ä–∏—á–∞—Å—Ç–Ω—ã–µ/–¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–Ω—ã–µ –æ–±–æ—Ä–æ—Ç—ã
   - –°–ª–æ–∂–Ω–æ–ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
   - –ò–Ω–≤–µ—Ä—Å–∏—è (–æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤)
   - –û—Ç–≥–ª–∞–≥–æ–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ

3. **–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∂–∞–Ω—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è**
   - –§—ç–Ω—Ç–µ–∑–∏: –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –º–∞–≥–∏–∏, —Å—É—â–µ—Å—Ç–≤, –º–∏—Ä–æ–≤
   - –î–µ—Ç–µ–∫—Ç–∏–≤: –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞, –∏–Ω—Ç–µ—Ä—å–µ—Ä—ã, —É–ª–∏–∫–∏
   - –†–æ–º–∞–Ω—Å: —ç–º–æ—Ü–∏–∏, –≤–Ω–µ—à–Ω–æ—Å—Ç—å, –æ–¥–µ–∂–¥–∞
   - –ö–ª–∞—Å—Å–∏–∫–∞: –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∑–º, –ø–µ–π–∑–∞–∂–∏, —Å–æ—Ü–∏—É–º

4. **–ö–æ–Ω—Ç–µ–∫—Å—Ç –∫—Ä–∏—Ç–∏—á–µ–Ω**
   - –ü–µ—Ä–≤–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ ‚Üí –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
   - –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ ‚Üí –∫—Ä–∞—Ç–∫–æ–µ
   - –û–ø–∏—Å–∞–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø–æ –≥–ª–∞–≤–∞–º

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Advanced Long Description Parser

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   INPUT: –¢–µ–∫—Å—Ç –≥–ª–∞–≤—ã                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 1: Paragraph Segmentation & Classification          ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ  ‚Ä¢ –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è!)                 ‚îÇ
‚îÇ  ‚Ä¢ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: Description vs Narrative vs Dialog        ‚îÇ
‚îÇ  ‚Ä¢ Preliminary scoring (0.0-1.0)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 2: Description Boundary Detection                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ  ‚Ä¢ Multi-paragraph window analysis                          ‚îÇ
‚îÇ  ‚Ä¢ Continuation signal detection                            ‚îÇ
‚îÇ  ‚Ä¢ End signal detection                                     ‚îÇ
‚îÇ  ‚Ä¢ Grouping paragraphs ‚Üí complete descriptions              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 3: Deep NLP Analysis (Multi-NLP)                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ  ‚Ä¢ SpaCy: Morphology, Syntax, Entities                      ‚îÇ
‚îÇ  ‚Ä¢ Natasha: Russian-specific patterns                       ‚îÇ
‚îÇ  ‚Ä¢ Stanza: Deep dependency parsing                          ‚îÇ
‚îÇ  ‚Ä¢ Ensemble voting –¥–ª—è type classification                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 4: Quality Scoring & Filtering                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ  ‚Ä¢ Visual richness score                                    ‚îÇ
‚îÇ  ‚Ä¢ Structural completeness                                  ‚îÇ
‚îÇ  ‚Ä¢ Literary quality assessment                              ‚îÇ
‚îÇ  ‚Ä¢ Anti-pattern filtering (dialog, meta-text, etc.)         ‚îÇ
‚îÇ  ‚Ä¢ Length appropriateness (500-3500 chars)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 5: Prompt Generation (Unified System)               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ  ‚Ä¢ Model-agnostic base prompt                               ‚îÇ
‚îÇ  ‚Ä¢ Model-specific formatting (SD3/DALL-E/Gemini/Flux)       ‚îÇ
‚îÇ  ‚Ä¢ Context enrichment (previous chapters)                   ‚îÇ
‚îÇ  ‚Ä¢ Style/genre adaptation                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            OUTPUT: High-Quality Descriptions                ‚îÇ
‚îÇ  ‚Ä¢ Type: LOCATION / CHARACTER / ATMOSPHERE                  ‚îÇ
‚îÇ  ‚Ä¢ Length: 500-3500 chars (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1000-2500)            ‚îÇ
‚îÇ  ‚Ä¢ Confidence: 0.65+                                        ‚îÇ
‚îÇ  ‚Ä¢ Prompts: –ì–æ—Ç–æ–≤—ã–µ –¥–ª—è SD3/DALL-E/Gemini/Flux             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìê STAGE 1: Paragraph Segmentation & Classification

### –¶–µ–ª—å
–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ **–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã** (–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è!), –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π.

### –ü–æ—á–µ–º—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –∞ –Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è?

**–ü—Ä–æ–±–ª–µ–º–∞ —Å sentence-based –ø–æ–¥—Ö–æ–¥–æ–º:**
```python
# –°–¢–ê–†–´–ô –ü–û–î–•–û–î (–ü–õ–û–•–û):
for sent in doc.sents:
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –û–î–ù–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    # –ü—Ä–æ–±–ª–µ–º–∞: –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ 3-20 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π!
    description = sent.text  # ‚Üê –§—Ä–∞–≥–º–µ–Ω—Ç!
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ‚Üí 5 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
- –ë–µ—Ä–µ—Ç—Å—è —Å–µ—Ä–µ–¥–∏–Ω–∞ ‚Üí –Ω–µ—Ç –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞
- "—Å—ä–µ–¥–∞–µ—Ç —á–∞—Å—Ç–∏ —Å–ª–æ–≤"

**–ù–û–í–´–ô –ü–û–î–•–û–î (–ü–†–ê–í–ò–õ–¨–ù–û):**
```python
# Paragraph-based analysis
paragraphs = text.split('\n\n')  # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º
for para in paragraphs:
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ (–º–æ–∂–µ—Ç –±—ã—Ç—å 3-20 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    # –ü–æ—Ç–æ–º –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–º–µ–∂–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
```

### –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–∑–±–∏–≤–∫–∏

```python
class ParagraphSegmenter:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã.
    """

    def segment(self, text: str) -> List[Paragraph]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —É–º–Ω–æ–π –ª–æ–≥–∏–∫–æ–π.
        """
        # Step 1: –†–∞–∑–±–∏–≤–∫–∞ –ø–æ —è–≤–Ω—ã–º –º–∞—Ä–∫–µ—Ä–∞–º
        raw_paragraphs = self._split_by_markers(text)

        # Step 2: Merge –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ (< 200 chars)
        merged = self._merge_short_paragraphs(raw_paragraphs)

        # Step 3: Split —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ (> 4000 chars)
        split_paragraphs = self._split_long_paragraphs(merged)

        # Step 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        classified = []
        for para_text in split_paragraphs:
            para = Paragraph(text=para_text)
            para.type = self._classify_paragraph(para_text)
            para.score = self._score_descriptiveness(para_text)
            classified.append(para)

        return classified

    def _split_by_markers(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º: –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏, —Ç–∏—Ä–µ –¥–∏–∞–ª–æ–≥–∞, etc.
        """
        # –Ø–≤–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        paragraphs = []
        current = []

        lines = text.split('\n')
        for line in lines:
            stripped = line.strip()

            # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ = –≥—Ä–∞–Ω–∏—Ü–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            if not stripped:
                if current:
                    paragraphs.append('\n'.join(current))
                    current = []
                continue

            # –î–∏–∞–ª–æ–≥ = –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
            if stripped.startswith('‚Äî') or stripped.startswith('- '):
                if current:
                    paragraphs.append('\n'.join(current))
                    current = []
                paragraphs.append(stripped)  # –î–∏–∞–ª–æ–≥ –æ—Ç–¥–µ–ª—å–Ω–æ
                continue

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥–ª–∞–≤—ã = –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
            if self._is_chapter_header(stripped):
                if current:
                    paragraphs.append('\n'.join(current))
                    current = []
                paragraphs.append(stripped)
                continue

            # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
            current.append(line)

        if current:
            paragraphs.append('\n'.join(current))

        return paragraphs

    def _classify_paragraph(self, text: str) -> ParagraphType:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ: DESCRIPTION, NARRATIVE, DIALOG, META.
        """
        # Dialog detection
        if text.strip().startswith('‚Äî') or text.count('‚Äî') > 2:
            return ParagraphType.DIALOG

        # Chapter header
        if re.match(r'^–ì–ª–∞–≤–∞\s+(–ø–µ—Ä–≤–∞—è|–≤—Ç–æ—Ä–∞—è|\d+)', text.strip()):
            return ParagraphType.META

        # Epigraph (quoted text)
        if text.strip().startswith('¬´') and text.strip().endswith('¬ª'):
            return ParagraphType.META

        # Description indicators
        description_score = self._score_descriptiveness(text)
        narrative_score = self._score_narrativeness(text)

        if description_score > narrative_score + 0.2:
            return ParagraphType.DESCRIPTION
        elif narrative_score > description_score + 0.2:
            return ParagraphType.NARRATIVE
        else:
            return ParagraphType.MIXED

    def _score_descriptiveness(self, text: str) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (0.0-1.0).
        """
        doc = nlp(text[:1000])  # Limit –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

        score = 0.0

        # –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö
        adj_count = sum(1 for t in doc if t.pos_ == "ADJ")
        noun_count = sum(1 for t in doc if t.pos_ == "NOUN")
        if noun_count > 0:
            adj_ratio = adj_count / noun_count
            score += min(0.3, adj_ratio * 0.5)

        # –ù–∞–ª–∏—á–∏–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–≤–µ—Ç–∞, —Ä–∞–∑–º–µ—Ä–∞, —Ñ–æ—Ä–º—ã
        visual_words = [
            "–±–µ–ª—ã–π", "—á–µ—Ä–Ω—ã–π", "—Å–µ—Ä—ã–π", "–∫—Ä–∞—Å–Ω—ã–π", "—Å–∏–Ω–∏–π", "–∑–µ–ª–µ–Ω—ã–π",
            "–±–æ–ª—å—à–æ–π", "–º–∞–ª–µ–Ω—å–∫–∏–π", "–æ–≥—Ä–æ–º–Ω—ã–π", "–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π",
            "–∫—Ä—É–≥–ª—ã–π", "–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π", "–æ—Å—Ç—Ä—ã–π", "–≥–ª–∞–¥–∫–∏–π"
        ]
        visual_count = sum(1 for w in visual_words if w in text.lower())
        score += min(0.25, visual_count * 0.05)

        # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã (–±—ã–ª, –∫–∞–∑–∞–ª—Å—è, –≤—ã–≥–ª—è–¥–µ–ª)
        descriptive_verbs = ["–±—ã–ª", "–±—ã–ª–∞", "–±—ã–ª–æ", "–±—ã–ª–∏", "–∫–∞–∑–∞–ª—Å—è",
                             "–∫–∞–∑–∞–ª–∞—Å—å", "–≤—ã–≥–ª—è–¥–µ–ª", "–Ω–∞–ø–æ–º–∏–Ω–∞–ª"]
        verb_count = sum(1 for v in descriptive_verbs if v in text.lower())
        score += min(0.2, verb_count * 0.05)

        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–≥–∏ (–Ω–∞–¥, –ø–æ–¥, –≤–æ–∫—Ä—É–≥)
        spatial_preps = ["–Ω–∞–¥", "–ø–æ–¥", "–≤–æ–∫—Ä—É–≥", "–º–µ–∂–¥—É", "—Ä—è–¥–æ–º",
                         "–≤–æ–∑–ª–µ", "—É", "–æ–∫–æ–ª–æ", "–ø–µ—Ä–µ–¥", "–∑–∞"]
        prep_count = sum(1 for p in spatial_preps if f" {p} " in text.lower())
        score += min(0.15, prep_count * 0.03)

        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≥–ª–∞–≥–æ–ª–æ–≤ –¥–µ–π—Å—Ç–≤–∏—è (—Å–Ω–∏–∂–∞–µ—Ç score)
        action_verbs = ["–ø–æ—à–µ–ª", "–ø–æ–±–µ–∂–∞–ª", "—Å—Ö–≤–∞—Ç–∏–ª", "–±—Ä–æ—Å–∏–ª—Å—è",
                        "–∑–∞–∫—Ä–∏—á–∞–ª", "—É–¥–∞—Ä–∏–ª", "–≤—ã—Å—Ç—Ä–µ–ª–∏–ª"]
        action_count = sum(1 for v in action_verbs if v in text.lower())
        score -= min(0.2, action_count * 0.05)

        return max(0.0, min(1.0, score))

    def _score_narrativeness(self, text: str) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏) –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞.
        """
        score = 0.0

        # –ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è –≤ –ø—Ä–æ—à–µ–¥—à–µ–º –≤—Ä–µ–º–µ–Ω–∏
        action_verbs = [
            "—Å–∫–∞–∑–∞–ª", "–ø–æ—à–µ–ª", "–≤–∑—è–ª", "–ø–æ—Å–º–æ—Ç—Ä–µ–ª", "–ø–æ–¥—É–º–∞–ª",
            "—Å—Ö–≤–∞—Ç–∏–ª", "–±—Ä–æ—Å–∏–ª—Å—è", "–æ—Ç–≤–µ—Ç–∏–ª", "—Å–ø—Ä–æ—Å–∏–ª", "–ø–æ–≤–µ—Ä–Ω—É–ª—Å—è"
        ]
        action_count = sum(1 for v in action_verbs if v in text.lower())
        score += min(0.4, action_count * 0.08)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        temporal_markers = [
            "–∑–∞—Ç–µ–º", "–ø–æ—Ç–æ–º", "–≤–¥—Ä—É–≥", "–≤–Ω–µ–∑–∞–ø–Ω–æ", "—Å–ø—É—Å—Ç—è",
            "—á–µ—Ä–µ–∑", "—Ç–æ–≥–¥–∞", "—Å–µ–π—á–∞—Å", "—Ç–µ–ø–µ—Ä—å"
        ]
        temporal_count = sum(1 for m in temporal_markers if m in text.lower())
        score += min(0.3, temporal_count * 0.1)

        # –ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö (–ø—Ä–∏–∑–Ω–∞–∫ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è)
        doc = nlp(text[:1000])
        adj_count = sum(1 for t in doc if t.pos_ == "ADJ")
        verb_count = sum(1 for t in doc if t.pos_ == "VERB")
        if verb_count > 0:
            verb_adj_ratio = verb_count / (adj_count + 1)
            if verb_adj_ratio > 2.0:
                score += 0.3

        return min(1.0, score)
```

---

## üìç STAGE 2: Description Boundary Detection

### –¶–µ–ª—å
–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å–º–µ–∂–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –æ–ø–∏—Å–∞–Ω–∏—è –≤ **–∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è** (500-3500 chars).

### Multi-Paragraph Window Analysis

```python
class DescriptionBoundaryDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä –≥—Ä–∞–Ω–∏—Ü –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π.
    """

    def detect_boundaries(
        self,
        paragraphs: List[Paragraph]
    ) -> List[CompleteDescription]:
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è.
        """
        descriptions = []
        i = 0

        while i < len(paragraphs):
            para = paragraphs[i]

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º dialog/meta paragraphs
            if para.type in [ParagraphType.DIALOG, ParagraphType.META]:
                i += 1
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ
            if para.score < 0.4:
                i += 1
                continue

            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            desc = self._extract_complete_description(paragraphs, i)
            if desc:
                descriptions.append(desc)
                i = desc.end_paragraph_idx + 1
            else:
                i += 1

        return descriptions

    def _extract_complete_description(
        self,
        paragraphs: List[Paragraph],
        start_idx: int
    ) -> Optional[CompleteDescription]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞—á–∏–Ω–∞—è —Å start_idx.
        """
        desc_paragraphs = [paragraphs[start_idx]]
        current_length = len(paragraphs[start_idx].text)

        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–∫–∞ –µ—Å—Ç—å continuation signals
        for i in range(start_idx + 1, len(paragraphs)):
            para = paragraphs[i]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –¥–æ–ª–∂–Ω—ã –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å?
            if not self._should_continue(para, desc_paragraphs):
                break

            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –¥–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ?
            if current_length + len(para.text) > 4000:
                break

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∫ –æ–ø–∏—Å–∞–Ω–∏—é
            desc_paragraphs.append(para)
            current_length += len(para.text)

            # –ú–∞–∫—Å–∏–º—É–º 15 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ –æ–¥–Ω–æ–º –æ–ø–∏—Å–∞–Ω–∏–∏
            if len(desc_paragraphs) >= 15:
                break

        # –í–∞–ª–∏–¥–∞—Ü–∏—è: –º–∏–Ω–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤
        full_text = '\n\n'.join(p.text for p in desc_paragraphs)
        if len(full_text) < 500:
            return None

        # –°–æ–∑–¥–∞–µ–º CompleteDescription
        return CompleteDescription(
            paragraphs=desc_paragraphs,
            text=full_text,
            start_paragraph_idx=start_idx,
            end_paragraph_idx=start_idx + len(desc_paragraphs) - 1,
            length_chars=len(full_text),
            length_category=self._categorize_length(len(full_text))
        )

    def _should_continue(
        self,
        para: Paragraph,
        current_description: List[Paragraph]
    ) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ.
        """
        # End signals - –ù–ï –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å
        if para.type == ParagraphType.DIALOG:
            return False

        if para.type == ParagraphType.META:
            return False

        # –Ø–≤–Ω—ã–π –∫–æ–Ω–µ—Ü –æ–ø–∏—Å–∞–Ω–∏—è
        end_patterns = [
            r'^(–ó–∞—Ç–µ–º|–ü–æ—Ç–æ–º|–í–¥—Ä—É–≥|–í–Ω–µ–∑–∞–ø–Ω–æ|–û–¥–Ω–∞–∫–æ|–ù–æ)\s',
            r'\b(—Å–∫–∞–∑–∞–ª|–ø–æ–¥—É–º–∞–ª|–ø–æ—à–µ–ª|–ø–æ–≤–µ—Ä–Ω—É–ª—Å—è|–±—Ä–æ—Å–∏–ª—Å—è)\b',
        ]
        para_start = para.text[:100].strip()
        if any(re.search(p, para_start) for p in end_patterns):
            return False

        # Continuation signals - –ü–†–û–î–û–õ–ñ–ê–¢–¨

        # –ü–∞—Ä–∞–≥—Ä–∞—Ñ —Ç–æ–∂–µ descriptive
        if para.score >= 0.5:
            return True

        # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        prev_entities = self._extract_entities(current_description[-1].text)
        curr_entities = self._extract_entities(para.text[:200])
        if len(set(prev_entities) & set(curr_entities)) > 0:
            return True

        # Spatial continuity (–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏)
        spatial_continuers = [
            "–Ω–∞–¥", "–ø–æ–¥", "—Ä—è–¥–æ–º", "–≤–æ–∑–ª–µ", "–æ–∫–æ–ª–æ", "–≤–Ω—É—Ç—Ä–∏",
            "—Å–Ω–∞—Ä—É–∂–∏", "–≤—ã—à–µ", "–Ω–∏–∂–µ", "–¥–∞–ª—å—à–µ", "–±–ª–∏–∂–µ"
        ]
        if any(w in para.text[:100].lower() for w in spatial_continuers):
            return True

        # Default: –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å
        return False

    def _categorize_length(self, length: int) -> str:
        """
        –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª–∏–Ω—ã –æ–ø–∏—Å–∞–Ω–∏—è.
        """
        if 500 <= length < 1000:
            return "medium"
        elif 1000 <= length < 2000:
            return "long"
        elif 2000 <= length < 3500:
            return "very_long"
        else:
            return "ultra_long"  # > 3500
```

### –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π

```python
def prioritize_by_length(self, descriptions: List[CompleteDescription]):
    """
    –°–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª–∏–Ω–Ω—ã–º.

    –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞:
    1. VERY_LONG (2000-3500) - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    2. LONG (1000-2000)      - –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    3. MEDIUM (500-1000)     - —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    4. < 500 chars           - –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å)
    """
    priority_map = {
        "very_long": 100,
        "long": 80,
        "medium": 60,
    }

    for desc in descriptions:
        length_priority = priority_map.get(desc.length_category, 40)

        # Bonus –∑–∞ descriptiveness
        avg_score = sum(p.score for p in desc.paragraphs) / len(desc.paragraphs)
        descriptiveness_bonus = avg_score * 20

        desc.priority = length_priority + descriptiveness_bonus

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é priority
    descriptions.sort(key=lambda d: d.priority, reverse=True)

    return descriptions
```

---

## üß† STAGE 3: Deep NLP Analysis (–£–ª—É—á—à–µ–Ω–Ω–∞—è Multi-NLP)

### –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

**–ü—Ä–æ–±–ª–µ–º–∞:** SpaCy, Natasha, Stanza –∏–º–µ—é—Ç –ª–∏–º–∏—Ç—ã –Ω–∞ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞.

**–†–µ—à–µ–Ω–∏–µ:** Chunked analysis —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

```python
class LongTextNLPProcessor:
    """
    NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π (500-3500 chars).
    """

    def __init__(self):
        self.spacy_nlp = spacy.load("ru_core_news_lg")
        self.natasha_processor = EnhancedNatashaProcessor()
        self.stanza_processor = EnhancedStanzaProcessor()

        # Limits –¥–ª—è –º–æ–¥–µ–ª–µ–π
        self.SPACY_LIMIT = 1000000  # SpaCy ok
        self.NATASHA_LIMIT = 5000    # Natasha –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞
        self.STANZA_LIMIT = 10000    # Stanza ok —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏

    def analyze_long_description(
        self,
        text: str,
        desc_type_hint: Optional[DescriptionType] = None
    ) -> NLPAnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (500-3500 chars).
        """
        # Chunk text –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        chunks = self._chunk_text(text, max_size=2000)

        # Analyze each chunk
        chunk_results = []
        for chunk in chunks:
            result = self._analyze_chunk(chunk)
            chunk_results.append(result)

        # Aggregate results
        aggregated = self._aggregate_chunk_results(chunk_results)

        # Type classification (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω hint)
        if not desc_type_hint:
            aggregated.type = self._classify_type_ensemble(aggregated)
        else:
            aggregated.type = desc_type_hint

        # Calculate confidence
        aggregated.confidence = self._calculate_confidence(aggregated, text)

        return aggregated

    def _chunk_text(self, text: str, max_size: int) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ chunks –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """
        doc = self.spacy_nlp(text)
        chunks = []
        current_chunk = []
        current_length = 0

        for sent in doc.sents:
            sent_len = len(sent.text)

            if current_length + sent_len > max_size and current_chunk:
                # –ó–∞–≤–µ—Ä—à–∏—Ç—å —Ç–µ–∫—É—â–∏–π chunk
                chunks.append(' '.join(current_chunk))
                current_chunk = [sent.text]
                current_length = sent_len
            else:
                current_chunk.append(sent.text)
                current_length += sent_len

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def _analyze_chunk(self, chunk: str) -> ChunkAnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω chunk —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤.
        """
        result = ChunkAnalysisResult()

        # SpaCy analysis
        spacy_result = self._analyze_with_spacy(chunk)
        result.entities.extend(spacy_result.entities)
        result.pos_tags.extend(spacy_result.pos_tags)
        result.dependencies.extend(spacy_result.dependencies)

        # Natasha analysis (–µ—Å–ª–∏ chunk –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)
        if len(chunk) <= self.NATASHA_LIMIT:
            natasha_result = self.natasha_processor.analyze(chunk)
            result.entities.extend(natasha_result.entities)

        # Stanza analysis (–¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞)
        if len(chunk) <= self.STANZA_LIMIT:
            stanza_result = self.stanza_processor.analyze(chunk)
            result.dependencies.extend(stanza_result.dependencies)

        return result

    def _aggregate_chunk_results(
        self,
        chunk_results: List[ChunkAnalysisResult]
    ) -> NLPAnalysisResult:
        """
        –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö chunks.
        """
        aggregated = NLPAnalysisResult()

        # Merge entities (deduplicate)
        all_entities = []
        for cr in chunk_results:
            all_entities.extend(cr.entities)

        # Deduplicate –ø–æ —Ç–µ–∫—Å—Ç—É entity
        seen = set()
        for entity in all_entities:
            if entity.text.lower() not in seen:
                aggregated.entities.append(entity)
                seen.add(entity.text.lower())

        # Aggregate POS tags (count)
        pos_counter = Counter()
        for cr in chunk_results:
            for tag in cr.pos_tags:
                pos_counter[tag] += 1
        aggregated.pos_distribution = dict(pos_counter)

        # Aggregate dependencies (sample –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
        aggregated.dependencies = chunk_results[0].dependencies[:50]

        return aggregated

    def _calculate_confidence(
        self,
        analysis: NLPAnalysisResult,
        original_text: str
    ) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π confidence –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è.

        –§–∞–∫—Ç–æ—Ä—ã:
        1. –í–∏–∑—É–∞–ª—å–Ω–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å (35%)
        2. –õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (30%)
        3. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ—Å—Ç—å (20%)
        4. –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å —Ç–∏–ø–∞ (15%)
        """
        # Factor 1: Visual richness
        visual_score = self._score_visual_richness(analysis, original_text)

        # Factor 2: Linguistic quality
        linguistic_score = self._score_linguistic_quality(analysis)

        # Factor 3: Structural completeness
        structure_score = self._score_structure(original_text)

        # Factor 4: Type specificity
        type_score = self._score_type_specificity(analysis, original_text)

        confidence = (
            visual_score * 0.35 +
            linguistic_score * 0.30 +
            structure_score * 0.20 +
            type_score * 0.15
        )

        return confidence
```

---

## üé® STAGE 5: Unified Prompt Generation System

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã

```python
class UnifiedPromptGenerator:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.
    """

    def __init__(self):
        self.formatters = {
            "stable_diffusion_3": SD3PromptFormatter(),
            "dalle_3": DALLE3PromptFormatter(),
            "gemini_imagen_3": GeminiPromptFormatter(),
            "flux": FluxPromptFormatter(),
            "midjourney": MidjourneyPromptFormatter(),
        }

    def generate_prompts(
        self,
        description: CompleteDescription,
        models: List[str]
    ) -> Dict[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

        Args:
            description: –ò–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ["stable_diffusion_3", "dalle_3", ...]

        Returns:
            Dict —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        """
        # Step 1: Create base prompt (model-agnostic)
        base_prompt = self._create_base_prompt(description)

        # Step 2: Format –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        prompts = {}
        for model in models:
            formatter = self.formatters.get(model)
            if formatter:
                prompts[model] = formatter.format(base_prompt, description)

        return prompts

    def _create_base_prompt(
        self,
        description: CompleteDescription
    ) -> BasePrompt:
        """
        –°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç (model-agnostic).
        """
        base = BasePrompt()

        # Core content
        base.description_text = description.text

        # Metadata
        base.description_type = description.type  # LOCATION/CHARACTER/ATMOSPHERE
        base.length = description.length_chars
        base.genre = description.genre  # fantasy, detective, romance, etc.

        # Extract visual elements
        base.visual_elements = self._extract_visual_elements(description)

        # Extract entities
        base.entities = description.nlp_result.entities

        # Extract mood/atmosphere
        base.mood = self._extract_mood(description)

        # Context from previous chapters
        base.context = description.cross_chapter_context

        return base

    def _extract_visual_elements(
        self,
        description: CompleteDescription
    ) -> VisualElements:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è.
        """
        elements = VisualElements()
        text = description.text.lower()

        # Colors
        color_patterns = {
            "–±–µ–ª—ã–π": "white", "—á–µ—Ä–Ω—ã–π": "black", "—Å–µ—Ä—ã–π": "gray",
            "–∫—Ä–∞—Å–Ω—ã–π": "red", "—Å–∏–Ω–∏–π": "blue", "–∑–µ–ª–µ–Ω—ã–π": "green",
            "–∂–µ–ª—Ç—ã–π": "yellow", "–∑–æ–ª–æ—Ç–æ–π": "golden", "—Å–µ—Ä–µ–±—Ä—è–Ω—ã–π": "silver"
        }
        for ru, en in color_patterns.items():
            if ru in text:
                elements.colors.append(en)

        # Lighting
        lighting_keywords = ["—Å–≤–µ—Ç", "—Ç–µ–Ω—å", "—Å—É–º—Ä–∞–∫", "—Å–æ–ª–Ω—Ü–µ", "–ª—É–Ω–∞",
                             "—è—Ä–∫–∏–π", "—Ç—É—Å–∫–ª—ã–π", "–æ—Å–≤–µ—â–µ–Ω–Ω—ã–π"]
        elements.lighting = [kw for kw in lighting_keywords if kw in text]

        # Weather (–¥–ª—è –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã)
        weather_keywords = ["–¥–æ–∂–¥—å", "—Å–Ω–µ–≥", "–≤–µ—Ç–µ—Ä", "—Ç—É–º–∞–Ω", "–æ–±–ª–∞–∫–∞"]
        elements.weather = [kw for kw in weather_keywords if kw in text]

        # Time of day
        time_keywords = {"—É—Ç—Ä–æ": "morning", "–¥–µ–Ω—å": "day", "–≤–µ—á–µ—Ä": "evening",
                         "–Ω–æ—á—å": "night", "—Ä–∞—Å—Å–≤–µ—Ç": "dawn", "–∑–∞–∫–∞—Ç": "sunset"}
        for ru, en in time_keywords.items():
            if ru in text:
                elements.time_of_day = en
                break

        # Architecture (–¥–ª—è –ª–æ–∫–∞—Ü–∏–π)
        arch_keywords = ["–∑–∞–º–æ–∫", "–¥–≤–æ—Ä–µ—Ü", "–±–∞—à–Ω—è", "–∫—Ä–µ–ø–æ—Å—Ç—å", "–∑–¥–∞–Ω–∏–µ",
                         "–¥–æ–º", "—Ö—Ä–∞–º", "—Å–æ–±–æ—Ä"]
        elements.architecture = [kw for kw in arch_keywords if kw in text]

        return elements
```

### Model-Specific Formatters

#### 1. Stable Diffusion 3 Formatter

```python
class SD3PromptFormatter:
    """
    Formatter –¥–ª—è Stable Diffusion 3.

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–æ 10,000 —Å–∏–º–≤–æ–ª–æ–≤
    - –î–µ—Ç–∞–ª—å–Ω—ã–µ natural language –æ–ø–∏—Å–∞–Ω–∏—è
    - Negative prompts –ù–ï —Ä–∞–±–æ—Ç–∞—é—Ç (–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)
    - Style/composition —É–∫–∞–∑–∞–Ω–∏—è
    """

    def format(
        self,
        base_prompt: BasePrompt,
        description: CompleteDescription
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è SD3.
        """
        parts = []

        # 1. Subject (–≥–ª–∞–≤–Ω—ã–π –æ–±—ä–µ–∫—Ç/—Å—Ü–µ–Ω–∞)
        subject = self._format_subject(base_prompt)
        parts.append(subject)

        # 2. Setting & Composition
        setting = self._format_setting(base_prompt)
        parts.append(setting)

        # 3. Lighting & Atmosphere
        lighting = self._format_lighting(base_prompt)
        parts.append(lighting)

        # 4. Details & Textures
        details = self._format_details(base_prompt)
        parts.append(details)

        # 5. Style directive
        style = self._format_style(base_prompt)
        parts.append(style)

        # Combine
        prompt = ', '.join(parts)

        # Add quality modifiers
        prompt += ", highly detailed, 4K resolution, professional artwork"

        return prompt

    def _format_subject(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≥–ª–∞–≤–Ω—ã–π —Å—É–±—ä–µ–∫—Ç.
        """
        if base.description_type == DescriptionType.CHARACTER:
            # Extract character description (first 500 chars)
            char_desc = base.description_text[:500]
            return f"Character portrait: {char_desc}"

        elif base.description_type == DescriptionType.LOCATION:
            # Extract location name/type
            location_type = self._identify_location_type(base)
            return f"{location_type}"

        elif base.description_type == DescriptionType.ATMOSPHERE:
            # Focus on mood
            return f"Atmospheric scene with {base.mood} mood"

        return "Scene"

    def _format_setting(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç setting/composition.
        """
        elements = []

        # Architecture
        if base.visual_elements.architecture:
            arch = ", ".join(base.visual_elements.architecture[:3])
            elements.append(f"featuring {arch}")

        # Environment
        if base.description_type == DescriptionType.LOCATION:
            env = self._extract_environment(base.description_text)
            if env:
                elements.append(env)

        # Composition hint
        if base.description_type == DescriptionType.CHARACTER:
            elements.append("medium shot, portrait composition")
        elif base.description_type == DescriptionType.LOCATION:
            elements.append("wide-angle view, establishing shot")

        return ', '.join(elements) if elements else ""

    def _format_lighting(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Å–≤–µ—â–µ–Ω–∏–µ –∏ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É.
        """
        elements = []

        # Time of day
        if base.visual_elements.time_of_day:
            elements.append(f"{base.visual_elements.time_of_day} light")

        # Weather
        if base.visual_elements.weather:
            weather = ", ".join(base.visual_elements.weather[:2])
            elements.append(weather)

        # Lighting keywords
        if base.visual_elements.lighting:
            lighting = base.visual_elements.lighting[0]
            elements.append(f"{lighting} lighting")

        # Fallback
        if not elements:
            elements.append("natural lighting")

        return ', '.join(elements)

    def _format_details(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª–∏ –∏ —Ç–µ–∫—Å—Ç—É—Ä—ã.
        """
        elements = []

        # Colors
        if base.visual_elements.colors:
            colors = ", ".join(base.visual_elements.colors[:4])
            elements.append(f"colors: {colors}")

        # Textures (extract from text)
        textures = self._extract_textures(base.description_text)
        if textures:
            elements.append(textures)

        # Materials
        materials = self._extract_materials(base.description_text)
        if materials:
            elements.append(materials)

        return ', '.join(elements) if elements else "detailed textures"

    def _format_style(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç style directive.
        """
        if base.genre == "fantasy":
            return "epic fantasy art style, painterly, dramatic"
        elif base.genre == "detective":
            return "noir style, cinematic, mysterious atmosphere"
        elif base.genre == "romance":
            return "romantic painting style, soft focus, emotional"
        else:
            return "realistic illustration style, detailed"
```

#### 2. DALL-E 3 Formatter

```python
class DALLE3PromptFormatter:
    """
    Formatter –¥–ª—è DALL-E 3.

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - GPT-4 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã (–º–æ–∂–Ω–æ –±—ã—Ç—å –º–µ–Ω–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º)
    - –ú–∞–∫—Å–∏–º—É–º 4000 —Å–∏–º–≤–æ–ª–æ–≤
    - –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç structured prompts
    - –•–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç natural language
    """

    def format(
        self,
        base_prompt: BasePrompt,
        description: CompleteDescription
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è DALL-E 3.
        """
        # DALL-E 3 works well with natural storytelling
        # Use description text directly with minor enhancements

        prompt_parts = []

        # 1. Opening (context setting)
        opening = self._create_opening(base_prompt)
        prompt_parts.append(opening)

        # 2. Main description (first 2000 chars)
        main_desc = base_prompt.description_text[:2000]
        prompt_parts.append(main_desc)

        # 3. Style/mood enhancement
        enhancement = self._create_enhancement(base_prompt)
        prompt_parts.append(enhancement)

        # Combine
        prompt = ' '.join(prompt_parts)

        # Truncate –∫ 4000 chars –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(prompt) > 4000:
            prompt = prompt[:3997] + "..."

        return prompt

    def _create_opening(self, base: BasePrompt) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç opening statement –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        """
        if base.description_type == DescriptionType.CHARACTER:
            return "Create a detailed portrait:"
        elif base.description_type == DescriptionType.LOCATION:
            return "Visualize this location:"
        elif base.description_type == DescriptionType.ATMOSPHERE:
            return "Capture this atmospheric scene:"
        return "Create an image of:"

    def _create_enhancement(self, base: BasePrompt) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç enhancement —Å style/quality directives.
        """
        enhancements = []

        # Genre-specific style
        if base.genre:
            enhancements.append(f"in {base.genre} genre style")

        # Quality
        enhancements.append("highly detailed, photorealistic quality")

        # Mood
        if base.mood:
            enhancements.append(f"with {base.mood} mood")

        return ", ".join(enhancements)
```

#### 3. Gemini Imagen 3 Formatter

```python
class GeminiPromptFormatter:
    """
    Formatter –¥–ª—è Gemini Imagen 3.

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - Descriptive language —Å adjectives/adverbs
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ: Subject + Style + Context
    - –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Ö–æ—Ä–æ—à–æ
    - Iteration-friendly (–º–æ–∂–Ω–æ —É—Ç–æ—á–Ω—è—Ç—å)
    """

    def format(
        self,
        base_prompt: BasePrompt,
        description: CompleteDescription
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Gemini Imagen 3.
        """
        # Gemini –ª—é–±–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å —è–≤–Ω—ã–º
        # —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ Subject, Style, Details

        parts = []

        # 1. Subject with detailed adjectives
        subject = self._format_subject_with_adjectives(base_prompt)
        parts.append(f"Subject: {subject}")

        # 2. Style directive
        style = self._format_style_directive(base_prompt)
        parts.append(f"Style: {style}")

        # 3. Composition & framing
        composition = self._format_composition(base_prompt)
        parts.append(f"Composition: {composition}")

        # 4. Lighting & color
        lighting = self._format_lighting_color(base_prompt)
        parts.append(f"Lighting: {lighting}")

        # 5. Mood & atmosphere
        mood = self._format_mood_atmosphere(base_prompt)
        parts.append(f"Atmosphere: {mood}")

        # 6. Additional details
        details = self._extract_key_details(base_prompt)
        if details:
            parts.append(f"Details: {details}")

        # Combine
        prompt = '. '.join(parts)

        return prompt

    def _format_subject_with_adjectives(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç subject —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–º–∏.
        """
        # Extract adjectives from description
        adjectives = self._extract_adjectives(base.description_text[:500])

        if base.description_type == DescriptionType.CHARACTER:
            return f"A {', '.join(adjectives[:5])} character"
        elif base.description_type == DescriptionType.LOCATION:
            location_type = self._identify_location_type(base)
            return f"A {', '.join(adjectives[:5])} {location_type}"
        else:
            return f"A {', '.join(adjectives[:3])} scene"

    def _format_composition(self, base: BasePrompt) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç composition —Å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏.
        """
        if base.description_type == DescriptionType.CHARACTER:
            return "medium shot, 85mm portrait lens, shallow depth of field"
        elif base.description_type == DescriptionType.LOCATION:
            return "wide-angle shot, 24mm lens, deep focus, establishing view"
        else:
            return "balanced composition, rule of thirds"
```

#### 4. Flux Formatter

```python
class FluxPromptFormatter:
    """
    Formatter –¥–ª—è Flux (pollinations.ai).

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - Natural language, –∫–∞–∫ –∏–∑ –∫–Ω–∏–≥–∏
    - 100-1000+ —Å–∏–º–≤–æ–ª–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ
    - –ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
    - –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç narrative descriptions
    """

    def format(
        self,
        base_prompt: BasePrompt,
        description: CompleteDescription
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Flux.
        """
        # Flux works best with natural literary language
        # Use the original description with minimal processing

        prompt_parts = []

        # 1. Use first 800 chars of description
        main_text = base_prompt.description_text[:800]
        prompt_parts.append(main_text)

        # 2. Add quality hint
        prompt_parts.append("Detailed illustration.")

        # Combine
        prompt = ' '.join(prompt_parts)

        # Optimal: 100-1000 chars
        if len(prompt) > 1000:
            prompt = prompt[:997] + "..."

        return prompt
```

### Usage Example

```python
# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã

# 1. –£ –Ω–∞—Å –µ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
description = CompleteDescription(
    text="–ó–∞–º–æ–∫ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –∫—Ä—É—Ç–æ–≥–æ —Ö–æ–ª–º–∞...",  # 2500 chars
    type=DescriptionType.LOCATION,
    length_chars=2500,
    genre="fantasy",
    # ... –¥—Ä—É–≥–∏–µ –ø–æ–ª—è
)

# 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
generator = UnifiedPromptGenerator()
prompts = generator.generate_prompts(
    description=description,
    models=["stable_diffusion_3", "dalle_3", "gemini_imagen_3", "flux"]
)

# 3. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã
sd3_prompt = prompts["stable_diffusion_3"]
# ‚Üí "Medieval castle, featuring towers, fortress walls, wide-angle view,
#     sunset light, dramatic clouds, colors: gray, golden, red,
#     detailed stone textures, epic fantasy art style, highly detailed, 4K"

dalle3_prompt = prompts["dalle_3"]
# ‚Üí "Visualize this location: –ó–∞–º–æ–∫ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –∫—Ä—É—Ç–æ–≥–æ —Ö–æ–ª–º–∞,
#     –µ–≥–æ –º–∞—Å—Å–∏–≤–Ω—ã–µ —Å—Ç–µ–Ω—ã –∏–∑ —Å–µ—Ä–æ–≥–æ –≥—Ä–∞–Ω–∏—Ç–∞ –≤–æ–∑–≤—ã—à–∞–ª–∏—Å—å –Ω–∞–¥ –æ–∫—Ä—É–∂–∞—é—â–∏–º –ª–µ—Å–æ–º...
#     in fantasy genre style, highly detailed, with dramatic mood"

gemini_prompt = prompts["gemini_imagen_3"]
# ‚Üí "Subject: A majestic, towering, ancient, granite castle.
#     Style: Epic fantasy illustration. Composition: wide-angle shot, 24mm lens.
#     Lighting: golden sunset light, dramatic shadows. Atmosphere: grand, imposing.
#     Details: stone textures, towers, forest"

flux_prompt = prompts["flux"]
# ‚Üí "–ó–∞–º–æ–∫ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –∫—Ä—É—Ç–æ–≥–æ —Ö–æ–ª–º–∞, –µ–≥–æ –º–∞—Å—Å–∏–≤–Ω—ã–µ —Å—Ç–µ–Ω—ã
#     –∏–∑ —Å–µ—Ä–æ–≥–æ –≥—Ä–∞–Ω–∏—Ç–∞ –≤–æ–∑–≤—ã—à–∞–ª–∏—Å—å –Ω–∞–¥ –æ–∫—Ä—É–∂–∞—é—â–∏–º –ª–µ—Å–æ–º, —Å–ª–æ–≤–Ω–æ –∫–∞–º–µ–Ω–Ω—ã–π
#     —Å—Ç—Ä–∞–∂... Detailed illustration."

# 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–º–ø—Ç—ã –≤ –ë–î
description.prompts = prompts
await save_to_database(description)
```

---

## üìä –ù–æ–≤–∞—è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Thresholds

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π

```python
# NEW CONFIGURATION - Optimized for Long Descriptions (500-3500 chars)
ADVANCED_DESCRIPTION_EXTRACTION_CONFIG = {
    # ====================================================================
    # LENGTH CONSTRAINTS - REVOLUTIONARY CHANGE
    # ====================================================================
    "min_char_length": 500,           # ‚Üê –ú–∏–Ω–∏–º—É–º 500 (–±—ã–ª–æ 50!)
    "max_char_length": 4000,          # ‚Üê –ú–∞–∫—Å–∏–º—É–º 4000 (–±—ã–ª–æ 1000)
    "optimal_char_length": (1000, 2500),  # ‚Üê Sweet spot (–±—ã–ª–æ 150-500)

    # Priority zones
    "length_priority_zones": {
        "very_long": (2000, 3500),    # –í—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - 100 pts
        "long": (1000, 2000),         # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - 80 pts
        "medium": (500, 1000),        # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - 60 pts
        "short": (100, 500),          # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - 40 pts (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å)
    },

    # ====================================================================
    # QUALITY THRESHOLDS
    # ====================================================================
    "min_confidence_score": 0.60,     # ‚Üê Raised (–±—ã–ª–æ 0.50)
    "optimal_confidence_score": 0.75, # ‚Üê Target quality
    "min_visual_richness": 0.40,      # ‚Üê Require rich visual vocab

    # Multi-paragraph requirements
    "min_paragraphs": 2,               # ‚Üê –ú–∏–Ω–∏–º—É–º 2 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
    "max_paragraphs": 15,              # ‚Üê –ú–∞–∫—Å–∏–º—É–º 15 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    "prefer_multi_paragraph": True,

    # ====================================================================
    # TYPE-SPECIFIC THRESHOLDS
    # ====================================================================
    "location_min_confidence": 0.60,
    "location_min_length": 800,        # Locations –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏
    "location_optimal": (1500, 3000),  # –î–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –ª–æ–∫–∞—Ü–∏–π

    "character_min_confidence": 0.55,
    "character_min_length": 600,       # Character descriptions
    "character_optimal": (1000, 2000),

    "atmosphere_min_confidence": 0.50,
    "atmosphere_min_length": 500,      # Atmospheric descriptions
    "atmosphere_optimal": (800, 1500),

    # ====================================================================
    # FILTERING CONFIGURATION
    # ====================================================================
    "strict_anti_pattern_filtering": True,
    "filter_incomplete_sentences": True,
    "filter_dialog": True,
    "filter_meta_text": True,
    "filter_fragments": True,           # NEW: —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ–±—Ä—ã–≤–∫–∏

    # Maximum tolerable issues
    "max_dialog_ratio": 0.1,            # < 10% –¥–∏–∞–ª–æ–≥–∞ allowed
    "max_fragment_ratio": 0.05,         # < 5% —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤

    # ====================================================================
    # MODEL-SPECIFIC SETTINGS
    # ====================================================================
    "model_settings": {
        "stable_diffusion_3": {
            "preferred_length": (1000, 3500),
            "max_length": 10000,         # SD3 supports up to 10K!
            "style": "detailed_natural_language"
        },
        "dalle_3": {
            "preferred_length": (800, 2500),
            "max_length": 4000,
            "style": "narrative_descriptive"
        },
        "gemini_imagen_3": {
            "preferred_length": (600, 2000),
            "max_length": None,  # No limit
            "style": "structured_detailed"
        },
        "flux": {
            "preferred_length": (500, 1500),
            "max_length": 2000,
            "style": "literary_natural"
        }
    },

    # ====================================================================
    # PERFORMANCE SETTINGS
    # ====================================================================
    "chunk_size_for_nlp": 2000,         # Chunk size –¥–ª—è NLP analysis
    "parallel_processing": True,         # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤
    "max_processing_time_per_chapter": 30,  # 30 —Å–µ–∫—É–Ω–¥ max (acceptable)
}
```

### –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è Priority Score Formula

```python
def calculate_advanced_priority_score(self, description: CompleteDescription) -> float:
    """
    –ù–û–í–ê–Ø –§–û–†–ú–£–õ–ê –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –î–õ–ò–ù–ù–´–ï –æ–ø–∏—Å–∞–Ω–∏—è.

    Weights:
    - Length (50%)      ‚Üê –°–∞–º—ã–π –≤–∞–∂–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä!
    - Type (20%)        ‚Üê –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–∏–ø–∞
    - Confidence (20%)  ‚Üê –ö–∞—á–µ—Å—Ç–≤–æ
    - Visual (10%)      ‚Üê –í–∏–∑—É–∞–ª—å–Ω–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
    """
    if not description.is_suitable_for_generation:
        return 0.0

    # ================================================================
    # LENGTH SCORE (50% weight) - REVOLUTIONARY PRIORITY
    # ================================================================
    length = description.length_chars
    length_score = 0.0

    if 2000 <= length <= 3500:
        # VERY LONG - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        length_score = 100.0
    elif 1000 <= length < 2000:
        # LONG - –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        length_score = 80.0
    elif 500 <= length < 1000:
        # MEDIUM - —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        length_score = 60.0
    elif 300 <= length < 500:
        # SHORT - –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        length_score = 40.0
    else:
        # < 300 - —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å
        length_score = 0.0

    # ================================================================
    # TYPE SCORE (20% weight)
    # ================================================================
    type_priorities = {
        DescriptionType.LOCATION: 75,
        DescriptionType.CHARACTER: 60,
        DescriptionType.ATMOSPHERE: 45,
        DescriptionType.OBJECT: 30,
    }
    type_score = type_priorities.get(description.type, 30)

    # ================================================================
    # CONFIDENCE SCORE (20% weight)
    # ================================================================
    confidence_score = description.confidence * 100  # 0.0-1.0 ‚Üí 0-100

    # ================================================================
    # VISUAL RICHNESS SCORE (10% weight)
    # ================================================================
    visual_score = description.visual_richness_score * 100

    # ================================================================
    # WEIGHTED SUM
    # ================================================================
    final_score = (
        length_score * 0.50 +      # 50% - LENGTH IS KING
        type_score * 0.20 +        # 20% - type priority
        confidence_score * 0.20 +  # 20% - quality
        visual_score * 0.10        # 10% - visual richness
    )

    return min(100.0, final_score)
```

---

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ)

### –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –£–ª—É—á—à–µ–Ω–∏—è

#### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
```
–î–û (—Ç–µ–∫—É—â–µ–µ):
  OBJECT:      672 (53.5%)  ‚ùå
  LOCATION:    503 (40.0%)
  CHARACTER:    61 (4.9%)   ‚ùå
  ATMOSPHERE:   21 (1.7%)   ‚ùå

–ü–û–°–õ–ï (—Ü–µ–ª–µ–≤–æ–µ):
  LOCATION:    ~600 (48%)   ‚úÖ +97, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
  CHARACTER:   ~400 (32%)   ‚úÖ +339, –û–ì–†–û–ú–ù–û–ï —É–ª—É—á—à–µ–Ω–∏–µ
  ATMOSPHERE:  ~200 (16%)   ‚úÖ +179, –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ
  OBJECT:       ~57 (4%)    ‚úÖ -615, –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
```

#### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω (–†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï)
```
–î–û (—Ç–µ–∫—É—â–µ–µ):
  –°—Ä–µ–¥–Ω–µ–µ:     104.7 chars  ‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
  –ú–µ–¥–∏–∞–Ω–∞:     102 chars    ‚ùå
  < 100 chars: 587 (46.7%)  ‚ùå –ü–æ–ª–æ–≤–∏–Ω–∞ –º—É—Å–æ—Ä
  100-500:     670 (53.3%)  ‚ö†Ô∏è –ö–æ—Ä–æ—Ç–∫–∏–µ
  500-1000:    0 (0.0%)     ‚ùå –ù–ï–¢ —Å—Ä–µ–¥–Ω–∏—Ö
  1000-2000:   0 (0.0%)     ‚ùå –ù–ï–¢ –¥–ª–∏–Ω–Ω—ã—Ö
  2000-3500:   0 (0.0%)     ‚ùå –ù–ï–¢ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö
  > 3500:      0 (0.0%)     ‚ùå

–ü–û–°–õ–ï (—Ü–µ–ª–µ–≤–æ–µ):
  –°—Ä–µ–¥–Ω–µ–µ:     ~1500 chars  ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
  –ú–µ–¥–∏–∞–Ω–∞:     ~1300 chars  ‚úÖ
  < 500 chars: <30 (< 3%)   ‚úÖ –î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–æ
  500-1000:    ~200 (15%)   ‚úÖ –°—Ä–µ–¥–Ω–∏–µ (Flux optimal)
  1000-2000:   ~500 (40%)   ‚úÖ –î–ª–∏–Ω–Ω—ã–µ (DALL-E/Gemini optimal)
  2000-3500:   ~450 (36%)   ‚úÖ –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ (SD3 excellent)
  > 3500:      ~70 (6%)     ‚úÖ Ultra-long (—Ä–∞–∑–±–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
```

#### –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
```
–î–û:
  Top 10 confidence: –ú—É—Å–æ—Ä (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã)
  Avg confidence:    0.45
  Complete desc:     ~30%
  Visualizable:      ~20%
  Suitable –¥–ª—è SD3:  0%      ‚ùå

–ü–û–°–õ–ï:
  Top 10 confidence: –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
  Avg confidence:    0.72    ‚úÖ –í—ã—à–µ threshold
  Complete desc:     >98%    ‚úÖ Boundary detection
  Visualizable:      >85%    ‚úÖ Visual richness scoring
  Suitable –¥–ª—è SD3:  >80%    ‚úÖ –î–ª–∏–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
```

### –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –£–ª—É—á—à–µ–Ω–∏—è

#### –ü—Ä–∏–º–µ—Ä 1: –ö–û–†–û–¢–ö–û–ï (—Ç–µ–∫—É—â–∞—è —Å–∏—Å—Ç–µ–º–∞) ‚ùå
```
–î–ª–∏–Ω–∞: 104 chars
Type: LOCATION
Confidence: 0.95 (–õ–û–ñ–ù–û –í–´–°–û–ö–ò–ô!)

"–º–∞—Ä—Ö–∏–∏. –ú–∞—Ä–∫–≥—Ä–∞—Ñ—Å—Ç–≤ —ç—Ç–∏—Ö —á–µ—Ç—ã—Ä–µ: –ó–∞–ø–∞–¥–Ω–æ–µ, –í–µ—Ä—Ö–Ω–µ–µ, –û–∑—ë—Ä–Ω–æ–µ –∏ –ù–∏–∂–Ω–µ–µ. –ë–æ–ª–¥—É–∏–Ω –ê–¥–æ–≤–∞—Ä–¥–æ, ¬´–ù–æ–≤–æ–µ"

–ü—Ä–æ–±–ª–µ–º—ã:
‚ùå –û–±—Ä—ã–≤–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã)
‚ùå –°–ø—Ä–∞–≤–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ
‚ùå –ù–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º–æ
‚ùå Confidence 0.95 - —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ù–ê–û–ë–û–†–û–¢
```

#### –ü—Ä–∏–º–µ—Ä 2: –î–õ–ò–ù–ù–û–ï (–Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞) ‚úÖ‚úÖ
```
–î–ª–∏–Ω–∞: 2347 chars
Type: LOCATION
Confidence: 0.89
Priority: 95.2

"–ó–∞–º–æ–∫ –ö–∞—ç—Ä –ú–æ—Ä—Ö–µ–Ω —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è –≤ —Å–∞–º–æ–º —Å–µ—Ä–¥—Ü–µ –î—Ä–∞–∫–æ–Ω—å–∏—Ö –≥–æ—Ä, –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ
–∫—Ä—É—Ç–æ–≥–æ —Å–∫–∞–ª–∏—Å—Ç–æ–≥–æ —É—Ç–µ—Å–∞, –≤–æ–∑–≤—ã—à–∞–≤—à–µ–≥–æ—Å—è –Ω–∞–¥ –≥–ª—É–±–æ–∫–∏–º —É—â–µ–ª—å–µ–º. –ú–∞—Å—Å–∏–≤–Ω—ã–µ
—Å—Ç–µ–Ω—ã –∏–∑ —Ç–µ–º–Ω–æ-—Å–µ—Ä–æ–≥–æ –≥—Ä–∞–Ω–∏—Ç–∞, –ø–æ—Ç–µ–º–Ω–µ–≤—à–∏–µ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –∏ –Ω–µ–ø–æ–≥–æ–¥—ã,
–æ–ø–æ—è—Å—ã–≤–∞–ª–∏ –∫—Ä–µ–ø–æ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–æ–ª—å—Ü–∞–º–∏ –∑–∞—â–∏—Ç–Ω—ã—Ö —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π. –ß–µ—Ç—ã—Ä–µ
–≤—ã—Å–æ–∫–∏–µ –±–∞—à–Ω–∏ —Å –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω—ã–º–∏ —à–ø–∏–ª—è–º–∏ —Å—Ç–æ—è–ª–∏ –ø–æ —É–≥–ª–∞–º –≥–ª–∞–≤–Ω–æ–≥–æ –¥–≤–æ—Ä–∞,
–∏—Ö —Å–∏–ª—É—ç—Ç—ã —á–µ—Ç–∫–æ –≤—ã—Ä–∏—Å–æ–≤—ã–≤–∞–ª–∏—Å—å –Ω–∞ —Ñ–æ–Ω–µ –ø–∞—Å–º—É—Ä–Ω–æ–≥–æ –Ω–µ–±–∞.

–ì–ª–∞–≤–Ω—ã–µ –≤–æ—Ä–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–ª–∏ —Å–æ–±–æ–π –º–∞—Å—Å–∏–≤–Ω—É—é –∞—Ä–∫—É –∏–∑ —á–µ—Ä–Ω–æ–≥–æ –±–∞–∑–∞–ª—å—Ç–∞,
—É–∫—Ä–∞—à–µ–Ω–Ω—É—é –≤—ã–≤–µ—Ç—Ä–µ–Ω–Ω—ã–º–∏ –±–∞—Ä–µ–ª—å–µ—Ñ–∞–º–∏ –≤–æ–ª—á—å–∏—Ö –≥–æ–ª–æ–≤ - –¥—Ä–µ–≤–Ω–µ–≥–æ —Å–∏–º–≤–æ–ª–∞
—à–∫–æ–ª—ã –≤–µ–¥—å–º–∞–∫–æ–≤. –¢—è–∂–µ–ª–∞—è –¥—É–±–æ–≤–∞—è —Å—Ç–≤–æ—Ä–∫–∞, –æ–∫–æ–≤–∞–Ω–Ω–∞—è –∂–µ–ª–µ–∑–æ–º –∏ –ø–æ–∫—Ä—ã—Ç–∞—è
–≥–ª—É–±–æ–∫–∏–º–∏ –∑–∞—Ä—É–±–∫–∞–º–∏ –æ—Ç –º–µ—á–µ–π –∏ –∫–æ–≥—Ç–µ–π, –±—ã–ª–∞ –ø—Ä–∏–æ—Ç–∫—Ä—ã—Ç–∞. –ù–∞–¥ –≤—Ö–æ–¥–æ–º
–≤–æ–∑–≤—ã—à–∞–ª–∞—Å—å –º–∞—Å—Å–∏–≤–Ω–∞—è —Ä–µ—à–µ—Ç–∫–∞, –≥–æ—Ç–æ–≤–∞—è –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç –æ–ø—É—Å—Ç–∏—Ç—å—Å—è –∏
–∑–∞–ø–µ—Ä–µ—Ç—å –Ω–µ–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö –≥–æ—Å—Ç–µ–π.

–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–≤–æ—Ä –±—ã–ª –≤—ã–º–æ—â–µ–Ω –Ω–µ—Ä–æ–≤–Ω—ã–º–∏ –∫–∞–º–µ–Ω–Ω—ã–º–∏ –ø–ª–∏—Ç–∞–º–∏, –º–µ–∂–¥—É –∫–æ—Ç–æ—Ä—ã–º–∏
–ø—Ä–æ–±–∏–≤–∞–ª–∞—Å—å —Ä–µ–¥–∫–∞—è —Ç—Ä–∞–≤–∞. –í —Ü–µ–Ω—Ç—Ä–µ —Å—Ç–æ—è–ª –≤—ã—Å–æ—Ö—à–∏–π —Ñ–æ–Ω—Ç–∞–Ω –≤ –≤–∏–¥–µ –≥—Ä–∏—Ñ–æ–Ω–∞,
–µ–≥–æ —á–∞—à–∞ –¥–∞–≤–Ω–æ –∑–∞—Ä–æ—Å–ª–∞ –º—Ö–æ–º. –ü–æ –ø–µ—Ä–∏–º–µ—Ç—Ä—É —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ
–ø–æ—Å—Ç—Ä–æ–π–∫–∏: –æ—Ä—É–∂–µ–π–Ω–∞—è —Å –∑–∞–∫–æ–ø—á–µ–Ω–Ω—ã–º–∏ —Å—Ç–µ–Ω–∞–º–∏, –∫–æ–Ω—é—à–Ω–∏ —Å –ø–æ–∫–æ—Å–∏–≤—à–µ–π—Å—è
–∫—Ä—ã—à–µ–π, —Å—Ç–∞—Ä–∞—è –∫—É–∑–Ω–∏—Ü–∞, –∏–∑ —Ç—Ä—É–±—ã –∫–æ—Ç–æ—Ä–æ–π —É–∂–µ –º–Ω–æ–≥–æ –ª–µ—Ç –Ω–µ –ø–æ–¥–Ω–∏–º–∞–ª—Å—è –¥—ã–º.

–ñ–∏–ª—ã–µ –ø–æ–∫–æ–∏ –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –≤ –≥–ª–∞–≤–Ω–æ–π –±–∞—à–Ω–µ. –£–∑–∫–∏–µ –æ–∫–Ω–∞-–±–æ–π–Ω–∏—Ü—ã –ø—Ä–æ–ø—É—Å–∫–∞–ª–∏
–º–∞–ª–æ —Å–≤–µ—Ç–∞, –∫–æ—Ä–∏–¥–æ—Ä—ã —Ç–æ–Ω—É–ª–∏ –≤ –ø–æ–ª—É–º—Ä–∞–∫–µ. –ö–∞–º–µ–Ω–Ω—ã–µ —Å—Ç–µ–Ω—ã –ø–æ–∫—Ä—ã–≤–∞–ª –∏–Ω–µ–π,
–Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ª–µ—Ç–æ - –º–∞–≥–∏—è –¥—Ä–µ–≤–Ω–∏—Ö –∑–∞—â–∏—Ç–Ω—ã—Ö —á–∞—Ä –≤—Å–µ –µ—â–µ –¥–µ–π—Å—Ç–≤–æ–≤–∞–ª–∞,
–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –≤–Ω—É—Ç—Ä–∏ –∫—Ä–µ–ø–æ—Å—Ç–∏ –≤–µ—á–Ω—É—é –ø—Ä–æ—Ö–ª–∞–¥—É. –ü–æ —É–≥–ª–∞–º –∫–æ—Ä–∏–¥–æ—Ä–æ–≤
–º–µ—Ä—Ü–∞–ª–∏ –º–∞–≥–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Å—Ç–∞–ª–ª—ã, –¥–∞–≤–∞–≤—à–∏–µ —Ç—É—Å–∫–ª—ã–π –≥–æ–ª—É–±–æ–≤–∞—Ç—ã–π —Å–≤–µ—Ç.

–í—Å—è –∫—Ä–µ–ø–æ—Å—Ç—å –¥—ã—à–∞–ª–∞ –¥—É—Ö–æ–º —É—à–µ–¥—à–µ–π —ç–ø–æ—Ö–∏, –≤–µ–ª–∏—á–∏–µ–º –∏ —É–ø–∞–¥–∫–æ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
–ù–µ–∫–æ–≥–¥–∞ –≥—Ä–æ–∑–Ω—ã–π –æ–ø–ª–æ—Ç –≤–µ–¥—å–º–∞–∫–æ–≤ —Ç–µ–ø–µ—Ä—å —Å—Ç–æ—è–ª –ø–æ–ª—É–∑–∞–±—Ä–æ—à–µ–Ω–Ω—ã–º, –Ω–∞—Å–µ–ª–µ–Ω–Ω—ã–º
–ª–∏—à—å –ø—Ä–∏–∑—Ä–∞–∫–∞–º–∏ –ø—Ä–æ—à–ª–æ–≥–æ –∏ —Ä–µ–¥–∫–∏–º–∏ –≥–æ—Å—Ç—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –ø–æ–º–Ω–∏–ª–∏ –ø—É—Ç—å
—Å–∫–≤–æ–∑—å –≥–æ—Ä–Ω—ã–µ –ø–µ—Ä–µ–≤–∞–ª—ã..."

–ü–æ—á–µ–º—É –ò–î–ï–ê–õ–¨–ù–û:
‚úÖ‚úÖ 2347 chars - excellent –¥–ª—è SD3, DALL-E 3
‚úÖ‚úÖ –ó–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ –º–Ω–æ–≥–æ–ø–∞—Ä–∞–≥—Ä–∞—Ñ–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
‚úÖ‚úÖ –ù–µ–≤–µ—Ä–æ—è—Ç–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è:
    - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–∑–∞–º–æ–∫, –±–∞—à–Ω–∏, –≤–æ—Ä–æ—Ç–∞, –¥–≤–æ—Ä)
    - –ú–∞—Ç–µ—Ä–∏–∞–ª—ã (–≥—Ä–∞–Ω–∏—Ç, –±–∞–∑–∞–ª—å—Ç, –¥—É–±, –∂–µ–ª–µ–∑–æ)
    - –¶–≤–µ—Ç–∞ (—Ç–µ–º–Ω–æ-—Å–µ—Ä—ã–π, —á–µ—Ä–Ω—ã–π, –≥–æ–ª—É–±–æ–≤–∞—Ç—ã–π)
    - –¢–µ–∫—Å—Ç—É—Ä—ã (–∫–∞–º–µ–Ω—å, –º–æ—Ö, –∏–Ω–µ–π, –∑–∞—Ä—É–±–∫–∏)
    - –û—Å–≤–µ—â–µ–Ω–∏–µ (–ø–æ–ª—É–º—Ä–∞–∫, —Ç—É—Å–∫–ª—ã–π —Å–≤–µ—Ç –∫—Ä–∏—Å—Ç–∞–ª–ª–æ–≤)
    - –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ (–≤–µ–ª–∏—á–∏–µ, —É–ø–∞–¥–æ–∫, –¥—Ä–µ–≤–Ω–æ—Å—Ç—å)
‚úÖ‚úÖ –ú–æ–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 4-5 —Ä–∞–∑–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
    1. Exterior view (–∫—Ä–µ–ø–æ—Å—Ç—å –Ω–∞ —É—Ç–µ—Å–µ)
    2. Main gate (–≤–æ—Ä–æ—Ç–∞ —Å –±–∞—Ä–µ–ª—å–µ—Ñ–∞–º–∏)
    3. Inner courtyard (–¥–≤–æ—Ä —Å —Ñ–æ–Ω—Ç–∞–Ω–æ–º)
    4. Interior corridor (–º–∞–≥–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Å—Ç–∞–ª–ª—ã)
    5. Overall atmosphere (–¥—É—Ö —É—à–µ–¥—à–µ–π —ç–ø–æ—Ö–∏)
‚úÖ‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –í–°–ï–• –º–æ–¥–µ–ª–µ–π:
    - SD3: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (2347 chars < 10K limit)
    - DALL-E 3: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (2347 chars < 4K limit)
    - Gemini: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    - Flux: –ø–µ—Ä–≤—ã–µ 1000 chars
```

---

## üöÄ –ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–π Implementation Plan

### Phase 1: Core Infrastructure (2-3 –Ω–µ–¥–µ–ª–∏)

#### Task 1.1: Paragraph-Based Segmentation ‚ú® –ù–û–í–û–ï
**–°—Ä–æ–∫:** 3-4 –¥–Ω—è

- [ ] Implement `ParagraphSegmenter`
- [ ] –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è!)
- [ ] –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ (DESCRIPTION/NARRATIVE/DIALOG/META)
- [ ] Preliminary scoring
- [ ] Unit tests —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏

**Files:**
- `backend/app/services/paragraph_segmenter.py` (new, ~400 lines)

#### Task 1.2: Description Boundary Detection ‚ú® –ö–†–ò–¢–ò–ß–ù–û
**–°—Ä–æ–∫:** 5-7 –¥–Ω–µ–π

- [ ] Implement `DescriptionBoundaryDetector`
- [ ] Multi-paragraph window analysis
- [ ] Continuation/end signal detection
- [ ] Grouping paragraphs ‚Üí complete descriptions (500-3500 chars)
- [ ] Length categorization (medium/long/very_long)
- [ ] Priority scoring –ø–æ –¥–ª–∏–Ω–µ

**Files:**
- `backend/app/services/boundary_detector.py` (new, ~500 lines)

#### Task 1.3: Long Text NLP Processing ‚ú® –ù–û–í–û–ï
**–°—Ä–æ–∫:** 4-5 –¥–Ω–µ–π

- [ ] Implement `LongTextNLPProcessor`
- [ ] Chunked analysis –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
- [ ] Aggregation —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ chunks
- [ ] Multi-NLP ensemble –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
- [ ] Confidence calculation (multi-factor)

**Files:**
- `backend/app/services/long_text_nlp_processor.py` (new, ~600 lines)
- Modify `backend/app/services/multi_nlp_manager.py` (+200 lines)

#### Task 1.4: Advanced Anti-Patterns
**–°—Ä–æ–∫:** 2-3 –¥–Ω—è

- [ ] Chapter headers detection
- [ ] Dialog filtering (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
- [ ] Meta-text filtering
- [ ] Fragment detection & filtering
- [ ] Incomplete sentence detection

**Files:**
- `backend/app/services/enhanced_nlp_system.py` (+200 lines)

---

### Phase 2: Unified Prompt Generation (2-3 –Ω–µ–¥–µ–ª–∏)

#### Task 2.1: Base Prompt System ‚ú® –ù–û–í–û–ï
**–°—Ä–æ–∫:** 3-4 –¥–Ω—è

- [ ] Implement `BasePrompt` structure
- [ ] Visual elements extraction
- [ ] Mood/atmosphere extraction
- [ ] Entity extraction & categorization
- [ ] Context enrichment

**Files:**
- `backend/app/services/prompt_generation/base_prompt.py` (new, ~300 lines)

#### Task 2.2: Model-Specific Formatters ‚ú® –ö–†–ò–¢–ò–ß–ù–û
**–°—Ä–æ–∫:** 7-10 –¥–Ω–µ–π

- [ ] `SD3PromptFormatter` (Stable Diffusion 3)
- [ ] `DALLE3PromptFormatter` (DALL-E 3)
- [ ] `GeminiPromptFormatter` (Gemini Imagen 3)
- [ ] `FluxPromptFormatter` (Flux/pollinations)
- [ ] `MidjourneyPromptFormatter` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**Files:**
- `backend/app/services/prompt_generation/formatters/` (new directory)
  - `sd3_formatter.py` (~400 lines)
  - `dalle3_formatter.py` (~300 lines)
  - `gemini_formatter.py` (~350 lines)
  - `flux_formatter.py` (~200 lines)

#### Task 2.3: Unified Generator
**–°—Ä–æ–∫:** 2-3 –¥–Ω—è

- [ ] Implement `UnifiedPromptGenerator`
- [ ] Multi-model prompt generation
- [ ] Prompt storage –≤ –ë–î
- [ ] API endpoints –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤

**Files:**
- `backend/app/services/prompt_generation/unified_generator.py` (new, ~250 lines)

---

### Phase 3: Database & Model Updates (1 –Ω–µ–¥–µ–ª—è)

#### Task 3.1: Database Schema Updates
**–°—Ä–æ–∫:** 2-3 –¥–Ω—è

- [ ] Add `prompts` JSONB field –∫ Description model
- [ ] Add `length_category` field (medium/long/very_long)
- [ ] Add `paragraph_count` field
- [ ] Add `visual_richness_score` field
- [ ] Migration script

**Files:**
- `backend/alembic/versions/xxxx_add_prompt_fields.py` (new)
- `backend/app/models/description.py` (modify)

#### Task 3.2: Updated Priority Calculation
**–°—Ä–æ–∫:** 1-2 –¥–Ω—è

- [ ] Implement new `calculate_advanced_priority_score()`
- [ ] Update –≤ Description model
- [ ] Batch re-calculation –¥–ª—è existing descriptions

**Files:**
- `backend/app/models/description.py` (modify method)

---

### Phase 4: Testing & Validation (2 –Ω–µ–¥–µ–ª–∏)

#### Task 4.1: Re-Parse Test Book
**–°—Ä–æ–∫:** 2-3 –¥–Ω—è

- [ ] Drop existing descriptions from "–í–µ–¥—å–º–∞–∫"
- [ ] Full re-parse —Å –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π
- [ ] Generate prompts –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- [ ] Store –≤ –ë–î

#### Task 4.2: Metrics Validation
**–°—Ä–æ–∫:** 3-4 –¥–Ω—è

- [ ] Measure length distribution
- [ ] Measure type distribution
- [ ] Measure confidence distribution
- [ ] Quality manual review (top 100)
- [ ] Compare: old vs new system

#### Task 4.3: Image Generation Test
**–°—Ä–æ–∫:** 5-7 –¥–Ω–µ–π

- [ ] Generate images —Å SD3 (top 50)
- [ ] Generate images —Å DALL-E 3 (top 50)
- [ ] Generate images —Å Gemini (top 50)
- [ ] Generate images —Å Flux (top 50)
- [ ] Visual quality assessment
- [ ] User feedback collection

---

## üìä Success Metrics (KPIs) - –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ

### Primary KPIs

#### 1. Length Distribution ‚ú® –ì–õ–ê–í–ù–´–ô –ú–ï–¢–†–ò–ö
```
Target:
  500-1000 chars:  15%
  1000-2000 chars: 40%  ‚Üê Optimal –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
  2000-3500 chars: 36%  ‚Üê Excellent –¥–ª—è SD3
  > 3500 chars:    6%
  < 500 chars:     < 3% ‚Üê –ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å

Current:
  < 100 chars:  46.7%  ‚ùå
  100-500:      53.3%  ‚ùå
  > 500:        0.0%   ‚ùå
```

#### 2. Type Distribution Accuracy
```
Target:
  LOCATION:  45-50%
  CHARACTER: 30-35%
  ATMOSPHERE: 15-20%
  OBJECT:    < 5%

Current:
  OBJECT:    53.5%  ‚ùå
  LOCATION:  40.0%  ‚ö†Ô∏è
  CHARACTER: 4.9%   ‚ùå
  ATMOSPHERE: 1.7%  ‚ùå
```

#### 3. Quality Score
```
Target:
  Avg confidence: > 0.72
  Avg visual richness: > 0.65
  Complete descriptions: > 98%

Current:
  Avg confidence: ~0.45  ‚ùå
  Complete: ~30%         ‚ùå
```

#### 4. Model Suitability ‚ú® –ù–û–í–´–ô –ú–ï–¢–†–ò–ö
```
Target:
  Suitable –¥–ª—è SD3:        > 80%  ‚Üê –¥–ª–∏–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ
  Suitable –¥–ª—è DALL-E 3:   > 85%
  Suitable –¥–ª—è Gemini:     > 90%
  Suitable –¥–ª—è Flux:       > 95%

Current:
  Suitable –¥–ª—è –ª—é–±–æ–π –º–æ–¥–µ–ª–∏: ~20%  ‚ùå
```

### Secondary KPIs

#### 5. Fragment Elimination
```
Target: < 2% —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
Current: ~70% —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤  ‚ùå
```

#### 6. Processing Time
```
Target: < 30 seconds per chapter (acceptable per user)
Current: ~4 seconds (maintain or improve)
```

#### 7. Prompt Generation Coverage
```
Target: 100% descriptions have prompts –¥–ª—è –≤—Å–µ—Ö 4 –º–æ–¥–µ–ª–µ–π
```

---

## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

### –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ò–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç –í–µ—Ä—Å–∏–∏ 1.0

1. **üöÄ –†–µ–≤–æ–ª—é—Ü–∏—è –¥–ª–∏–Ω—ã:**
   - OLD: 50-500 chars (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ)
   - NEW: 500-3500 chars (—Ñ–æ–∫—É—Å –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è)
   - –ü–†–ò–û–†–ò–¢–ï–¢: –î–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –ø–∞—Ä—Å—è—Ç—Å—è –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å

2. **üé® –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–º–ø—Ç–æ–≤:**
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 4+ –º–æ–¥–µ–ª–µ–π (SD3, DALL-E 3, Gemini, Flux)
   - Model-specific formatting
   - Automatic prompt generation –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π

3. **üìê Paragraph-based –ø–æ–¥—Ö–æ–¥:**
   - –ù–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∫–∞–∫ –±–∞–∑–æ–≤–∞—è –µ–¥–∏–Ω–∏—Ü–∞
   - Multi-paragraph grouping
   - Boundary detection –¥–ª—è –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π

4. **üî¨ Advanced NLP –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤:**
   - Chunked analysis
   - Result aggregation
   - Multi-factor confidence scoring

### –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?

**–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (2025) –õ–Æ–ë–Ø–¢ –¥–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è:**
- SD3: –¥–æ 10,000 —Å–∏–º–≤–æ–ª–æ–≤!
- DALL-E 3: –¥–æ 4,000 —Å–∏–º–≤–æ–ª–æ–≤
- Gemini: –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
- Flux: 100-1000+ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ

**–•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –ü–û–õ–ù–ê –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π:**
- –§—ç–Ω—Ç–µ–∑–∏: –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –º–∏—Ä–æ–≤ (1000-3000 chars)
- –ö–ª–∞—Å—Å–∏–∫–∞: –ø–µ–π–∑–∞–∂–∏, –∏–Ω—Ç–µ—Ä—å–µ—Ä—ã (800-2000 chars)
- –î–µ—Ç–µ–∫—Ç–∏–≤: –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞, —É–ª–∏–∫–∏ (600-1500 chars)

**–¢–µ–∫—É—â–∞—è —Å–∏—Å—Ç–µ–º–∞ –ò–ì–ù–û–†–ò–†–£–ï–¢ –ª—É—á—à–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç:**
- –ú–∞–∫—Å–∏–º—É–º 398 chars - —ç—Ç–æ —Å–º–µ—à–Ω–æ –º–∞–ª–æ!
- –ù–æ–ª—å –æ–ø–∏—Å–∞–Ω–∏–π > 500 chars
- –§–æ–∫—É—Å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω—ã—Ö —Å—Ü–µ–Ω

### –û–∂–∏–¥–∞–µ–º—ã–π –≠—Ñ—Ñ–µ–∫—Ç

**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π:**
- OLD: ~200-300 usable (< 25%)
- NEW: ~1000-1100 excellent (> 80%)

**–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–æ–¥–µ–ª–µ–π:**
- OLD: —Ç–æ–ª—å–∫–æ Flux (—á–∞—Å—Ç–∏—á–Ω–æ)
- NEW: SD3 + DALL-E 3 + Gemini + Flux (–≤—Å–µ –º–æ–¥–µ–ª–∏)

**–ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:**
- OLD: 20% –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º—ã
- NEW: 85%+ —Å–æ–∑–¥–∞—é—Ç –æ—Ç–ª–∏—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

---

**–°—Ç–∞—Ç—É—Å:** –ì–æ—Ç–æ–≤–æ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** Phase 1 - Core Infrastructure
**Estimated Time:** 4-6 –Ω–µ–¥–µ–ª—å –ø–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

**–í–µ—Ä—Å–∏—è:** 2.0 (Extended - Long Descriptions Focus)
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 2025-11-05
**–ê–≤—Ç–æ—Ä:** Claude Code

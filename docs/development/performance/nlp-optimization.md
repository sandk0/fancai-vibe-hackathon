# NLP Parsing Optimization Plan for Flux Image Generation

**Date:** 2025-11-05
**Author:** Claude Code
**Version:** 1.0
**Status:** Comprehensive Analysis & Implementation Plan

---

## Executive Summary

### Current Situation: CRITICAL

–ê–Ω–∞–ª–∏–∑ 1257 –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –∫–Ω–∏–≥–∏ "–í–µ–¥—å–º–∞–∫. –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–æ–∫ –≤–æ—Ä–æ–Ω–æ–≤" –≤—ã—è–≤–∏–ª **–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã** –≤ –ø–∞—Ä—Å–∏–Ω–≥–µ:

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #1: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
```
–¢–ï–ö–£–©–ï–ï (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ):
  OBJECT:      672 (53.5%)  ‚Üê –î–û–ú–ò–ù–ò–†–£–ï–¢, –Ω–æ –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (40%)
  LOCATION:    503 (40.0%)  ‚Üê –î–æ–ª–∂–Ω–æ –±—ã—Ç—å 75% –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
  CHARACTER:    61 (4.9%)   ‚Üê –ö–ê–¢–ê–°–¢–†–û–§–ê! –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 60%
  ATMOSPHERE:   21 (1.7%)   ‚Üê –ü–û–ß–¢–ò –ù–ï–¢! –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 45%

–û–ñ–ò–î–ê–ï–ú–û–ï (–ø—Ä–∞–≤–∏–ª—å–Ω–æ):
  LOCATION:    ~600 (48%)   ‚Üê –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–∫—É—Å
  CHARACTER:   ~400 (32%)   ‚Üê –í—Ç–æ—Ä–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
  ATMOSPHERE:  ~200 (16%)   ‚Üê –¢—Ä–µ—Ç–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
  OBJECT:       ~57 (4%)    ‚Üê –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
```

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #2: –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏–π - –£–ñ–ê–°–ù–û–ï
**–¢–æ–ø-10 –ø–æ confidence score (0.90-0.95) - —ç—Ç–æ –ú–£–°–û–†:**
- `"–∑–≤–µ–∑–¥—ã. –ñ–∞–Ω-–ê–Ω—Ç–µ–ª—å–º –ë—Ä–∏–ª—å—è-–°–∞–≤–∞—Ä–µ–Ω –ì–ª–∞–≤–∞ —Å–µ–¥—å–º–∞—è..."` - **–ó–ê–ì–û–õ–û–í–û–ö –ì–õ–ê–í–´**
- `"–º–∞—Ä—Ö–∏–∏. –ú–∞—Ä–∫–≥—Ä–∞—Ñ—Å—Ç–≤ —ç—Ç–∏—Ö —á–µ—Ç—ã—Ä–µ..."` - **–û–ë–†–´–í–û–ö –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø**
- `"–í–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç—å—é, —Ç–æ –µ—Å—Ç—å –æ—Ç –≥–æ–ª–æ–≤—ã..."` - **–§–†–ê–ì–ú–ï–ù–¢ –ò–ó –°–ü–†–ê–í–û–ß–ù–ò–ö–ê**

**–ü—Ä–æ–±–ª–µ–º–∞:** –°–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞, –∞ –Ω–µ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è.

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #3: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è Flux
```
–¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
  –°—Ä–µ–¥–Ω–µ–µ:  104.7 chars  ‚Üê –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û (Flux –Ω—É–∂–Ω–æ 100-500+)
  –ú–µ–¥–∏–∞–Ω–∞:  102 chars
  –ú–∏–Ω–∏–º—É–º:  50 chars     ‚Üê –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ—Ä–æ–≥
  –ú–∞–∫—Å–∏–º—É–º: 398 chars

–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã:
  < 100 chars:  587 (46.7%)  ‚Üê –ü–û–õ–û–í–ò–ù–ê —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ!
  100-500:      670 (53.3%)  ‚Üê –ü–æ–¥—Ö–æ–¥—è—Ç –ø–æ –¥–ª–∏–Ω–µ, –Ω–æ –ö–ê–ß–ï–°–¢–í–û —É–∂–∞—Å–Ω–æ–µ
  > 500 chars:  0 (0.0%)    ‚Üê –ù–ï–¢ –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
```

#### üî¥ –ü—Ä–æ–±–ª–µ–º–∞ #4: –ö–æ—Ä–Ω–µ–≤–∞—è –ø—Ä–∏—á–∏–Ω–∞
–í `backend/app/models/description.py:153-162`:
```python
# –ë–æ–Ω—É—Å –∑–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É (15-300 —Å–∏–º–≤–æ–ª–æ–≤)
if 15 <= content_length <= 300:
    length_score = 15
```

**–°–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ–¥ 15-300 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ Flux —Ç—Ä–µ–±—É–µ—Ç 100-500+ —Å–∏–º–≤–æ–ª–æ–≤!**

---

## Flux Requirements Analysis

### Optimal Flux Prompt Format (Research Results)

–ò–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è Flux AI (pollinations.ai) —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ:

#### ‚úÖ –ò–¥–µ–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
```
–î–ª–∏–Ω–∞:        100-500+ —Å–∏–º–≤–æ–ª–æ–≤ (–¥–æ 1000 –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω)
–Ø–∑—ã–∫:         Natural language, –¥–µ—Ç–∞–ª—å–Ω—ã–π, –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–π
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:    Subject ‚Üí Setting ‚Üí Lighting ‚Üí Color ‚Üí Mood ‚Üí Composition
–°—Ç–∏–ª—å:        Narrative descriptions, –∫–∞–∫ –∏–∑ –∫–Ω–∏–≥–∏
–ò–∑–±–µ–≥–∞—Ç—å:     –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ø–∏—Å–∫–∏, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —è–∑—ã–∫, weight syntax
```

#### ‚úÖ –ü—Ä–∏–º–µ—Ä –û–¢–õ–ò–ß–ù–û–ì–û –æ–ø–∏—Å–∞–Ω–∏—è (578 chars)
```
"–í–µ—Ç–µ—Ä –¥—É–ª –Ω–∞ —Å–µ–≤–µ—Ä–æ-–≤–æ—Å—Ç–æ–∫, –Ω–∞–¥ —Å–µ–ª–µ–Ω–∏—è–º–∏ –î–∂—É–∞–ª–¥–µ –Ω–µ—Å–ø–µ—à–Ω–æ –ø–ª—ã–ª–∏ –Ω–∏–∑–∫–∏–µ
–æ–±–ª–∞–∫–∞, –∑–∞–∫—Ä—ã–≤–∞–≤—à–∏–µ —Å–æ–ª–Ω—Ü–µ; –∏–∑ –≤—Å–µ—Ö –±–∞—à–µ–Ω –∏ –º–∏–Ω–∞—Ä–µ—Ç–æ–≤ –¢–∞—Ä –í–∞–ª–æ–Ω–∞ –±—ã–ª–∏
–≤–∏–¥–Ω—ã —Ç–æ–ª—å–∫–æ –æ—Å—Ç—Ä—ã–µ –≤–µ—Ä—à–∏–Ω—ã. –•–æ–ª–æ–¥–Ω—ã–π, —Ä–æ–≤–Ω—ã–π —Å–≤–µ—Ç —Ä–æ–≤–Ω—ã–º–∏ —Å–µ—Ä—ã–º–∏ –º–∞–∑–∫–∞–º–∏
–ª–æ–∂–∏–ª—Å—è –Ω–∞ –±–µ–ª—ã–µ –∫–∞–º–Ω–∏ –≥–æ—Ä–æ–¥–∞, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –∑–¥–∞–Ω–∏–π,
–Ω–∞–ø–æ–º–∏–Ω–∞–≤—à–∏—Ö –æ–≥—Ä–æ–º–Ω—ã–µ –∏–≥—Ä–∞–ª—å–Ω—ã–µ –∫–æ—Å—Ç–∏ –∏–ª–∏ —á—å–∏-—Ç–æ –ø–∞–ª—å—Ü—ã, —É—Å—Ç—Ä–µ–º–ª—ë–Ω–Ω—ã–µ –∫
–Ω–µ–±—É. –ü–æ –∏–∑–≤–∏–ª–∏—Å—Ç—ã–º —É–ª–∏—Ü–∞–º, –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –ø–æ—á—ë—Ç–Ω–æ–π —Å—Ç—Ä–∞–∂–∏ –®–∞–π–¥–æ –¥–≤–∏–≥–∞–ª—Å—è –≤
–≤–æ—Å—Ç–æ—á–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –±–ª–µ—Å—Ç—è—â–∏–π –ø–æ–¥ —Å–µ—Ä—ã–º –Ω–µ–±–æ–º –ø–∞–ª–∞–Ω–∫–∏–Ω. –û—Ç –¥–≤–æ—Ä—Ü–∞ –≤
—Å—Ç–æ—Ä–æ–Ω—É –ß—ë—Ä–Ω–æ–π –±–∞—à–Ω–∏."
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –∏–¥–µ–∞–ª—å–Ω–æ:**
- ‚úÖ 578 —Å–∏–º–≤–æ–ª–æ–≤ - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
- ‚úÖ –í–∫–ª—é—á–∞–µ—Ç: –ª–æ–∫–∞—Ü–∏—é (–¢–∞—Ä –í–∞–ª–æ–Ω), –ø–æ–≥–æ–¥—É (–≤–µ—Ç–µ—Ä, –æ–±–ª–∞–∫–∞), –æ—Å–≤–µ—â–µ–Ω–∏–µ (—Ö–æ–ª–æ–¥–Ω—ã–π —Å–≤–µ—Ç), —Ü–≤–µ—Ç–∞ (–±–µ–ª—ã–µ –∫–∞–º–Ω–∏, —Å–µ—Ä–æ–µ –Ω–µ–±–æ), –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (—Ö–æ–ª–æ–¥–Ω–æ–µ, –≤–µ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ)
- ‚úÖ –ñ–∏–≤–æ–ø–∏—Å–Ω–æ–µ, –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
- ‚úÖ –ó–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ, coherent –æ–ø–∏—Å–∞–Ω–∏–µ
- ‚úÖ Natural language, –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ

#### ‚ùå –ü—Ä–∏–º–µ—Ä –ü–õ–û–•–û–ì–û –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (—Ç–µ–∫—É—â–∞—è —Å–∏—Å—Ç–µ–º–∞)
```
", —á—Ç–æ –∞–º–±–∏—Ü–∏–∏ —Ç–∞–º–æ—à–Ω–∏—Ö –º–∞—Ä–∫–≥—Ä–∞—Ñ–æ–≤ ‚Äî –ø—Ä–æ–¥–≤–∏–≥–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ –≤ –¥–æ–ª–∏–Ω—ã
–î—Ä–∞–∫–æ–Ω—å–∏—Ö –≥–æ—Ä, –æ—Ç—Ç–æ–≥–æ –≤–ª–∞–¥–µ–Ω–∏—è —Å–≤–æ–∏ —Ü–∏—Å–º–æ–Ω—Ç–∞–Ω—Å–∫–∏–º–∏ –Ω–∞–∑—ã–≤–∞—Ç—å –ø–æ–≤–µ–ª–æ—Å—å,
–¥–µ—Å–∫–∞—Ç—å,"
```

**–ü–æ—á–µ–º—É —ç—Ç–æ —É–∂–∞—Å–Ω–æ:**
- ‚ùå –û–±—Ä—ã–≤–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–ø—è—Ç–æ–π!)
- ‚ùå –ù–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–µ (–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ)
- ‚ùå –§—Ä–∞–≥–º–µ–Ω—Ç, –Ω–µ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–∞—è –º—ã—Å–ª—å
- ‚ùå –ù–µ—Ç detalied visual elements

---

## Detailed Problem Analysis

### Problem 1: Fragment Extraction ("—Å—ä–µ–¥–∞–µ—Ç —á–∞—Å—Ç–∏ —Å–ª–æ–≤")

**–ü—Ä–∏—á–∏–Ω–∞:** –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥—Ä–∞–Ω–∏—Ü –æ–ø–∏—Å–∞–Ω–∏–π.

–¢–µ–∫—É—â–∞—è –ª–æ–≥–∏–∫–∞:
```python
# enhanced_nlp_system.py:350-370
def _extract_entity_descriptions(self, doc):
    for sent in doc.sents:
        # –ò–∑–≤–ª–µ–∫–∞–µ—Ç –û–î–ù–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        description = sent.text.strip()
        # –ü—Ä–æ–±–ª–µ–º–∞: –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å multi-sentence!
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ 3 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
- –ë–µ—Ä–µ—Ç—Å—è —Å–µ—Ä–µ–¥–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è, –ø–æ—Ç–µ—Ä—è–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç
- "—Å—ä–µ–¥–∞–µ—Ç —á–∞—Å—Ç–∏" - –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç

### Problem 2: Dialog and Service Text Extraction

**–ü—Ä–∏–º–µ—Ä—ã –∏–∑ –ë–î:**
```
‚ùå "–ì–ª–∞–≤–∞ –≤–æ—Å—å–º–∞—è ¬´–í–æ—Ç —Å–ª–æ–≤–∞ –ü—Ä–æ—Ä–æ–∫–∞: –≤–æ–∏—Å—Ç–∏–Ω—É –≥–æ–≤–æ—Ä—é –≤–∞–º..."
‚ùå "–∑–≤–µ–∑–¥—ã. –ñ–∞–Ω-–ê–Ω—Ç–µ–ª—å–º –ë—Ä–∏–ª—å—è-–°–∞–≤–∞—Ä–µ–Ω –ì–ª–∞–≤–∞ —Å–µ–¥—å–º–∞—è..."
‚ùå "–¢–≤–æ—ë –ø—Ä–∏–∑–Ω–∞–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ, –∫—Å—Ç–∞—Ç–∏, –Ω–µ –∏–º–µ–µ—Ç —Å–∏–ª—ã..."
```

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:
- –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥–ª–∞–≤
- –≠–ø–∏–≥—Ä–∞—Ñ–æ–≤
- –î–∏–∞–ª–æ–≥–æ–≤ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
- –ê–≤—Ç–æ—Ä—Å–∫–∏—Ö —Ä–µ–º–∞—Ä–æ–∫
- –°–ª—É–∂–µ–±–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

### Problem 3: Wrong Type Classification

**–ü–æ—á–µ–º—É OBJECT –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç (53.5%):**

–¢–µ–∫—É—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–ª–∏—à–∫–æ–º —à–∏—Ä–æ–∫–∏–µ:
```python
# natasha_processor.py:60-65
"object_indicators": [
    r"\b(?:–º–µ—á|–∫–∏–Ω–∂–∞–ª|—â–∏—Ç|–ª—É–∫|–∫–æ–ø—å—ë|—Ç–æ–ø–æ—Ä)\b",  # –û—Ä—É–∂–∏–µ
    r"\b(?:–∑–æ–ª–æ—Ç–æ|—Å–µ—Ä–µ–±—Ä–æ|–º–µ–¥—å|–∂–µ–ª–µ–∑–æ|—Å—Ç–∞–ª—å)\b",  # –ú–∞—Ç–µ—Ä–∏–∞–ª—ã
    # ... —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
]
```

–õ—é–±–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ ‚Üí –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ OBJECT, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –ª–æ–∫–∞—Ü–∏–∏ –∏–ª–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞.

**–ü—Ä–∏–º–µ—Ä:**
```
"–ì–µ—Ä–∞–ª—å—Ç –¥–µ—Ä–∂–∞–ª –≤ —Ä—É–∫–µ —Å–µ—Ä–µ–±—Ä—è–Ω—ã–π –º–µ—á –∏ —à–µ–ª –ø–æ –º—Ä–∞—á–Ω–æ–º—É –ª–µ—Å—É."
                        ^^^^^^^^^^^^^^
–¢–µ–∫—É—â–∞—è —Å–∏—Å—Ç–µ–º–∞: OBJECT (–∏–∑-–∑–∞ "–º–µ—á")
–ü—Ä–∞–≤–∏–ª—å–Ω–æ: LOCATION –∏–ª–∏ CHARACTER (–ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã)
```

### Problem 4: Confidence Score Calculation Issues

**–¢–µ–∫—É—â–∞—è –ª–æ–≥–∏–∫–∞ (enhanced_nlp_system.py:400-420):**
```python
def _calculate_general_descriptive_score(self, sent) -> float:
    adj_count = sum(1 for token in sent if token.pos_ == "ADJ")
    noun_count = sum(1 for token in sent if token.pos_ == "NOUN")

    # –ü—Ä–æ–±–ª–µ–º–∞: —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞
    if adj_count > 0 and noun_count > 0:
        return 0.5 + (adj_count / (noun_count + adj_count)) * 0.3
    # –ò—Ç–æ–≥–æ: 0.5-0.8 –¥–ª—è –ø–æ—á—Ç–∏ –ª—é–±–æ–≥–æ —Ç–µ–∫—Å—Ç–∞!
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≥–ª–∞–≤ –ø–æ–ª—É—á–∞—é—Ç 0.9-0.95 confidence (–∏–∑-–∑–∞ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö)
- –†–µ–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–ª—É—á–∞—é—Ç 0.3-0.4 confidence (—Å–ª–æ–∂–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
- **–°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ù–ê–û–ë–û–†–û–¢!**

---

## Strategy 1: Description Boundary Detection

### Goal
–ò–∑–≤–ª–µ–∫–∞—Ç—å –ó–ê–ö–û–ù–ß–ï–ù–ù–´–ï –º–Ω–æ–≥–æ–ø—Ä–µ–¥–ª–æ–∂–µ–Ω—á–µ—Å–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è, –∞ –Ω–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã.

### Approach: Multi-Sentence Window Analysis

#### Step 1: Identify Description Start
**Indicators:**
```python
DESCRIPTION_START_PATTERNS = [
    # Visual scene setting
    r"^\s*[–ê-–Ø–Å].*?\b(–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏|—Å—Ç–æ—è–ª|–Ω–∞—Ö–æ–¥–∏–ª—Å—è|–≤–∏–¥–Ω–µ–ª—Å—è)",

    # Location descriptions
    r"^\s*[–ê-–Ø–Å].*?\b(–≥–æ—Ä–æ–¥|–¥–µ—Ä–µ–≤–Ω—è|–∑–∞–º–æ–∫|–¥–≤–æ—Ä–µ—Ü|–ª–µ—Å|–ø–æ–ª–µ|—Ä–µ–∫–∞|–≥–æ—Ä–∞)",

    # Character introductions
    r"^\s*[–ê-–Ø–Å].*?\b(–º—É–∂—á–∏–Ω–∞|–∂–µ–Ω—â–∏–Ω–∞|—á–µ–ª–æ–≤–µ–∫|–¥–µ–≤—É—à–∫–∞|—Å—Ç–∞—Ä–∏–∫|–≤–µ–¥—å–º–∞–∫)",

    # Atmospheric openings
    r"^\s*(–°–æ–ª–Ω—Ü–µ|–õ—É–Ω–∞|–í–µ—Ç–µ—Ä|–ù–µ–±–æ|–¢—É—á–∏|–¢—É–º–∞–Ω|–°–≤–µ—Ç)",
]
```

#### Step 2: Continue Until Description End
**Continuation indicators:**
```python
DESCRIPTION_CONTINUE_SIGNALS = {
    "has_descriptive_verbs": ["–±—ã–ª", "–∫–∞–∑–∞–ª—Å—è", "–≤—ã–≥–ª—è–¥–µ–ª", "–Ω–∞–ø–æ–º–∏–Ω–∞–ª"],
    "has_visual_adjectives": ["–≤—ã—Å–æ–∫–∏–π", "—Ç–µ–º–Ω—ã–π", "–æ–≥—Ä–æ–º–Ω—ã–π", "–≤–µ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π"],
    "has_spatial_prepositions": ["–Ω–∞–¥", "–ø–æ–¥", "–≤–æ–∫—Ä—É–≥", "—Ä—è–¥–æ–º", "–º–µ–∂–¥—É"],
    "has_color_words": ["–±–µ–ª—ã–π", "—á–µ—Ä–Ω—ã–π", "—Å–µ—Ä—ã–π", "–∫—Ä–∞—Å–Ω—ã–π", "–∑–æ–ª–æ—Ç–æ–π"],
}

DESCRIPTION_END_SIGNALS = {
    "dialog_start": ['‚Äî', '¬´', '"', "- "],
    "action_verb": ["—Å–∫–∞–∑–∞–ª", "–ø–æ–¥—É–º–∞–ª", "–ø–æ—à–µ–ª", "–ø–æ–≤–µ—Ä–Ω—É–ª—Å—è", "–±—Ä–æ—Å–∏–ª—Å—è"],
    "narrative_shift": ["–û–¥–Ω–∞–∫–æ", "–ù–æ", "–í–ø—Ä–æ—á–µ–º", "–ú–µ–∂–¥—É —Ç–µ–º"],
    "temporal_marker": ["–ó–∞—Ç–µ–º", "–ü–æ—Ç–æ–º", "–ß–µ—Ä–µ–∑", "–°–ø—É—Å—Ç—è"],
}
```

#### Step 3: Group Sentences
```python
def extract_complete_description(self, doc, start_sent_idx: int) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞—á–∏–Ω–∞—è —Å start_sent_idx.

    Returns:
        Complete multi-sentence description or None
    """
    sentences = list(doc.sents)
    description_sents = [sentences[start_sent_idx]]

    for i in range(start_sent_idx + 1, len(sentences)):
        sent = sentences[i]

        # Check if should continue
        if self._should_continue_description(sent, description_sents):
            description_sents.append(sent)
        else:
            break

        # Max 5 sentences –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
        if len(description_sents) >= 5:
            break

    # Require minimum 2 sentences for complete description
    if len(description_sents) >= 2:
        full_text = " ".join(s.text for s in description_sents)

        # Validate length for Flux
        if 100 <= len(full_text) <= 1000:
            return full_text

    return None
```

### Implementation Location
–î–æ–±–∞–≤–∏—Ç—å –≤ `backend/app/services/enhanced_nlp_system.py`:
- –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ `_extract_complete_descriptions()`
- –í—ã–∑—ã–≤–∞—Ç—å –î–û `_extract_entity_descriptions()`
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SpaCy `doc.sents` —Å window analysis

---

## Strategy 2: Anti-Patterns for Filtering

### Goal
–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: –¥–∏–∞–ª–æ–≥–∏, –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ª—É–∂–µ–±–Ω—ã–π —Ç–µ–∫—Å—Ç.

### Anti-Pattern Categories

#### 1. Chapter Headers and Epigraphs
```python
CHAPTER_HEADER_PATTERNS = [
    r"^–ì–ª–∞–≤–∞\s+(–ø–µ—Ä–≤–∞—è|–≤—Ç–æ—Ä–∞—è|—Ç—Ä–µ—Ç—å—è|\d+)",
    r"^–ì–õ–ê–í–ê\s+[IVX\d]+",
    r"^–ß–∞—Å—Ç—å\s+\d+",
    r"^–ö–Ω–∏–≥–∞\s+\d+",
    r"^–ü—Ä–æ–ª–æ–≥$|^–≠–ø–∏–ª–æ–≥$",
]

EPIGRAPH_PATTERNS = [
    r"^[¬´"].*?[¬ª"]$",  # Quoted epigraphs
    r"¬©.*?\d{4}",      # Copyright
    r"^\s*\*\s*\*\s*\*\s*$",  # Section dividers
]
```

#### 2. Dialog Detection
```python
DIALOG_PATTERNS = [
    r"^‚Äî\s*",                    # Em dash dialog
    r"^-\s+[–ê-–Ø–Å]",              # Hyphen dialog
    r"[¬´"].*?[¬ª"]",              # Quoted speech
    r"\b—Å–∫–∞–∑–∞–ª\b|\b–≥–æ–≤–æ—Ä–∏–ª\b|\b–æ—Ç–≤–µ—Ç–∏–ª\b|\b–ø—Ä–æ–∏–∑–Ω—ë—Å\b",  # Speech verbs
]

def is_dialog(self, text: str) -> bool:
    # Check for direct speech markers
    if re.search(r'^[‚Äî-]\s*', text):
        return True

    # Check for speech verb + quoted text pattern
    if re.search(r'(—Å–∫–∞–∑–∞–ª|—Å–ø—Ä–æ—Å–∏–ª|–æ—Ç–≤–µ—Ç–∏–ª).*?[¬´"]', text):
        return True

    # High punctuation ratio (dialogs have many commas, dashes)
    punct_ratio = len(re.findall(r'[,‚Äî!?]', text)) / len(text.split())
    if punct_ratio > 0.3:
        return True

    return False
```

#### 3. Author's Remarks and Meta-Text
```python
META_TEXT_PATTERNS = [
    r"\(–ø—Ä–∏–º–µ—á\.\s*(?:–∞–≤—Ç–æ—Ä–∞|—Ä–µ–¥–∞–∫—Ç–æ—Ä–∞|–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞)\)",
    r"\[.*?\]",  # Square bracket notes
    r"—Å–º\.\s+–≥–ª–∞–≤—É\s+\d+",  # Cross-references
    r"–¥–∞–ª–µ–µ\s+–≤\s+—Ç–µ–∫—Å—Ç–µ",
    r"–∫–∞–∫\s+(?:—Å–∫–∞–∑–∞–Ω–æ|—É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å|–∏–∑–≤–µ—Å—Ç–Ω–æ)",
]
```

#### 4. Incomplete Sentences
```python
def is_incomplete_sentence(self, text: str) -> bool:
    # Starts with lowercase or punctuation
    if text[0].islower() or text[0] in ',.:;‚Äî':
        return True

    # Ends abruptly without proper punctuation
    if not text.rstrip()[-1] in '.!?':
        return True

    # Very short fragments
    if len(text.split()) < 5:
        return True

    return False
```

### Filter Pipeline
```python
def should_filter_out(self, text: str) -> Tuple[bool, str]:
    """
    Returns (should_filter, reason)
    """
    # Check all anti-patterns
    if any(re.search(p, text) for p in CHAPTER_HEADER_PATTERNS):
        return (True, "chapter_header")

    if self.is_dialog(text):
        return (True, "dialog")

    if any(re.search(p, text) for p in META_TEXT_PATTERNS):
        return (True, "meta_text")

    if self.is_incomplete_sentence(text):
        return (True, "incomplete")

    return (False, "")
```

---

## Strategy 3: Improved Type Classification

### Goal
–ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: LOCATION (48%), CHARACTER (32%), ATMOSPHERE (16%), OBJECT (4%)

### Hierarchical Classification

#### Priority 1: LOCATION (highest priority)
```python
LOCATION_STRONG_INDICATORS = {
    "nouns": [
        "–≥–æ—Ä–æ–¥", "–¥–µ—Ä–µ–≤–Ω—è", "–∑–∞–º–æ–∫", "–¥–≤–æ—Ä–µ—Ü", "–∫—Ä–µ–ø–æ—Å—Ç—å",
        "–ª–µ—Å", "–ø–æ–ª–µ", "—Ä–µ–∫–∞", "–æ–∑–µ—Ä–æ", "–º–æ—Ä–µ", "–≥–æ—Ä–∞",
        "—É–ª–∏—Ü–∞", "–ø–ª–æ—â–∞–¥—å", "–¥–æ–º", "–∑–¥–∞–Ω–∏–µ", "–±–∞—à–Ω—è",
        "–∫–æ–º–Ω–∞—Ç–∞", "–∑–∞–ª", "–ø–∞–ª–∞—Ç–∞", "–∫–æ—Ä–∏–¥–æ—Ä",
    ],
    "verbs": ["—Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª—Å—è", "–Ω–∞—Ö–æ–¥–∏–ª—Å—è", "–ø—Ä–æ—Å—Ç–∏—Ä–∞–ª—Å—è", "–≤–∏–¥–Ω–µ–ª—Å—è"],
    "patterns": [
        r"(?:–≤|–Ω–∞)\s+(?:—Å–µ–≤–µ—Ä–µ|—é–≥–µ|–∑–∞–ø–∞–¥–µ|–≤–æ—Å—Ç–æ–∫–µ)",
        r"(?:–Ω–∞–¥|–ø–æ–¥|–º–µ–∂–¥—É)\s+(?:–≥–æ—Ä–∞–º–∏|—Ä–µ–∫–∞–º–∏|—Ö–æ–ª–º–∞–º–∏)",
    ],
}

def is_location_description(self, doc, text: str) -> float:
    score = 0.0

    # Count location nouns
    location_count = sum(
        1 for token in doc
        if token.lemma_.lower() in LOCATION_STRONG_INDICATORS["nouns"]
    )
    score += location_count * 0.2

    # Spatial prepositions boost
    spatial_preps = ["–Ω–∞–¥", "–ø–æ–¥", "–≤–æ–∫—Ä—É–≥", "–º–µ–∂–¥—É", "—Ä—è–¥–æ–º —Å"]
    spatial_count = sum(1 for prep in spatial_preps if prep in text.lower())
    score += spatial_count * 0.15

    # Geographic/architectural vocabulary
    if re.search(r'\b(–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞|–∑–¥–∞–Ω–∏–µ|—Å—Ç—Ä–æ–µ–Ω–∏–µ|—Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ)\b', text):
        score += 0.3

    return min(1.0, score)
```

#### Priority 2: CHARACTER (second priority)
```python
CHARACTER_STRONG_INDICATORS = {
    "appearance": [
        "–ª–∏—Ü–æ", "–≥–ª–∞–∑–∞", "–≤–æ–ª–æ—Å—ã", "—Ä—É–∫–∏", "—Ä–æ—Å—Ç", "—Ñ–∏–≥—É—Ä–∞",
        "–æ–¥–µ–∂–¥–∞", "–ø–ª–∞—â", "–¥–æ—Å–ø–µ—Ö–∏", "—à–ª–µ–º", "—Å–∞–ø–æ–≥–∏",
    ],
    "characteristics": [
        "–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π", "—Ö—É–¥–æ–π", "–ø–æ–ª–Ω—ã–π", "–º–æ–ª–æ–¥–æ–π", "—Å—Ç–∞—Ä—ã–π",
        "–±–ª–µ–¥–Ω—ã–π", "–∑–∞–≥–æ—Ä–µ–ª—ã–π", "—Å–µ–¥–æ–π", "—Ä—ã–∂–∏–π", "–±–µ–ª–æ–∫—É—Ä—ã–π",
    ],
    "patterns": [
        r"(?:–µ–≥–æ|–µ—ë|–∏—Ö)\s+(?:–ª–∏—Ü–æ|–≥–ª–∞–∑–∞|–≤–æ–ª–æ—Å—ã|—Ä—É–∫–∏)",
        r"\b(?:–≤—ã–≥–ª—è–¥–µ–ª|–∫–∞–∑–∞–ª—Å—è|–±—ã–ª\s+–ø–æ—Ö–æ–∂)",
    ],
}

def is_character_description(self, doc, text: str) -> float:
    score = 0.0

    # Named entities (PERSON)
    persons = [ent for ent in doc.ents if ent.label_ == "PER"]
    if persons:
        score += 0.4

    # Appearance vocabulary
    appearance_count = sum(
        1 for token in doc
        if token.lemma_.lower() in CHARACTER_STRONG_INDICATORS["appearance"]
    )
    score += appearance_count * 0.15

    # Physical characteristics
    char_adj_count = sum(
        1 for token in doc
        if token.pos_ == "ADJ" and
        token.lemma_.lower() in CHARACTER_STRONG_INDICATORS["characteristics"]
    )
    score += char_adj_count * 0.1

    return min(1.0, score)
```

#### Priority 3: ATMOSPHERE
```python
ATMOSPHERE_STRONG_INDICATORS = {
    "weather": ["–≤–µ—Ç–µ—Ä", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥", "—Ç—É–º–∞–Ω", "—Å–æ–ª–Ω—Ü–µ", "–æ–±–ª–∞–∫–∞"],
    "lighting": ["—Å–≤–µ—Ç", "—Ç–µ–Ω—å", "—Å—É–º—Ä–∞–∫", "—Ä–∞—Å—Å–≤–µ—Ç", "–∑–∞–∫–∞—Ç", "–ø–æ–ª–¥–µ–Ω—å"],
    "mood": ["–º—Ä–∞—á–Ω—ã–π", "—Å–≤–µ—Ç–ª—ã–π", "—Ç–∏—Ö–∏–π", "—Ç—Ä–µ–≤–æ–∂–Ω—ã–π", "—Å–ø–æ–∫–æ–π–Ω—ã–π"],
    "sensory": ["–∑–∞–ø–∞—Ö", "–∞—Ä–æ–º–∞—Ç", "–∑–≤—É–∫", "—à—É–º", "—Ç–∏—à–∏–Ω–∞", "—Ö–æ–ª–æ–¥", "—Ç–µ–ø–ª–æ"],
}

def is_atmosphere_description(self, doc, text: str) -> float:
    score = 0.0

    # Weather and lighting
    weather_count = sum(
        1 for w in ATMOSPHERE_STRONG_INDICATORS["weather"]
        if w in text.lower()
    )
    score += weather_count * 0.2

    # Sensory vocabulary
    sensory_count = sum(
        1 for s in ATMOSPHERE_STRONG_INDICATORS["sensory"]
        if s in text.lower()
    )
    score += sensory_count * 0.15

    # Time of day indicators
    if re.search(r'\b(—É—Ç—Ä–æ|–¥–µ–Ω—å|–≤–µ—á–µ—Ä|–Ω–æ—á—å|—Ä–∞—Å—Å–≤–µ—Ç|–∑–∞–∫–∞—Ç)\b', text):
        score += 0.25

    return min(1.0, score)
```

#### Decision Logic
```python
def classify_description_type(self, doc, text: str) -> DescriptionType:
    scores = {
        DescriptionType.LOCATION: self.is_location_description(doc, text),
        DescriptionType.CHARACTER: self.is_character_description(doc, text),
        DescriptionType.ATMOSPHERE: self.is_atmosphere_description(doc, text),
    }

    # Require minimum threshold
    max_score = max(scores.values())
    if max_score < 0.4:
        return DescriptionType.OBJECT  # Fallback

    # Return type with highest score
    return max(scores, key=scores.get)
```

---

## Strategy 4: Confidence Score Overhaul

### Current Problem
```python
# –ü–õ–û–•–û: –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ ADJ/NOUN
confidence = 0.5 + (adj_count / (noun_count + adj_count)) * 0.3
# –†–µ–∑—É–ª—å—Ç–∞—Ç: –í—Å–µ –ø–æ–ª—É—á–∞—é—Ç 0.5-0.8, –Ω–µ—Ç discrimination
```

### New Multi-Factor Scoring

```python
def calculate_description_confidence(
    self,
    doc,
    text: str,
    desc_type: DescriptionType
) -> float:
    """
    Multi-factor confidence scoring –¥–ª—è literary descriptions.

    Factors:
    1. Linguistic quality (30%)
    2. Visual richness (25%)
    3. Structural completeness (20%)
    4. Type-specific indicators (15%)
    5. Length appropriateness (10%)
    """

    # Factor 1: Linguistic Quality (30%)
    linguistic_score = self._score_linguistic_quality(doc)

    # Factor 2: Visual Richness (25%)
    visual_score = self._score_visual_richness(doc, text)

    # Factor 3: Structural Completeness (20%)
    structure_score = self._score_structural_completeness(text)

    # Factor 4: Type-Specific Indicators (15%)
    type_score = self._score_type_specificity(doc, text, desc_type)

    # Factor 5: Length Appropriateness (10%)
    length_score = self._score_length_for_flux(text)

    # Weighted sum
    confidence = (
        linguistic_score * 0.30 +
        visual_score * 0.25 +
        structure_score * 0.20 +
        type_score * 0.15 +
        length_score * 0.10
    )

    return confidence

def _score_linguistic_quality(self, doc) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.
    """
    score = 0.0

    # Rich adjective usage
    adj_tokens = [t for t in doc if t.pos_ == "ADJ"]
    noun_tokens = [t for t in doc if t.pos_ == "NOUN"]

    if noun_tokens:
        adj_ratio = len(adj_tokens) / len(noun_tokens)
        # Optimal: 0.3-0.7 adjectives per noun
        if 0.3 <= adj_ratio <= 0.7:
            score += 0.4
        elif 0.1 <= adj_ratio < 0.3:
            score += 0.2

    # Syntactic complexity (using dependency depth)
    depths = [len(list(token.ancestors)) for token in doc]
    avg_depth = sum(depths) / len(depths) if depths else 0
    # Literary descriptions: depth 2-4
    if 2 <= avg_depth <= 4:
        score += 0.3

    # Variety of POS tags (–Ω–µ —Ç–æ–ª—å–∫–æ ADJ+NOUN)
    pos_variety = len(set(t.pos_ for t in doc))
    if pos_variety >= 6:  # ADJ, NOUN, VERB, ADV, ADP, DET
        score += 0.3

    return score

def _score_visual_richness(self, doc, text: str) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏ (–≤–∞–∂–Ω–æ –¥–ª—è Flux).
    """
    score = 0.0

    # Color words
    colors = ["–±–µ–ª—ã–π", "—á–µ—Ä–Ω—ã–π", "—Å–µ—Ä—ã–π", "–∫—Ä–∞—Å–Ω—ã–π", "–∑–µ–ª–µ–Ω—ã–π", "—Å–∏–Ω–∏–π",
              "–∂–µ–ª—Ç—ã–π", "–∑–æ–ª–æ—Ç–æ–π", "—Å–µ—Ä–µ–±—Ä—è–Ω—ã–π", "—Ç–µ–º–Ω—ã–π", "—Å–≤–µ—Ç–ª—ã–π"]
    color_count = sum(1 for c in colors if c in text.lower())
    score += min(0.3, color_count * 0.1)

    # Size/scale descriptors
    scale = ["–æ–≥—Ä–æ–º–Ω—ã–π", "–±–æ–ª—å—à–æ–π", "–º–∞–ª–µ–Ω—å–∫–∏–π", "–∫—Ä–æ—à–µ—á–Ω—ã–π", "–≥—Ä–æ–º–∞–¥–Ω—ã–π",
             "–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π", "—à–∏—Ä–æ–∫–∏–π", "—É–∑–∫–∏–π", "–¥–ª–∏–Ω–Ω—ã–π", "–∫–æ—Ä–æ—Ç–∫–∏–π"]
    scale_count = sum(1 for s in scale if s in text.lower())
    score += min(0.25, scale_count * 0.1)

    # Lighting vocabulary
    lighting = ["—Å–≤–µ—Ç", "—Ç–µ–Ω—å", "—Å—É–º—Ä–∞–∫", "—è—Ä–∫–∏–π", "—Ç—É—Å–∫–ª—ã–π", "–±–ª–µ—Å—Ç—è—â–∏–π",
                "–æ—Å–≤–µ—â–µ–Ω–Ω—ã–π", "—Ç–µ–º–Ω—ã–π", "—Å–∏—è—é—â–∏–π"]
    lighting_count = sum(1 for l in lighting if l in text.lower())
    score += min(0.25, lighting_count * 0.1)

    # Texture/material words
    texture = ["–≥–ª–∞–¥–∫–∏–π", "—à–µ—Ä—à–∞–≤—ã–π", "–º—è–≥–∫–∏–π", "—Ç–≤–µ—Ä–¥—ã–π", "–∫–∞–º–µ–Ω–Ω—ã–π",
               "–¥–µ—Ä–µ–≤—è–Ω–Ω—ã–π", "–º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–π"]
    texture_count = sum(1 for t in texture if t in text.lower())
    score += min(0.2, texture_count * 0.1)

    return score

def _score_structural_completeness(self, text: str) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ—Å—Ç–∏.
    """
    score = 0.0

    # Starts with capital letter
    if text[0].isupper():
        score += 0.3
    else:
        return 0.0  # Incomplete sentence - disqualify

    # Ends with proper punctuation
    if text.rstrip()[-1] in '.!?':
        score += 0.3
    else:
        score += 0.1  # Partial credit

    # Multiple complete sentences (multi-sentence descriptions better)
    sentence_count = len([s for s in text.split('.') if len(s.strip()) > 20])
    if sentence_count >= 2:
        score += 0.4
    elif sentence_count == 1:
        score += 0.2

    return score

def _score_type_specificity(
    self,
    doc,
    text: str,
    desc_type: DescriptionType
) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–∏–ø—É –æ–ø–∏—Å–∞–Ω–∏—è.
    """
    if desc_type == DescriptionType.LOCATION:
        return self.is_location_description(doc, text)
    elif desc_type == DescriptionType.CHARACTER:
        return self.is_character_description(doc, text)
    elif desc_type == DescriptionType.ATMOSPHERE:
        return self.is_atmosphere_description(doc, text)
    else:
        return 0.5  # OBJECT - neutral

def _score_length_for_flux(self, text: str) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã –¥–ª—è Flux requirements.
    """
    length = len(text)

    # Optimal: 150-500 chars
    if 150 <= length <= 500:
        return 1.0

    # Acceptable: 100-150 or 500-800
    elif (100 <= length < 150) or (500 < length <= 800):
        return 0.7

    # Marginal: 80-100 or 800-1000
    elif (80 <= length < 100) or (800 < length <= 1000):
        return 0.4

    # Too short or too long
    else:
        return 0.0
```

---

## Strategy 5: Threshold Optimization

### Current Settings (–ü–õ–û–•–ò–ï)
```python
# backend/app/services/enhanced_nlp_system.py
MIN_DESCRIPTION_LENGTH = 50        # ‚Üê –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è Flux
MAX_DESCRIPTION_LENGTH = 1000      # ‚Üê OK
MIN_WORD_COUNT = 10                # ‚Üê –°–ª–∏—à–∫–æ–º –º–∞–ª–æ
CONFIDENCE_THRESHOLD = 0.3         # ‚Üê –ö–†–ò–¢–ò–ß–ï–°–ö–ò –Ω–∏–∑–∫–∏–π
```

### Optimized Settings for Flux
```python
# NEW SETTINGS - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è Flux
DESCRIPTION_EXTRACTION_CONFIG = {
    # Length constraints (aligned with Flux)
    "min_char_length": 100,           # ‚Üê Minimum for Flux
    "max_char_length": 1000,          # ‚Üê Maximum (soft limit)
    "optimal_char_length": (150, 500), # ‚Üê Sweet spot for Flux
    "min_word_count": 15,             # ‚Üê ~15 words = ~100 chars

    # Quality thresholds
    "min_confidence_score": 0.50,     # ‚Üê Raised from 0.3
    "optimal_confidence_score": 0.70, # ‚Üê Target quality
    "min_visual_richness": 0.30,      # ‚Üê Require visual vocabulary

    # Type-specific thresholds
    "location_min_confidence": 0.55,   # ‚Üê Higher for locations
    "character_min_confidence": 0.50,
    "atmosphere_min_confidence": 0.45,
    "object_min_confidence": 0.60,     # ‚Üê Highest (discourage)

    # Multi-sentence requirements
    "prefer_multi_sentence": True,
    "min_sentences_for_boost": 2,      # ‚Üê 2+ sentences get bonus
    "max_sentences": 5,                # ‚Üê Cap at 5 sentences

    # Filtering aggressiveness
    "strict_anti_pattern_filtering": True,
    "filter_incomplete_sentences": True,
    "filter_dialog": True,
    "filter_meta_text": True,
}
```

### Priority Score Adjustment

–û–±–Ω–æ–≤–∏—Ç—å `backend/app/models/description.py`:
```python
def calculate_priority_score(self) -> float:
    """
    –ù–û–í–ê–Ø –§–û–†–ú–£–õ–ê –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è Flux generation.
    """
    if not self.is_suitable_for_generation:
        return 0.0

    # Base type priority (unchanged)
    type_priority = self.get_type_priority()

    # Confidence weight (increased importance)
    confidence_weight = float(self.confidence_score or 0) * 30  # ‚Üê Was 20

    # NEW: Length scoring for Flux
    content_length = len(self.content or "")
    length_score = 0.0

    if 150 <= content_length <= 500:
        # Optimal range for Flux
        length_score = 25  # ‚Üê Maximum bonus
    elif 100 <= content_length < 150:
        # Acceptable minimum
        length_score = 15
    elif 500 < content_length <= 800:
        # Good but long
        length_score = 20
    elif 80 <= content_length < 100:
        # Marginal
        length_score = 5
    else:
        # Too short (<80) or too long (>800)
        length_score = 0

    # NEW: Visual richness bonus
    visual_bonus = 0.0
    if self.complexity_level == "complex":
        visual_bonus = 10
    elif self.complexity_level == "medium":
        visual_bonus = 5

    return min(100.0,
               type_priority +
               confidence_weight +
               length_score +
               visual_bonus)
```

---

## Strategy 6: Cross-Chapter Context Tracking

### Goal
Maintain consistency for character and location descriptions across chapters.

### Context Manager Architecture
```python
class DescriptionContextManager:
    """
    Tracks entities (characters, locations) across chapters –¥–ª—è consistency.
    """

    def __init__(self):
        self.entity_registry = {
            "characters": {},  # name -> canonical_description
            "locations": {},   # name -> canonical_description
        }
        self.chapter_contexts = {}  # chapter_id -> context

    def register_entity(
        self,
        entity_name: str,
        entity_type: str,
        description: str,
        chapter_id: str
    ):
        """
        Register a new entity or update existing.
        """
        registry = self.entity_registry[f"{entity_type}s"]

        if entity_name in registry:
            # Entity already seen - merge descriptions
            existing = registry[entity_name]
            registry[entity_name] = self._merge_descriptions(
                existing, description
            )
        else:
            # New entity
            registry[entity_name] = {
                "canonical_description": description,
                "first_seen_chapter": chapter_id,
                "mentions": [chapter_id],
            }

    def get_entity_context(self, entity_name: str, entity_type: str) -> Optional[str]:
        """
        Get canonical description for an entity.
        """
        registry = self.entity_registry[f"{entity_type}s"]
        if entity_name in registry:
            return registry[entity_name]["canonical_description"]
        return None

    def _merge_descriptions(
        self,
        existing: Dict,
        new_description: str
    ) -> Dict:
        """
        Merge new description with existing canonical description.
        Keep longest and most detailed.
        """
        old_desc = existing["canonical_description"]

        # Choose longer and more detailed
        if len(new_description) > len(old_desc) * 1.2:
            existing["canonical_description"] = new_description

        return existing

    def enrich_description_with_context(
        self,
        description: str,
        entities_mentioned: List[str]
    ) -> str:
        """
        Enrich current description with context from previous chapters.
        """
        enriched = description

        for entity in entities_mentioned:
            # Try to find entity in registry
            context = self.get_entity_context(entity, "character")
            if not context:
                context = self.get_entity_context(entity, "location")

            if context:
                # Add context as prefix (for Flux prompt)
                enriched = f"{context}. {enriched}"
                break  # Add context for one main entity only

        return enriched
```

### Integration with Processors
```python
# In multi_nlp_manager.py
async def extract_descriptions(
    self,
    text: str,
    chapter_id: str = None,
    book_id: str = None
) -> ProcessingResult:
    # ... existing extraction logic ...

    # NEW: Context tracking
    if book_id:
        context_mgr = await self.get_or_create_context_manager(book_id)

        for desc in result.descriptions:
            # Register entities
            if desc.entities_mentioned:
                for entity in desc.entities_mentioned:
                    context_mgr.register_entity(
                        entity,
                        desc.type.value,
                        desc.content,
                        chapter_id
                    )

            # Enrich with context
            if desc.type in [DescriptionType.CHARACTER, DescriptionType.LOCATION]:
                enriched = context_mgr.enrich_description_with_context(
                    desc.content,
                    desc.entities_mentioned or []
                )
                desc.content = enriched

    return result
```

---

## Implementation Plan

### Phase 1: Core Improvements (Priority P0)
**–°—Ä–æ–∫:** 1-2 –Ω–µ–¥–µ–ª–∏

#### Task 1.1: Description Boundary Detection
- [ ] Implement `extract_complete_description()` method
- [ ] Add sentence window analysis
- [ ] Add continuation/end signal detection
- [ ] Test on sample chapters

**Files to modify:**
- `backend/app/services/enhanced_nlp_system.py` (+200 lines)

#### Task 1.2: Anti-Pattern Filtering
- [ ] Implement `should_filter_out()` pipeline
- [ ] Add chapter header detection
- [ ] Add dialog detection
- [ ] Add meta-text filtering
- [ ] Test filter effectiveness

**Files to modify:**
- `backend/app/services/enhanced_nlp_system.py` (+150 lines)
- `backend/app/services/natasha_processor.py` (+100 lines)

#### Task 1.3: Confidence Score Overhaul
- [ ] Implement `calculate_description_confidence()` multi-factor scoring
- [ ] Add linguistic quality scoring
- [ ] Add visual richness scoring
- [ ] Add structural completeness scoring
- [ ] Test score discrimination

**Files to modify:**
- `backend/app/services/enhanced_nlp_system.py` (+250 lines)

#### Task 1.4: Threshold Optimization
- [ ] Update config values for Flux requirements
- [ ] Update `Description.calculate_priority_score()`
- [ ] Add Flux-specific length scoring

**Files to modify:**
- `backend/app/services/enhanced_nlp_system.py` (config update)
- `backend/app/models/description.py` (method rewrite)

---

### Phase 2: Advanced Features (Priority P1)
**–°—Ä–æ–∫:** 2-3 –Ω–µ–¥–µ–ª–∏

#### Task 2.1: Improved Type Classification
- [ ] Implement hierarchical classification
- [ ] Add type-specific scorers
- [ ] Implement decision logic with thresholds
- [ ] Re-balance type distribution

**Files to modify:**
- `backend/app/services/enhanced_nlp_system.py` (+300 lines)
- `backend/app/services/natasha_processor.py` (+150 lines)

#### Task 2.2: Cross-Chapter Context Tracking
- [ ] Implement `DescriptionContextManager`
- [ ] Add entity registry
- [ ] Add description merging
- [ ] Integrate with multi_nlp_manager

**Files to create:**
- `backend/app/services/description_context_manager.py` (new file, ~400 lines)

**Files to modify:**
- `backend/app/services/multi_nlp_manager.py` (+100 lines)

---

### Phase 3: Testing & Validation (Priority P0)
**–°—Ä–æ–∫:** 1 –Ω–µ–¥–µ–ª—è

#### Task 3.1: Re-parse Test Book
- [ ] Drop existing descriptions from DB
- [ ] Re-parse "–í–µ–¥—å–º–∞–∫" with new system
- [ ] Compare results: old vs new

#### Task 3.2: Validation Metrics
- [ ] Measure type distribution
- [ ] Measure average length
- [ ] Measure confidence score distribution
- [ ] Check for fragments/incomplete sentences
- [ ] Manual review of top 100 descriptions

#### Task 3.3: Flux Integration Test
- [ ] Generate images for top 50 descriptions
- [ ] Visual quality assessment
- [ ] Identify remaining issues

---

## Expected Results

### Quantitative Improvements

#### Type Distribution
```
BEFORE (Current):
  OBJECT:      672 (53.5%)
  LOCATION:    503 (40.0%)
  CHARACTER:    61 (4.9%)
  ATMOSPHERE:   21 (1.7%)

AFTER (Target):
  LOCATION:    ~600 (48%)   ‚Üê +97 descriptions, proper priority
  CHARACTER:   ~400 (32%)   ‚Üê +339 descriptions, HUGE improvement
  ATMOSPHERE:  ~200 (16%)   ‚Üê +179 descriptions, proper coverage
  OBJECT:       ~57 (4%)    ‚Üê -615 descriptions, minimized
```

#### Length Statistics
```
BEFORE (Current):
  Average:     104.7 chars  (too short for Flux)
  Median:      102 chars
  < 100 chars: 587 (46.7%)
  100-500:     670 (53.3%)
  > 500:       0 (0.0%)

AFTER (Target):
  Average:     ~250 chars   (optimal for Flux)
  Median:      ~220 chars
  < 100 chars: <50 (< 5%)   ‚Üê Dramatically reduced
  100-500:     ~1100 (85%)  ‚Üê Optimal range
  > 500:       ~150 (10%)   ‚Üê Long detailed descriptions
```

#### Quality Metrics
```
BEFORE (Current):
  Top 10 confidence: Garbage (chapter headers, fragments)
  Avg confidence:    0.45
  Complete sents:    ~30%
  Visualizable:      ~20%

AFTER (Target):
  Top 10 confidence: High-quality complete descriptions
  Avg confidence:    0.65   ‚Üê Higher threshold effect
  Complete sents:    >95%   ‚Üê Boundary detection
  Visualizable:      >80%   ‚Üê Visual richness scoring
```

### Qualitative Improvements

#### Before (Current System)
```
‚ùå "–∑–≤–µ–∑–¥—ã. –ñ–∞–Ω-–ê–Ω—Ç–µ–ª—å–º –ë—Ä–∏–ª—å—è-–°–∞–≤–∞—Ä–µ–Ω –ì–ª–∞–≤–∞ —Å–µ–¥—å–º–∞—è –ï–∂–µ–º–µ—Å—è—á–Ω–æ–µ..."
   - Fragment
   - Chapter header mixed in
   - Not visualizable

‚ùå "–º–∞—Ä—Ö–∏–∏. –ú–∞—Ä–∫–≥—Ä–∞—Ñ—Å—Ç–≤ —ç—Ç–∏—Ö —á–µ—Ç—ã—Ä–µ: –ó–∞–ø–∞–¥–Ω–æ–µ, –í–µ—Ä—Ö–Ω–µ–µ, –û–∑—ë—Ä–Ω–æ–µ..."
   - Starts with lowercase (fragment)
   - Informational, not descriptive
   - Not suitable for image generation
```

#### After (New System)
```
‚úÖ "–ú–∞—Å—Å–∏–≤–Ω–∞—è –∫—Ä–µ–ø–æ—Å—Ç—å –≤–æ–∑–≤—ã—à–∞–ª–∞—Å—å –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ —Ö–æ–ª–º–∞, –µ—ë —Å–µ—Ä—ã–µ –∫–∞–º–µ–Ω–Ω—ã–µ
—Å—Ç–µ–Ω—ã –æ—Ç–±—Ä–∞—Å—ã–≤–∞–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–Ω–∏ –Ω–∞ –æ–∫—Ä—É–∂–∞—é—â–∏–π –ª–µ—Å. –ß–µ—Ç—ã—Ä–µ –≤—ã—Å–æ–∫–∏–µ
–±–∞—à–Ω–∏ –ø–æ —É–≥–ª–∞–º –±—ã–ª–∏ —É–≤–µ–Ω—á–∞–Ω—ã –æ—Å—Ç—Ä–æ–∫–æ–Ω–µ—á–Ω—ã–º–∏ —à–ø–∏–ª—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ
–ø—Ä–æ–Ω–∑–∞–ª–∏ –Ω–∏–∑–∫–∏–µ –æ–±–ª–∞–∫–∞. –í —Å–≤–µ—Ç–µ –∑–∞—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–ª–Ω—Ü–∞ –∫–∞–º–Ω–∏ –ø—Ä–∏–æ–±—Ä–µ–ª–∏
—Ç—ë–ø–ª—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –æ—Ç—Ç–µ–Ω–æ–∫."
   (237 chars, LOCATION, confidence: 0.85)
   ‚úÖ Complete multi-sentence description
   ‚úÖ Rich visual vocabulary (colors, lighting, architecture)
   ‚úÖ Perfect for Flux image generation

‚úÖ "–ì–µ—Ä–∞–ª—å—Ç –±—ã–ª –≤—ã—Å–æ–∫–∏–º –º—É–∂—á–∏–Ω–æ–π —Å –±–µ–ª–æ—Å–Ω–µ–∂–Ω—ã–º–∏ –≤–æ–ª–æ—Å–∞–º–∏ –∏
—è–Ω—Ç–∞—Ä–Ω—ã–º–∏ –≥–ª–∞–∑–∞–º–∏ –∫–æ—à–∫–∏. –®—Ä–∞–º—ã –ø–µ—Ä–µ—Å–µ–∫–∞–ª–∏ –µ–≥–æ –∑–∞–≥–æ—Ä–µ–ª–æ–µ –ª–∏—Ü–æ,
–ø—Ä–∏–¥–∞–≤–∞—è —Å—É—Ä–æ–≤–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ. –ù–∞ –ø–ª–µ—á–∞—Ö –ø–æ–∫–æ–∏–ª—Å—è —Ç—ë–º–Ω—ã–π –∫–æ–∂–∞–Ω—ã–π
–ø–ª–∞—â, –ø–æ–¥ –∫–æ—Ç–æ—Ä—ã–º –≤–∏–¥–Ω–µ–ª–∏—Å—å —Ä—É–∫–æ—è—Ç–∏ –¥–≤—É—Ö –º–µ—á–µ–π - —Å—Ç–∞–ª—å–Ω–æ–≥–æ –∏
—Å–µ—Ä–µ–±—Ä—è–Ω–æ–≥–æ."
   (204 chars, CHARACTER, confidence: 0.92)
   ‚úÖ Detailed character description
   ‚úÖ Physical features + clothing
   ‚úÖ Visualizable for portrait generation
```

---

## Risk Assessment & Mitigation

### Risk 1: Over-Filtering
**Description:** –°–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –º–æ–≥—É—Ç –æ—Ç–±—Ä–æ—Å–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è.

**Mitigation:**
- –°–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ descriptions –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É `filtered_descriptions` –¥–ª—è review
- –î–æ–±–∞–≤–∏—Ç—å admin endpoint –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö
- A/B testing —Å —Ä–∞–∑–Ω—ã–º–∏ threshold values

### Risk 2: Performance Impact
**Description:** Multi-sentence window analysis –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥.

**Mitigation:**
- Benchmark current vs new system
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å sentence analysis
- Parallel processing –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≥–ª–∞–≤
- Target: < 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –≥–ª–∞–≤—É (acceptable per user requirements)

### Risk 3: Russian Language Specifics
**Description:** –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —Å–ª–æ–∂–Ω–µ–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—É—á–µ–Ω—ã –º–æ–¥–µ–ª–∏.

**Mitigation:**
- Ensemble approach —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (SpaCy + Natasha + Stanza)
- Emphasize rule-based patterns –¥–ª—è —Ä—É—Å—Å–∫–æ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏
- Natasha specifically good for literary Russian (per research)
- Continuous improvement —á–µ—Ä–µ–∑ feedback loop

---

## Success Metrics (KPIs)

### Primary KPIs
1. **Type Distribution Accuracy**
   - Target: LOCATION 45-50%, CHARACTER 30-35%, ATMOSPHERE 15-20%
   - Current: OBJECT dominates (53.5%) ‚Üê WRONG

2. **Description Length for Flux**
   - Target: 85%+ in 100-500 char range
   - Current: 53.3% in range, but many too short

3. **Quality Score**
   - Target: Avg confidence > 0.65
   - Current: Avg confidence ~0.45

4. **Fragment Elimination**
   - Target: < 5% incomplete sentences
   - Current: ~70% fragments/incomplete

### Secondary KPIs
5. **Flux Image Generation Success Rate**
   - Target: > 80% of descriptions produce good images
   - Measure through manual review

6. **Processing Time**
   - Target: < 5 seconds per chapter (user-specified acceptable)
   - Current: ~4 seconds (need to maintain)

7. **False Positive Rate (junk descriptions)**
   - Target: < 10%
   - Current: ~50-60% (headers, dialogs, fragments)

---

## Conclusion

–¢–µ–∫—É—â–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ **fundamentally broken** –¥–ª—è Flux image generation:
- –ò–∑–≤–ª–µ–∫–∞–µ—Ç fragments –≤–º–µ—Å—Ç–æ complete descriptions
- –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è type classification (OBJECT dominates)
- –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ descriptions –¥–ª—è Flux
- –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç dialog/meta-text
- Confidence scores —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–æ–±–æ—Ä–æ—Ç

–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç **–≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã**:
1. ‚úÖ Description boundary detection ‚Üí complete multi-sentence descriptions
2. ‚úÖ Anti-pattern filtering ‚Üí eliminate junk (headers, dialogs, fragments)
3. ‚úÖ Improved type classification ‚Üí correct distribution
4. ‚úÖ New confidence scoring ‚Üí discriminate quality
5. ‚úÖ Optimized thresholds ‚Üí aligned with Flux (100-500 chars)
6. ‚úÖ Context tracking ‚Üí consistency across chapters

**Expected outcome:**
- 1200+ high-quality descriptions (vs current 200-300 usable)
- 80%+ suitable for Flux image generation (vs current 20%)
- Correct type distribution matching priorities
- Elimination of fragments, dialogs, service text

**Implementation time:** 4-6 weeks full implementation + testing

---

**Status:** Ready for implementation approval
**Next Step:** Begin Phase 1 - Core Improvements

---

**Document version:** 1.0
**Last updated:** 2025-11-05
**Author:** Claude Code

# Testing Guide - BookReader AI

–í—Å–µ–æ–±—ä–µ–º–ª—é—â–µ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º—ã BookReader AI, –≤–∫–ª—é—á–∞—è unit, integration, E2E —Ç–µ—Å—Ç—ã, —Å–∫—Ä–∏–ø—Ç—ã –∏ best practices.

## –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –ü–∏—Ä–∞–º–∏–¥–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```
E2E Tests (10%)
‚îú‚îÄ‚îÄ Playwright/Cypress
‚îî‚îÄ‚îÄ Critical user journeys

Integration Tests (20%)
‚îú‚îÄ‚îÄ API endpoints
‚îú‚îÄ‚îÄ Database interactions
‚îî‚îÄ‚îÄ Service integrations

Unit Tests (70%)
‚îú‚îÄ‚îÄ Business logic
‚îú‚îÄ‚îÄ Components
‚îî‚îÄ‚îÄ Utilities
```

### Coverage —Ü–µ–ª–∏
- **Unit tests:** >90% coverage
- **Integration tests:** –í—Å–µ API endpoints
- **E2E tests:** –û—Å–Ω–æ–≤–Ω—ã–µ user flows

---

## Backend Testing

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥—ã
**–§–∞–π–ª:** `backend/conftest.py`

```python
@pytest.fixture(scope="session")
async def test_db():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –ë–î."""
    test_engine = create_async_engine(
        "postgresql+asyncpg://test:test@localhost/bookreader_test"
    )
    
    async with test_engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    yield test_engine
    
    async with test_engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

@pytest.fixture
async def test_session(test_db):
    """–°–µ—Å—Å–∏—è –ë–î –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
    async with AsyncSession(test_db) as session:
        yield session
        await session.rollback()

@pytest.fixture
async def test_user(test_session):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    user = User(
        email="test@example.com",
        password_hash="hashed_password",
        full_name="Test User"
    )
    test_session.add(user)
    await test_session.commit()
    return user
```

### Unit —Ç–µ—Å—Ç—ã –º–æ–¥–µ–ª–∏
```python
class TestBookModel:
    async def test_book_creation(self, test_session, test_user):
        book = Book(
            user_id=test_user.id,
            title="Test Book",
            author="Test Author",
            file_path="/test/book.epub",
            file_format=BookFormat.EPUB,
            file_size=1024000
        )
        
        test_session.add(book)
        await test_session.commit()
        
        assert book.id is not None
        assert book.title == "Test Book"
        assert book.is_parsed is False
    
    async def test_reading_progress_calculation(self, test_session):
        book = BookFactory.create()
        
        # –°–æ–∑–¥–∞–µ–º 10 –≥–ª–∞–≤
        for i in range(1, 11):
            chapter = ChapterFactory.create(
                book_id=book.id,
                chapter_number=i
            )
            test_session.add(chapter)
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –Ω–∞ 5 –≥–ª–∞–≤–µ
        progress = ReadingProgress(
            user_id=book.user_id,
            book_id=book.id,
            current_chapter=5
        )
        test_session.add(progress)
        await test_session.commit()
        
        progress_percent = book.get_reading_progress_percent(book.user_id)
        assert progress_percent == 40.0  # 4/10 –≥–ª–∞–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ
```

### –¢–µ—Å—Ç—ã —Å–µ—Ä–≤–∏—Å–æ–≤
```python
class TestNLPProcessor:
    @pytest.fixture
    def nlp_processor(self, test_session):
        return NLPProcessor(test_session)
    
    async def test_location_extraction(self, nlp_processor):
        text = "–í –¥—Ä–µ–≤–Ω–µ–º –∑–∞–º–∫–µ –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ —Ö–æ–ª–º–∞ –∂–∏–ª–∏ –ø—Ä–∏–≤–∏–¥–µ–Ω–∏—è."
        chapter_id = uuid4()
        
        descriptions = await nlp_processor.extract_descriptions_from_text(
            text, chapter_id
        )
        
        location_descs = [d for d in descriptions if d.type == DescriptionType.LOCATION]
        assert len(location_descs) > 0
        
        location = location_descs[0]
        assert "–∑–∞–º–æ–∫" in location.content.lower()
        assert location.confidence_score >= 0.7
        assert location.priority_score >= 70.0
    
    @pytest.mark.parametrize("text,expected_type", [
        ("–≤—ã—Å–æ–∫–∏–π –º—É–∂—á–∏–Ω–∞ —Å —Å–µ–¥–æ–π –±–æ—Ä–æ–¥–æ–π", DescriptionType.CHARACTER),
        ("–¥—Ä–µ–≤–Ω–∏–π –∫–∞–º–µ–Ω–Ω—ã–π –∑–∞–º–æ–∫", DescriptionType.LOCATION),
        ("–º—Ä–∞—á–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ —Ç—É–º–∞–Ω–∞", DescriptionType.ATMOSPHERE)
    ])
    async def test_description_classification(self, nlp_processor, text, expected_type):
        descriptions = await nlp_processor.extract_descriptions_from_text(
            text, uuid4()
        )
        
        assert any(d.type == expected_type for d in descriptions)
```

### API —Ç–µ—Å—Ç—ã
```python
class TestBooksAPI:
    async def test_get_books(self, test_client, test_user, auth_headers):
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∫–Ω–∏–≥–∏
        books = [BookFactory.create(user_id=test_user.id) for _ in range(5)]
        
        response = await test_client.get(
            "/api/v1/books",
            headers=auth_headers
        )
        
        assert response.status_code == 200
        data = response.json()
        assert len(data["books"]) == 5
        assert "pagination" in data
    
    async def test_upload_book(self, test_client, auth_headers):
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π EPUB —Ñ–∞–π–ª
        test_file = create_test_epub_file()
        
        response = await test_client.post(
            "/api/v1/books/upload",
            files={"file": ("test.epub", test_file, "application/epub+zip")},
            headers=auth_headers
        )
        
        assert response.status_code == 201
        data = response.json()
        assert "book" in data
        assert data["book"]["title"]
        assert "processing" in data
    
    async def test_unauthorized_access(self, test_client):
        response = await test_client.get("/api/v1/books")
        assert response.status_code == 401
```

---

## Frontend Testing

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Jest –∏ Testing Library
**–§–∞–π–ª:** `frontend/src/test-utils.tsx`

```typescript
import { render, RenderOptions } from '@testing-library/react';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { BrowserRouter } from 'react-router-dom';

interface CustomRenderOptions extends Omit<RenderOptions, 'wrapper'> {
  initialStores?: {
    auth?: Partial<AuthState>;
    books?: Partial<BooksState>;
  };
}

const AllTheProviders: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const queryClient = new QueryClient({
    defaultOptions: {
      queries: { retry: false },
      mutations: { retry: false }
    }
  });
  
  return (
    <QueryClientProvider client={queryClient}>
      <BrowserRouter>
        {children}
      </BrowserRouter>
    </QueryClientProvider>
  );
};

export const renderWithProviders = (
  ui: React.ReactElement,
  options: CustomRenderOptions = {}
) => {
  const { initialStores, ...renderOptions } = options;
  
  // –°–±—Ä–æ—Å stores –∫ –Ω–∞—á–∞–ª—å–Ω–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é
  if (initialStores?.auth) {
    useAuthStore.setState(initialStores.auth);
  }
  
  return render(ui, { wrapper: AllTheProviders, ...renderOptions });
};

// Custom matchers
export * from '@testing-library/jest-dom';
```

### Component —Ç–µ—Å—Ç—ã
```typescript
describe('BookCard', () => {
  const mockBook: Book = {
    id: '1',
    title: 'Test Book',
    author: 'Test Author',
    genre: BookGenre.FANTASY,
    reading_progress: {
      progress_percentage: 45,
      current_chapter: 3
    }
  };
  
  it('renders book information correctly', () => {
    const onRead = jest.fn();
    
    renderWithProviders(
      <BookCard book={mockBook} view="grid" onRead={onRead} />
    );
    
    expect(screen.getByText('Test Book')).toBeInTheDocument();
    expect(screen.getByText('Test Author')).toBeInTheDocument();
    expect(screen.getByText('45%')).toBeInTheDocument();
  });
  
  it('calls onRead when read button is clicked', async () => {
    const onRead = jest.fn();
    const user = userEvent.setup();
    
    renderWithProviders(
      <BookCard book={mockBook} view="grid" onRead={onRead} />
    );
    
    await user.click(screen.getByRole('button', { name: /—á–∏—Ç–∞—Ç—å/i }));
    
    expect(onRead).toHaveBeenCalledWith(mockBook);
  });
});

describe('BookReader', () => {
  const mockChapterContent = {
    chapter: {
      id: '1',
      title: 'Chapter 1',
      content: 'Test chapter content...'
    },
    descriptions: []
  };
  
  it('displays chapter content', async () => {
    // Mock API response
    server.use(
      rest.get('/api/v1/books/:bookId/chapters/:chapter', (req, res, ctx) => {
        return res(ctx.json(mockChapterContent));
      })
    );
    
    renderWithProviders(<BookReader bookId="test-book-id" />);
    
    expect(await screen.findByText('Chapter 1')).toBeInTheDocument();
    expect(screen.getByText('Test chapter content...')).toBeInTheDocument();
  });
});
```

### Hook —Ç–µ—Å—Ç—ã
```typescript
describe('useBookUpload', () => {
  it('uploads book successfully', async () => {
    const { result } = renderHook(() => useBookUpload(), {
      wrapper: ({ children }) => (
        <QueryClientProvider client={new QueryClient()}>
          {children}
        </QueryClientProvider>
      )
    });
    
    const testFile = new File(['test content'], 'test.epub', {
      type: 'application/epub+zip'
    });
    
    act(() => {
      result.current.mutate({ file: testFile });
    });
    
    await waitFor(() => {
      expect(result.current.isSuccess).toBe(true);
    });
  });
});
```

---

## epub.js & CFI Testing (October 2025)

### Testing EpubReader Component

```typescript
describe('EpubReader', () => {
  it('should restore CFI position on load', async () => {
    const cfi = 'epubcfi(/6/4!/4/2/16/1:0)';
    const mockProgress = {
      reading_location_cfi: cfi,
      scroll_offset_percent: 45.5
    };

    (booksAPI.getReadingProgress as jest.Mock).mockResolvedValue(mockProgress);

    render(<EpubReader book={mockBook} />);

    await waitFor(() => {
      // Check that epub.js display() was called with CFI
      expect(mockRendition.display).toHaveBeenCalledWith(cfi);
    });
  });

  it('should save progress with debounce', async () => {
    jest.useFakeTimers();

    render(<EpubReader book={mockBook} />);

    // Simulate page turn (relocated event)
    const newCfi = 'epubcfi(/6/4!/4/2/20/1:0)';
    mockRendition._triggerEvent('relocated', {
      start: { cfi: newCfi, href: 'chapter2.xhtml' }
    });

    // Should not save immediately
    expect(booksAPI.updateReadingProgress).not.toHaveBeenCalled();

    // Fast-forward 2 seconds (debounce delay)
    jest.advanceTimersByTime(2000);

    await waitFor(() => {
      expect(booksAPI.updateReadingProgress).toHaveBeenCalledWith(
        mockBook.id,
        expect.objectContaining({
          reading_location_cfi: newCfi,
          scroll_offset_percent: expect.any(Number)
        })
      );
    });

    jest.useRealTimers();
  });

  it('should calculate scroll offset accurately', () => {
    const scrollTop = 100;
    const scrollHeight = 1000;
    const clientHeight = 800;
    const maxScroll = scrollHeight - clientHeight; // 200

    const scrollOffsetPercent = (scrollTop / maxScroll) * 100;

    expect(scrollOffsetPercent).toBe(50); // 100/200 * 100 = 50%
  });

  it('should reload descriptions on chapter change', async () => {
    render(<EpubReader book={mockBook} />);

    // Initial load for chapter 1
    await waitFor(() => {
      expect(booksAPI.getChapterDescriptions).toHaveBeenCalledWith(
        mockBook.id,
        1,
        false
      );
    });

    // Simulate chapter change (relocated to chapter 2)
    mockRendition._triggerEvent('relocated', {
      start: { cfi: 'epubcfi(/6/6!/4/2/1:0)', href: 'chapter2.xhtml' }
    });

    // Should reload descriptions for chapter 2
    await waitFor(() => {
      expect(booksAPI.getChapterDescriptions).toHaveBeenCalledWith(
        mockBook.id,
        2,
        false
      );
    });
  });
});
```

### Testing CFI System

```typescript
describe('CFI tracking', () => {
  it('should generate valid CFI on location change', () => {
    const book = ePub(new ArrayBuffer(1024));
    const cfi = book.locations.cfiFromPercentage(0.5);

    expect(cfi).toMatch(/^epubcfi\(/);
    expect(typeof cfi).toBe('string');
  });

  it('should convert CFI to percentage correctly', () => {
    const book = ePub(new ArrayBuffer(1024));
    const cfi = 'epubcfi(/6/4!/4/2/16/1:0)';

    const percentage = book.locations.percentageFromCfi(cfi);

    expect(percentage).toBeGreaterThanOrEqual(0);
    expect(percentage).toBeLessThanOrEqual(1);
  });

  it('should handle hybrid restoration (CFI + scroll)', async () => {
    const mockProgress = {
      reading_location_cfi: 'epubcfi(/6/4!/4/2/16/1:0)',
      scroll_offset_percent: 45.5
    };

    render(<EpubReader book={mockBook} />);

    await waitFor(() => {
      // Level 1: CFI restoration
      expect(mockRendition.display).toHaveBeenCalledWith(
        mockProgress.reading_location_cfi
      );

      // Level 2: Scroll offset restoration
      const contents = mockRendition.getContents();
      const scrollElement = contents[0].document.documentElement;
      const maxScroll = scrollElement.scrollHeight - scrollElement.clientHeight;
      const expectedScroll = (mockProgress.scroll_offset_percent / 100) * maxScroll;

      expect(scrollElement.scrollTop).toBeCloseTo(expectedScroll, 0);
    });
  });
});
```

### E2E Testing with epub.js

```typescript
test('epub.js reader with CFI restoration', async ({ page }) => {
  await page.goto('/library');

  // Open book
  await page.click('[data-testid="book-card"]:first-child');
  await page.click('[data-testid="read-button"]');

  // Wait for epub.js to load
  await page.waitForSelector('[data-testid="epub-viewer"]');

  // Read to 50%
  for (let i = 0; i < 10; i++) {
    await page.click('[data-testid="next-page"]');
    await page.waitForTimeout(300);
  }

  // Check progress indicator
  const progressBefore = await page.textContent('[data-testid="progress-percent"]');
  expect(progressBefore).toMatch(/\d+%/);

  // Close and reopen
  await page.click('[data-testid="close-reader"]');
  await page.goto('/library');
  await page.click('[data-testid="book-card"]:first-child');
  await page.click('[data-testid="read-button"]');

  // Wait for restoration
  await page.waitForSelector('[data-testid="epub-viewer"]');
  await page.waitForTimeout(1000); // Wait for CFI restoration

  // Check progress restored
  const progressAfter = await page.textContent('[data-testid="progress-percent"]');
  expect(progressAfter).toBe(progressBefore);
});
```

---

## E2E Testing

### Playwright Setup
**–§–∞–π–ª:** `tests/e2e/playwright.config.ts`

```typescript
export default defineConfig({
  testDir: './tests/e2e',
  fullyParallel: true,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 0,
  workers: process.env.CI ? 1 : undefined,
  reporter: 'html',
  use: {
    baseURL: 'http://localhost:3000',
    trace: 'on-first-retry',
    screenshot: 'only-on-failure'
  },
  projects: [
    { name: 'chromium', use: { ...devices['Desktop Chrome'] } },
    { name: 'firefox', use: { ...devices['Desktop Firefox'] } },
    { name: 'webkit', use: { ...devices['Desktop Safari'] } },
    { name: 'Mobile Chrome', use: { ...devices['Pixel 5'] } }
  ]
});
```

### E2E Tests
```typescript
test.describe('Book Reading Flow', () => {
  test.beforeEach(async ({ page }) => {
    // –õ–æ–≥–∏–Ω
    await page.goto('/login');
    await page.fill('[data-testid="email"]', 'test@example.com');
    await page.fill('[data-testid="password"]', 'password');
    await page.click('[data-testid="login-button"]');
    
    await page.waitForURL('/library');
  });
  
  test('upload and read book', async ({ page }) => {
    // –ó–∞–≥—Ä—É–∑–∫–∞ –∫–Ω–∏–≥–∏
    await page.click('[data-testid="upload-book-button"]');
    
    const fileInput = page.locator('input[type="file"]');
    await fileInput.setInputFiles('tests/fixtures/sample-book.epub');
    
    await page.click('[data-testid="upload-confirm"]');
    
    // –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
    await expect(page.locator('[data-testid="processing-indicator"]')).toBeVisible();
    await expect(page.locator('[data-testid="processing-indicator"]')).toBeHidden({ timeout: 30000 });
    
    // –û—Ç–∫—Ä—ã—Ç–∏–µ –∫–Ω–∏–≥–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è
    await page.click('[data-testid="book-card"]:first-child [data-testid="read-button"]');
    
    await page.waitForURL('/reader/*');
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∏—Ç–∞–ª–∫–∏
    await expect(page.locator('[data-testid="chapter-title"]')).toBeVisible();
    await expect(page.locator('[data-testid="chapter-content"]')).toBeVisible();
    
    // –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –≥–ª–∞–≤–∞–º
    await page.click('[data-testid="next-chapter"]');
    
    await expect(page.locator('[data-testid="chapter-title"]')).toHaveText(/–≥–ª–∞–≤–∞ 2/i);
  });
  
  test('image generation workflow', async ({ page }) => {
    // –ü–µ—Ä–µ—Ö–æ–¥ –≤ –≥–∞–ª–µ—Ä–µ—é
    await page.goto('/library');
    await page.click('[data-testid="book-card"]:first-child [data-testid="images-button"]');
    
    // –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    await page.click('[data-testid="generate-images-button"]');
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    await expect(page.locator('[data-testid="generation-progress"]')).toBeVisible();
    
    // –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)
    await expect(
      page.locator('[data-testid="generated-image"]').first()
    ).toBeVisible({ timeout: 60000 });
  });
});
```

---

## Testing Scripts

### –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
**–§–∞–π–ª:** `scripts/run-tests.sh`

```bash
#!/bin/bash

set -e

echo "üß™ Running BookReader AI Test Suite"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
print_status() {
    echo -e "\033[1;34m$1\033[0m"
}

print_success() {
    echo -e "\033[1;32m‚úÖ $1\033[0m"
}

print_error() {
    echo -e "\033[1;31m‚ùå $1\033[0m"
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
if [[ ! -f ".env.test" ]]; then
    print_error "Test environment file .env.test not found!"
    exit 1
fi

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
export $(cat .env.test | xargs)

# Backend —Ç–µ—Å—Ç—ã
print_status "Running Backend Tests..."
cd backend

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –ë–î
print_status "Setting up test database..."
python scripts/setup_test_db.py

# Unit —Ç–µ—Å—Ç—ã
print_status "Running unit tests..."
pytest tests/unit/ -v --cov=app --cov-report=html --cov-report=term

# Integration —Ç–µ—Å—Ç—ã
print_status "Running integration tests..."
pytest tests/integration/ -v

# –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –ë–î
python scripts/cleanup_test_db.py

cd ..

# Frontend —Ç–µ—Å—Ç—ã
print_status "Running Frontend Tests..."
cd frontend

# Unit —Ç–µ—Å—Ç—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
print_status "Running component tests..."
npm test -- --coverage --watchAll=false

# TypeScript –ø—Ä–æ–≤–µ—Ä–∫–∏
print_status "Running TypeScript checks..."
npm run type-check

cd ..

# E2E —Ç–µ—Å—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
if [[ "$RUN_E2E" == "true" ]]; then
    print_status "Running E2E Tests..."
    
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
    docker-compose -f docker-compose.test.yml up -d
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
    ./scripts/wait-for-services.sh
    
    # –ó–∞–ø—É—Å–∫ E2E —Ç–µ—Å—Ç–æ–≤
    cd frontend
    npx playwright test
    cd ..
    
    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
    docker-compose -f docker-compose.test.yml down
fi

print_success "All tests completed successfully! üéâ"
```

### Continuous Integration
**–§–∞–π–ª:** `.github/workflows/test.yml`

```yaml
name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  backend-tests:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_USER: test
          POSTGRES_DB: bookreader_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      working-directory: ./backend
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
        python -m spacy download ru_core_news_lg
    
    - name: Run tests
      working-directory: ./backend
      env:
        DATABASE_URL: postgresql+asyncpg://test:test@localhost/bookreader_test
        REDIS_URL: redis://localhost:6379
      run: |
        pytest tests/ -v --cov=app --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml

  frontend-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js 18
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install dependencies
      working-directory: ./frontend
      run: npm ci
    
    - name: Run tests
      working-directory: ./frontend
      run: |
        npm test -- --coverage --watchAll=false
        npm run type-check
        npm run lint
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/coverage-final.json

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [backend-tests, frontend-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install Playwright
      working-directory: ./frontend
      run: |
        npm ci
        npx playwright install --with-deps
    
    - name: Start application
      run: |
        docker-compose -f docker-compose.test.yml up -d
        ./scripts/wait-for-services.sh
    
    - name: Run E2E tests
      working-directory: ./frontend
      run: npx playwright test
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: failure()
      with:
        name: playwright-report
        path: frontend/playwright-report/
```

---

## Test Data Management

### –§–∞–±—Ä–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
```python
class UserFactory:
    @staticmethod
    def create(**kwargs) -> User:
        defaults = {
            'email': fake.email(),
            'password_hash': 'hashed_password',
            'full_name': fake.name(),
            'is_active': True
        }
        defaults.update(kwargs)
        return User(**defaults)

class BookFactory:
    @staticmethod
    def create(**kwargs) -> Book:
        defaults = {
            'title': fake.sentence(nb_words=3),
            'author': fake.name(),
            'genre': random.choice(list(BookGenre)),
            'file_path': f'/test/{fake.uuid4()}.epub',
            'file_format': BookFormat.EPUB,
            'file_size': random.randint(100000, 10000000)
        }
        defaults.update(kwargs)
        return Book(**defaults)
```

### Fixtures –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
```python
def create_test_epub_file() -> BytesIO:
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ EPUB —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
    
    zip_buffer = BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        # mimetype
        zip_file.writestr('mimetype', 'application/epub+zip')
        
        # META-INF/container.xml
        zip_file.writestr('META-INF/container.xml', CONTAINER_XML)
        
        # content.opf
        zip_file.writestr('OEBPS/content.opf', CONTENT_OPF)
        
        # Test chapter
        zip_file.writestr('OEBPS/chapter1.xhtml', CHAPTER_CONTENT)
    
    zip_buffer.seek(0)
    return zip_buffer
```

---

## Performance Testing

### Load Testing —Å Locust
```python
from locust import HttpUser, task, between

class BookReaderUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """–õ–æ–≥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        response = self.client.post('/api/v1/auth/login', json={
            'email': 'loadtest@example.com',
            'password': 'password'
        })
        
        if response.status_code == 200:
            self.token = response.json()['access_token']
            self.client.headers.update({
                'Authorization': f'Bearer {self.token}'
            })
    
    @task(3)
    def browse_books(self):
        """–ü—Ä–æ—Å–º–æ—Ç—Ä –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∫–Ω–∏–≥."""
        self.client.get('/api/v1/books')
    
    @task(2)
    def read_chapter(self):
        """–ß—Ç–µ–Ω–∏–µ –≥–ª–∞–≤—ã –∫–Ω–∏–≥–∏."""
        book_id = 'sample-book-id'
        chapter = random.randint(1, 10)
        self.client.get(f'/api/v1/books/{book_id}/chapters/{chapter}')
    
    @task(1)
    def generate_image(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        description_id = 'sample-description-id'
        self.client.post(f'/api/v1/images/generate/description/{description_id}')
```

---

## Multi-NLP System Testing (October 2025)

### Overview

Multi-NLP —Å–∏—Å—Ç–µ–º–∞ - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å BookReader AI. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –ø–æ–∫—Ä—ã–≤–∞—Ç—å –≤—Å–µ 3 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, 5 —Ä–µ–∂–∏–º–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∏ ensemble voting –ª–æ–≥–∏–∫—É.

### Unit Tests –¥–ª—è Multi-NLP Manager

**–§–∞–π–ª:** `backend/tests/unit/test_multi_nlp_manager.py`

```python
import pytest
import asyncio
from app.services.multi_nlp_manager import (
    MultiNLPManager,
    ProcessingMode,
    ProcessorType
)

class TestMultiNLPManager:
    """Comprehensive tests –¥–ª—è Multi-NLP —Å–∏—Å—Ç–µ–º—ã."""

    @pytest.fixture
    async def nlp_manager(self):
        """Fixture –¥–ª—è NLP manager."""
        manager = MultiNLPManager()
        await manager.initialize()
        yield manager
        await manager.cleanup()

    @pytest.mark.asyncio
    async def test_all_processors_initialized(self, nlp_manager):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""
        status = await nlp_manager.get_processor_status()

        assert status['spacy']['initialized'] is True
        assert status['natasha']['initialized'] is True
        assert status['stanza']['initialized'] is True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–π
        assert 'version' in status['spacy']
        assert 'model' in status['spacy']

    @pytest.mark.asyncio
    async def test_ensemble_voting_consensus(self, nlp_manager):
        """–¢–µ—Å—Ç ensemble voting —Å consensus filtering."""
        text = "–í –¥—Ä–µ–≤–Ω–µ–º –∫–∞–º–µ–Ω–Ω–æ–º –∑–∞–º–∫–µ –Ω–∞ —Å–∫–∞–ª–µ –∂–∏–ª –º–æ–≥—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–æ–ª—à–µ–±–Ω–∏–∫."

        result = await nlp_manager.process_text(
            text,
            mode=ProcessingMode.ENSEMBLE
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        assert result.success is True
        assert len(result.descriptions) > 0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ consensus –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
        for desc in result.descriptions:
            assert desc.consensus_strength >= 0.6  # Minimum threshold
            assert len(desc.sources) >= 2  # –ú–∏–Ω–∏–º—É–º 2 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å–æ–≥–ª–∞—Å–Ω—ã
            assert desc.priority_score > 0

            # –ï—Å–ª–∏ –≤—Å–µ 3 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å–æ–≥–ª–∞—Å–Ω—ã
            if len(desc.sources) == 3:
                # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å consensus bonus
                assert desc.consensus_bonus >= 10

    @pytest.mark.asyncio
    async def test_adaptive_mode_selection(self, nlp_manager):
        """–¢–µ—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –≤ ADAPTIVE mode."""

        # 1. –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ‚Üí SINGLE mode
        short_text = "–ö–æ–º–Ω–∞—Ç–∞ –±—ã–ª–∞ —Ç–µ–º–Ω–∞—è."
        result_short = await nlp_manager.process_text(
            short_text,
            mode=ProcessingMode.ADAPTIVE
        )

        assert result_short.mode_used == ProcessingMode.SINGLE
        assert len(result_short.processors_used) == 1

        # 2. –¢–µ–∫—Å—Ç —Å —Ä—É—Å—Å–∫–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ ‚Üí NATASHA –≤–∫–ª—é—á–µ–Ω–∞
        names_text = "–ê–Ω–Ω–∞ –ö–∞—Ä–µ–Ω–∏–Ω–∞ –≤–æ—à–ª–∞ –≤ –±–∞–ª—å–Ω—ã–π –∑–∞–ª, –≥–¥–µ –µ–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª –≥—Ä–∞—Ñ –í—Ä–æ–Ω—Å–∫–∏–π."
        result_names = await nlp_manager.process_text(
            names_text,
            mode=ProcessingMode.ADAPTIVE
        )

        assert ProcessorType.NATASHA in result_names.processors_used
        assert len(result_names.processors_used) >= 2

        # 3. –î–ª–∏–Ω–Ω—ã–π —Å–ª–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí ENSEMBLE mode
        complex_text = """
        –í —Å—Ç–∞—Ä–∏–Ω–Ω–æ–º –≥–æ—Ç–∏—á–µ—Å–∫–æ–º –∑–∞–º–∫–µ –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –∫—Ä—É—Ç–æ–π —Å–∫–∞–ª—ã, –æ–∫—Ä—É–∂–µ–Ω–Ω–æ–º
        –≥—É—Å—Ç—ã–º —Ç—É–º–∞–Ω–Ω—ã–º –ª–µ—Å–æ–º, –∂–∏–ª –º–æ–≥—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–æ–ª—à–µ–±–Ω–∏–∫ —Å –¥–ª–∏–Ω–Ω–æ–π —Å–µ–¥–æ–π
        –±–æ—Ä–æ–¥–æ–π. –ï–≥–æ —Ç–µ–º–Ω–∞—è –º–∞–Ω—Ç–∏—è –±—ã–ª–∞ —É–∫—Ä–∞—à–µ–Ω–∞ —Å–µ—Ä–µ–±—Ä—è–Ω—ã–º–∏ –∑–≤–µ–∑–¥–∞–º–∏, –∞ –≤
        —Ä—É–∫–∞—Ö –æ–Ω –¥–µ—Ä–∂–∞–ª –¥—Ä–µ–≤–Ω–∏–π –ø–æ—Å–æ—Ö –∏–∑ —á–µ—Ä–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞.
        """
        result_complex = await nlp_manager.process_text(
            complex_text,
            mode=ProcessingMode.ADAPTIVE
        )

        assert result_complex.mode_used == ProcessingMode.ENSEMBLE
        assert len(result_complex.processors_used) == 3

    @pytest.mark.asyncio
    async def test_parallel_mode_performance(self, nlp_manager):
        """–¢–µ—Å—Ç PARALLEL mode - –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ."""
        text = "–í—ã—Å–æ–∫–∏–π –º—É–∂—á–∏–Ω–∞ —Å —Å–µ–¥–æ–π –±–æ—Ä–æ–¥–æ–π —Å—Ç–æ—è–ª —É –æ–∫–Ω–∞ –∑–∞–º–∫–∞."

        import time
        start_time = time.time()

        result = await nlp_manager.process_text(
            text,
            mode=ProcessingMode.PARALLEL
        )

        processing_time = time.time() - start_time

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        assert result.success is True
        assert len(result.processors_used) == 3

        # PARALLEL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ —á–µ–º SEQUENTIAL
        # (–ü—Ä–∏–º–µ—Ä–Ω–æ –≤ 2-3 —Ä–∞–∑–∞ –¥–ª—è 3 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤)
        assert processing_time < 2.0  # –ù–µ –±–æ–ª–µ–µ 2 —Å–µ–∫—É–Ω–¥

    @pytest.mark.asyncio
    async def test_processor_weights_affect_consensus(self, nlp_manager):
        """–¢–µ—Å—Ç –≤–ª–∏—è–Ω–∏—è –≤–µ—Å–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –Ω–∞ consensus."""

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –≤–µ—Å–æ–≤
        await nlp_manager.update_processor_settings(
            ProcessorType.SPACY,
            weight=1.0,
            enabled=True
        )
        await nlp_manager.update_processor_settings(
            ProcessorType.NATASHA,
            weight=1.2,  # –ë–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –∏–º–µ–Ω
            enabled=True
        )
        await nlp_manager.update_processor_settings(
            ProcessorType.STANZA,
            weight=0.8,
            enabled=True
        )

        text = "–ê–Ω–Ω–∞ –ü–∞–≤–ª–æ–≤–Ω–∞ –®–µ—Ä–µ—Ä –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞ –∫–Ω—è–∑—è –í–∞—Å–∏–ª–∏—è –ö—É—Ä–∞–≥–∏–Ω–∞."

        result = await nlp_manager.process_text(
            text,
            mode=ProcessingMode.ENSEMBLE
        )

        # Natasha –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –±–æ–ª—å—à–∏–π –≤–∫–ª–∞–¥ –≤ consensus
        for desc in result.descriptions:
            if ProcessorType.NATASHA in desc.sources:
                # –û–ø–∏—Å–∞–Ω–∏—è –æ—Ç Natasha –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π priority
                natasha_priority = desc.processor_contributions.get(
                    ProcessorType.NATASHA, 0
                )
                assert natasha_priority > 0

    @pytest.mark.asyncio
    async def test_description_deduplication(self, nlp_manager):
        """–¢–µ—Å—Ç –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π."""
        text = "–°—Ç–∞—Ä—ã–π –∑–∞–º–æ–∫. –î—Ä–µ–≤–Ω–∏–π –∑–∞–º–æ–∫. –ó–∞–º–æ–∫ –±—ã–ª —Å—Ç–∞—Ä—ã–º."

        result = await nlp_manager.process_text(
            text,
            mode=ProcessingMode.ENSEMBLE
        )

        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–¥–Ω–æ –æ–±–æ–±—â–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ "—Å—Ç–∞—Ä—ã–π/–¥—Ä–µ–≤–Ω–∏–π –∑–∞–º–æ–∫"
        # –∞ –Ω–µ 3 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö
        location_descs = [
            d for d in result.descriptions
            if d.type == 'location'
        ]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        assert len(location_descs) <= 2  # –ú–∞–∫—Å–∏–º—É–º 2 (—Å —É—á–µ—Ç–æ–º —Å–∏–Ω–æ–Ω–∏–º–æ–≤)

    @pytest.mark.asyncio
    async def test_context_enrichment(self, nlp_manager):
        """–¢–µ—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π."""
        text = """
        –í –¥—Ä–µ–≤–Ω–µ–º –∑–∞–º–∫–µ –Ω–∞ —Å–∫–∞–ª–µ –±—ã–ª–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç.
        –û–¥–Ω–∞ –∏–∑ –Ω–∏—Ö –±—ã–ª–∞ –æ—Å–æ–±–µ–Ω–Ω–æ —Ç–µ–º–Ω–æ–π –∏ –º—Ä–∞—á–Ω–æ–π.
        """

        result = await nlp_manager.process_text(
            text,
            mode=ProcessingMode.ENSEMBLE
        )

        # –û–ø–∏—Å–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        for desc in result.descriptions:
            assert desc.context is not None
            assert 'surrounding_text' in desc.context
            assert 'entities' in desc.context

            # –î–ª—è –æ–ø–∏—Å–∞–Ω–∏—è "—Ç–µ–º–Ω–∞—è –∫–æ–º–Ω–∞—Ç–∞" –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å "–∑–∞–º–æ–∫"
            if '–∫–æ–º–Ω–∞—Ç–∞' in desc.content.lower():
                assert any(
                    '–∑–∞–º–æ–∫' in entity.lower()
                    for entity in desc.context.get('entities', [])
                )

    @pytest.mark.parametrize("processor_type,text,expected_strength", [
        (ProcessorType.SPACY, "–í—ã—Å–æ–∫–∏–π –º—É–∂—á–∏–Ω–∞ —Å –±–æ—Ä–æ–¥–æ–π", 0.7),
        (ProcessorType.NATASHA, "–ê–Ω–Ω–∞ –ö–∞—Ä–µ–Ω–∏–Ω–∞ –∏ –í—Ä–æ–Ω—Å–∫–∏–π", 0.8),
        (ProcessorType.STANZA, "–°–ª–æ–∂–Ω–∞—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π", 0.6)
    ])
    @pytest.mark.asyncio
    async def test_individual_processor_strengths(
        self,
        nlp_manager,
        processor_type,
        text,
        expected_strength
    ):
        """–¢–µ—Å—Ç —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        result = await nlp_manager.process_single_processor(
            text,
            processor_type
        )

        assert result.success is True
        assert len(result.descriptions) > 0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ confidence –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        avg_confidence = sum(d.confidence for d in result.descriptions) / len(result.descriptions)
        assert avg_confidence >= expected_strength


class TestMultiNLPIntegration:
    """Integration tests —Å –∫–Ω–∏–≥–∞–º–∏ –∏ –≥–ª–∞–≤–∞–º–∏."""

    @pytest.mark.asyncio
    async def test_process_full_chapter(self, nlp_manager, sample_chapter):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª–Ω–æ–π –≥–ª–∞–≤—ã."""
        chapter_text = sample_chapter.content  # ~5000 —Å–∏–º–≤–æ–ª–æ–≤

        result = await nlp_manager.process_text(
            chapter_text,
            mode=ProcessingMode.ENSEMBLE
        )

        # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è –≥–ª–∞–≤—ã
        assert len(result.descriptions) >= 20
        assert len(result.descriptions) <= 100

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–æ–≤
        types_count = {}
        for desc in result.descriptions:
            types_count[desc.type] = types_count.get(desc.type, 0) + 1

        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã
        assert 'location' in types_count
        assert 'character' in types_count

    @pytest.mark.asyncio
    async def test_quality_score_distribution(self, nlp_manager):
        """–¢–µ—Å—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è quality scores."""
        text = """
        –í —Å—Ç–∞—Ä–æ–º –∑–∞–º–∫–µ –∂–∏–ª –≤–æ–ª—à–µ–±–Ω–∏–∫. –ó–∞–º–æ–∫ –±—ã–ª –Ω–∞ –≥–æ—Ä–µ.
        –í–æ–ª—à–µ–±–Ω–∏–∫ –±—ã–ª –≤—ã—Å–æ–∫–∏–º. –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –±—ã–ª–∞ –º—Ä–∞—á–Ω–æ–π.
        """

        result = await nlp_manager.process_text(
            text,
            mode=ProcessingMode.ENSEMBLE
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ priority scores
        high_priority = [d for d in result.descriptions if d.priority_score > 80]
        medium_priority = [d for d in result.descriptions if 60 <= d.priority_score <= 80]
        low_priority = [d for d in result.descriptions if d.priority_score < 60]

        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–∑—É–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        assert len(high_priority) > 0  # –ï—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
        assert len(low_priority) < len(result.descriptions) / 2  # –ù–µ –±–æ–ª–µ–µ 50% –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
```

---

## CFI System Testing (October 2025)

### Overview

CFI (Canonical Fragment Identifier) —Å–∏—Å—Ç–µ–º–∞ –∫—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ø–æ–∑–∏—Ü–∏–∏ —á—Ç–µ–Ω–∏—è –≤ EPUB —Ñ–∞–π–ª–∞—Ö.

### Backend CFI Tests

**–§–∞–π–ª:** `backend/tests/unit/test_cfi_system.py`

```python
import pytest
from app.services.book_parser import BookParser
from app.models.book import ReadingProgress

class TestCFIGeneration:
    """–¢–µ—Å—Ç—ã –¥–ª—è CFI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""

    @pytest.fixture
    def book_parser(self):
        return BookParser()

    def test_cfi_format_validation(self, book_parser):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ CFI."""
        valid_cfis = [
            "epubcfi(/6/4!/4/2/16/1:0)",
            "epubcfi(/6/2!/4/2)",
            "epubcfi(/6/14!/4/2/10/3:25)"
        ]

        for cfi in valid_cfis:
            assert book_parser.validate_cfi(cfi) is True

        invalid_cfis = [
            "invalid",
            "epubcfi()",
            "/6/4!/4/2/16/1:0",  # Missing epubcfi prefix
            "epubcfi(/6/4"  # Incomplete
        ]

        for cfi in invalid_cfis:
            assert book_parser.validate_cfi(cfi) is False

    def test_cfi_extraction_from_epub(self, book_parser, sample_epub_path):
        """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è CFI locations –∏–∑ EPUB."""
        cfi_locations = book_parser.extract_chapter_cfis(sample_epub_path)

        assert len(cfi_locations) > 0

        for chapter_num, cfi in cfi_locations.items():
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
            assert cfi.startswith("epubcfi(")
            assert cfi.endswith(")")
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
            assert book_parser.validate_cfi(cfi) is True

    @pytest.mark.asyncio
    async def test_reading_progress_with_cfi(
        self,
        db_session,
        test_user,
        test_book
    ):
        """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ —á–µ—Ä–µ–∑ CFI."""

        # –°–æ–∑–¥–∞–Ω–∏–µ reading progress —Å CFI
        progress = ReadingProgress(
            user_id=test_user.id,
            book_id=test_book.id,
            current_chapter=3,
            reading_location_cfi="epubcfi(/6/8!/4/2/16/1:453)",
            scroll_offset_percent=23.5
        )

        db_session.add(progress)
        await db_session.commit()
        await db_session.refresh(progress)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        assert progress.reading_location_cfi == "epubcfi(/6/8!/4/2/16/1:453)"
        assert progress.scroll_offset_percent == 23.5

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–æ–¥–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        progress_percent = test_book.get_reading_progress_percent()

        # CFI —Å–∏—Å—Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç
        assert 0.0 <= progress_percent <= 100.0
        assert progress_percent > 0  # –ù–µ –≤ –Ω–∞—á–∞–ª–µ –∫–Ω–∏–≥–∏

    def test_cfi_comparison(self, book_parser):
        """–¢–µ—Å—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è CFI –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞."""

        cfi1 = "epubcfi(/6/4!/4/2/16/1:0)"
        cfi2 = "epubcfi(/6/8!/4/2/16/1:0)"
        cfi3 = "epubcfi(/6/4!/4/2/20/1:0)"

        # cfi2 –ø–æ–∑–∂–µ —á–µ–º cfi1 (–±–æ–ª—å—à–∏–π –Ω–æ–º–µ—Ä spine)
        assert book_parser.compare_cfis(cfi1, cfi2) == -1
        assert book_parser.compare_cfis(cfi2, cfi1) == 1

        # cfi3 –ø–æ–∑–∂–µ —á–µ–º cfi1 –≤ —Ç–æ–º –∂–µ spine
        assert book_parser.compare_cfis(cfi1, cfi3) == -1

        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ CFI
        assert book_parser.compare_cfis(cfi1, cfi1) == 0


class TestHybridProgressTracking:
    """–¢–µ—Å—Ç—ã –¥–ª—è hybrid —Å–∏—Å—Ç–µ–º—ã (CFI + scroll offset)."""

    @pytest.mark.asyncio
    async def test_hybrid_system_accuracy(
        self,
        db_session,
        test_book,
        test_user
    ):
        """–¢–µ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ hybrid —Å–∏—Å—Ç–µ–º—ã."""

        # –°—Ü–µ–Ω–∞—Ä–∏–π: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —á–∏—Ç–∞–µ—Ç –≥–ª–∞–≤—É 5, –ø—Ä–æ–∫—Ä—É—Ç–∏–ª 45% –≤–Ω–∏–∑
        progress = ReadingProgress(
            user_id=test_user.id,
            book_id=test_book.id,
            current_chapter=5,
            reading_location_cfi="epubcfi(/6/12!/4/2)",
            scroll_offset_percent=45.2,
            total_chapters=25
        )

        db_session.add(progress)
        await db_session.commit()

        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ hybrid —Å–∏—Å—Ç–µ–º—É
        overall_progress = test_book.get_reading_progress_percent()

        # –ì–ª–∞–≤–∞ 5 –∏–∑ 25 = 20% + 45% –æ—Ç —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤—ã
        # –ü—Ä–∏–º–µ—Ä–Ω–æ 20% + (4% * 0.45) ‚âà 21.8%
        assert 20.0 <= overall_progress <= 23.0

    @pytest.mark.asyncio
    async def test_scroll_offset_validation(self, db_session, test_user, test_book):
        """–¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ scroll offset."""

        # Valid scroll offsets
        valid_offsets = [0.0, 25.5, 50.0, 75.3, 99.9, 100.0]

        for offset in valid_offsets:
            progress = ReadingProgress(
                user_id=test_user.id,
                book_id=test_book.id,
                current_chapter=1,
                scroll_offset_percent=offset
            )

            db_session.add(progress)
            await db_session.commit()

            assert progress.scroll_offset_percent == offset

        # Invalid scroll offsets (–¥–æ–ª–∂–Ω—ã –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É)
        invalid_offsets = [-1.0, 101.0, 150.0]

        for offset in invalid_offsets:
            with pytest.raises(ValueError):
                progress = ReadingProgress(
                    user_id=test_user.id,
                    book_id=test_book.id,
                    current_chapter=1,
                    scroll_offset_percent=offset
                )
                db_session.add(progress)
                await db_session.commit()
```

### Frontend CFI Tests

**–§–∞–π–ª:** `frontend/src/__tests__/cfi-system.test.ts`

```typescript
import { describe, it, expect, beforeEach, vi } from 'vitest';
import { renderHook, waitFor } from '@testing-library/react';
import { useReadingProgress } from '@/hooks/useReadingProgress';
import type { Rendition } from 'epubjs';

describe('CFI System - Frontend', () => {
  let mockRendition: Partial<Rendition>;

  beforeEach(() => {
    // Mock epub.js rendition
    mockRendition = {
      location: {
        start: {
          cfi: 'epubcfi(/6/4!/4/2/16/1:0)',
          displayed: { page: 1, total: 10 },
          percentage: 0.45
        },
        end: {
          cfi: 'epubcfi(/6/4!/4/2/20/1:0)',
          percentage: 0.48
        }
      },
      display: vi.fn(),
      getRange: vi.fn()
    };
  });

  it('generates valid CFI on location change', () => {
    const location = mockRendition.location!;

    const cfi = extractCFI(location);

    expect(cfi).toBe('epubcfi(/6/4!/4/2/16/1:0)');
    expect(validateCFI(cfi)).toBe(true);
  });

  it('calculates scroll offset accurately', () => {
    const iframe = document.createElement('iframe');
    const mockDocument = {
      scrollTop: 450,
      scrollHeight: 2000,
      clientHeight: 800
    };

    // Mock iframe content
    vi.spyOn(iframe, 'contentDocument', 'get').mockReturnValue({
      documentElement: mockDocument
    } as any);

    const offset = calculateScrollOffset(iframe);

    // scrollTop / (scrollHeight - clientHeight)
    // 450 / (2000 - 800) = 450 / 1200 = 0.375 = 37.5%
    expect(offset).toBeCloseTo(37.5, 1);
  });

  it('restores position with hybrid system', async () => {
    const savedProgress = {
      reading_location_cfi: 'epubcfi(/6/8!/4/2/16/1:453)',
      scroll_offset_percent: 23.5,
      current_chapter: 5
    };

    const rendition = mockRendition as Rendition;

    // Restore position
    await restoreReadingPosition(rendition, savedProgress);

    // Verify CFI navigation was called
    expect(rendition.display).toHaveBeenCalledWith(
      savedProgress.reading_location_cfi
    );

    // Verify scroll offset will be applied after render
    await waitFor(() => {
      const iframe = document.querySelector('iframe');
      expect(iframe).toBeTruthy();

      // Check scroll was applied (within 1%)
      const currentOffset = calculateScrollOffset(iframe!);
      expect(Math.abs(currentOffset - 23.5)).toBeLessThan(1);
    });
  });

  it('handles CFI navigation errors gracefully', async () => {
    const invalidCFI = 'invalid-cfi';
    const rendition = mockRendition as Rendition;

    // Mock display throwing error
    vi.mocked(rendition.display).mockRejectedValue(new Error('Invalid CFI'));

    // Should fallback to chapter-based navigation
    await expect(
      restoreReadingPosition(rendition, {
        reading_location_cfi: invalidCFI,
        current_chapter: 5
      })
    ).resolves.not.toThrow();

    // Verify fallback was used
    expect(rendition.display).toHaveBeenCalled();
  });
});

describe('Reading Progress Auto-save', () => {
  it('debounces progress updates', async () => {
    vi.useFakeTimers();

    const { result } = renderHook(() => useReadingProgress('book-123'));

    // Simulate multiple rapid location changes
    act(() => {
      result.current.updateProgress({
        cfi: 'epubcfi(/6/4!/4/2/16/1:0)',
        scrollOffset: 10.5
      });
    });

    act(() => {
      result.current.updateProgress({
        cfi: 'epubcfi(/6/4!/4/2/18/1:0)',
        scrollOffset: 15.3
      });
    });

    act(() => {
      result.current.updateProgress({
        cfi: 'epubcfi(/6/4!/4/2/20/1:0)',
        scrollOffset: 20.1
      });
    });

    // Fast forward debounce timer (2 seconds)
    act(() => {
      vi.advanceTimersByTime(2000);
    });

    // Verify only one API call was made (with latest values)
    await waitFor(() => {
      expect(mockAPI.updateReadingProgress).toHaveBeenCalledTimes(1);
      expect(mockAPI.updateReadingProgress).toHaveBeenCalledWith({
        bookId: 'book-123',
        reading_location_cfi: 'epubcfi(/6/4!/4/2/20/1:0)',
        scroll_offset_percent: 20.1
      });
    });

    vi.useRealTimers();
  });
});

// Helper functions
function extractCFI(location: any): string {
  return location.start.cfi;
}

function validateCFI(cfi: string): boolean {
  return /^epubcfi\(\/\d+/.test(cfi);
}

function calculateScrollOffset(iframe: HTMLIFrameElement): number {
  const doc = iframe.contentDocument?.documentElement;
  if (!doc) return 0;

  const scrollableHeight = doc.scrollHeight - doc.clientHeight;
  if (scrollableHeight <= 0) return 0;

  return (doc.scrollTop / scrollableHeight) * 100;
}

async function restoreReadingPosition(
  rendition: Rendition,
  progress: any
): Promise<void> {
  try {
    // Navigate to CFI
    await rendition.display(progress.reading_location_cfi);

    // Apply scroll offset after render
    setTimeout(() => {
      const iframe = document.querySelector('iframe');
      if (iframe && progress.scroll_offset_percent) {
        const doc = iframe.contentDocument?.documentElement;
        if (doc) {
          const scrollableHeight = doc.scrollHeight - doc.clientHeight;
          const targetScroll = (progress.scroll_offset_percent / 100) * scrollableHeight;
          doc.scrollTop = targetScroll;
        }
      }
    }, 100);
  } catch (error) {
    console.error('CFI navigation failed, using fallback', error);
    // Fallback to chapter navigation
    // ... implementation
  }
}
```

---

## E2E Testing with Playwright (October 2025)

### Complete Reading Flow with CFI

**–§–∞–π–ª:** `frontend/tests/e2e/reading-flow.spec.ts`

```typescript
import { test, expect } from '@playwright/test';

test.describe('Complete Reading Flow with CFI Tracking', () => {
  test.beforeEach(async ({ page }) => {
    // Login
    await page.goto('/login');
    await page.fill('[name=email]', 'test@example.com');
    await page.fill('[name=password]', 'password123');
    await page.click('button[type=submit]');

    await page.waitForURL('/library');
  });

  test('reads book and restores exact position after reload', async ({ page }) => {
    // 1. Open book from library
    await page.click('[data-testid="book-card"]:first-child');
    await page.waitForSelector('.epub-reader');

    // 2. Wait for book to load
    await expect(page.locator('.epub-view')).toBeVisible();

    // 3. Read several pages
    await page.click('[data-testid="next-page"]');
    await page.waitForTimeout(500);

    await page.click('[data-testid="next-page"]');
    await page.waitForTimeout(500);

    await page.click('[data-testid="next-page"]');
    await page.waitForTimeout(500);

    // 4. Scroll partially down the page
    const iframe = page.frameLocator('.epub-view iframe');
    await iframe.locator('body').evaluate((body) => {
      body.scrollTop = body.scrollHeight * 0.3;  // 30% down
    });

    // 5. Wait for auto-save (2 second debounce + buffer)
    await page.waitForTimeout(3000);

    // 6. Get current CFI and scroll offset
    const currentCFI = await page.evaluate(() => {
      return (window as any).rendition?.location?.start?.cfi;
    });

    const currentScroll = await iframe.locator('body').evaluate((body) => {
      const scrollableHeight = body.scrollHeight - body.clientHeight;
      return (body.scrollTop / scrollableHeight) * 100;
    });

    expect(currentCFI).toBeTruthy();
    expect(currentCFI).toContain('epubcfi');
    expect(currentScroll).toBeGreaterThan(25);
    expect(currentScroll).toBeLessThan(35);

    // 7. Close and reopen browser (simulate)
    await page.close();

    // 8. Reopen same book
    const newPage = await page.context().newPage();
    await newPage.goto('/login');
    await newPage.fill('[name=email]', 'test@example.com');
    await newPage.fill('[name=password]', 'password123');
    await newPage.click('button[type=submit]');
    await newPage.waitForURL('/library');

    await newPage.click('[data-testid="book-card"]:first-child');
    await newPage.waitForSelector('.epub-reader');

    // 9. Wait for position restore
    await newPage.waitForTimeout(1500);

    // 10. Verify exact position restored
    const restoredCFI = await newPage.evaluate(() => {
      return (window as any).rendition?.location?.start?.cfi;
    });

    const restoredIframe = newPage.frameLocator('.epub-view iframe');
    const restoredScroll = await restoredIframe.locator('body').evaluate((body) => {
      const scrollableHeight = body.scrollHeight - body.clientHeight;
      return (body.scrollTop / scrollableHeight) * 100;
    });

    // CFI should be the same
    expect(restoredCFI).toBe(currentCFI);

    // Scroll offset should be within 2% (some variance acceptable)
    expect(Math.abs(restoredScroll - currentScroll!)).toBeLessThan(2);
  });

  test('generates image for highlighted description', async ({ page }) => {
    // 1. Open book
    await page.click('[data-testid="book-card"]:first-child');
    await page.waitForSelector('.epub-reader');

    // 2. Wait for descriptions to be highlighted
    await page.waitForSelector('.description-highlight', { timeout: 5000 });

    // 3. Click on highlighted description
    const highlight = page.locator('.description-highlight').first();
    await highlight.click();

    // 4. Verify modal opened
    await expect(page.locator('[data-testid="image-modal"]')).toBeVisible();

    // 5. Check description text is shown
    const descText = await page.locator('[data-testid="description-text"]').textContent();
    expect(descText).toBeTruthy();

    // 6. Click "Generate Image" button
    await page.click('[data-testid="generate-image-btn"]');

    // 7. Verify loading state
    await expect(page.locator('[data-testid="image-loading"]')).toBeVisible();

    // 8. Wait for image generation (max 60 seconds)
    await expect(
      page.locator('[data-testid="generated-image"]')
    ).toBeVisible({ timeout: 60000 });

    // 9. Verify image is displayed
    const imageSrc = await page.locator('[data-testid="generated-image"]').getAttribute('src');
    expect(imageSrc).toContain('/images/generated/');

    // 10. Close modal
    await page.click('[data-testid="close-modal"]');

    // 11. Click same highlight again - should show cached image
    await highlight.click();
    await expect(page.locator('[data-testid="image-modal"]')).toBeVisible();

    // Should show image immediately (no loading)
    await expect(page.locator('[data-testid="generated-image"]')).toBeVisible({ timeout: 1000 });
    expect(await page.locator('[data-testid="image-loading"]').isVisible()).toBe(false);
  });

  test('multi-device sync - reads on desktop, continues on mobile', async ({ page, browser }) => {
    // Simulate desktop reading
    await page.setViewportSize({ width: 1920, height: 1080 });

    // Open and read book
    await page.click('[data-testid="book-card"]:first-child');
    await page.waitForSelector('.epub-reader');

    // Read to chapter 5, 50% through
    for (let i = 0; i < 10; i++) {
      await page.click('[data-testid="next-page"]');
      await page.waitForTimeout(300);
    }

    // Wait for sync
    await page.waitForTimeout(3000);

    // Get desktop CFI
    const desktopCFI = await page.evaluate(() => {
      return (window as any).rendition?.location?.start?.cfi;
    });

    // Simulate mobile device
    const mobileContext = await browser.newContext({
      ...devices['iPhone 13']
    });

    const mobilePage = await mobileContext.newPage();

    // Login on mobile
    await mobilePage.goto('/login');
    await mobilePage.fill('[name=email]', 'test@example.com');
    await mobilePage.fill('[name=password]', 'password123');
    await mobilePage.click('button[type=submit]');
    await mobilePage.waitForURL('/library');

    // Open same book
    await mobilePage.click('[data-testid="book-card"]:first-child');
    await mobilePage.waitForSelector('.epub-reader');
    await mobilePage.waitForTimeout(1500);

    // Verify position synced from desktop
    const mobileCFI = await mobilePage.evaluate(() => {
      return (window as any).rendition?.location?.start?.cfi;
    });

    expect(mobileCFI).toBe(desktopCFI);

    await mobileContext.close();
  });
});

test.describe('Multi-NLP Generated Highlights E2E', () => {
  test('all 3 processors contribute to highlights', async ({ page }) => {
    // Upload book for processing
    await page.goto('/library');
    await page.click('[data-testid="upload-book"]');

    const fileInput = page.locator('input[type="file"]');
    await fileInput.setInputFiles('tests/fixtures/anna-karenina.epub');

    await page.click('[data-testid="confirm-upload"]');

    // Wait for Multi-NLP processing (ensemble mode)
    await expect(page.locator('[data-testid="processing-status"]')).toBeVisible();
    await expect(page.locator('[data-testid="processing-status"]')).toHaveText(/Processing.*ensemble/i);

    // Wait for completion (can take 30-60 seconds for large book)
    await expect(
      page.locator('[data-testid="processing-complete"]')
    ).toBeVisible({ timeout: 120000 });

    // Open processed book
    await page.click('[data-testid="book-card"]:first-child');
    await page.waitForSelector('.epub-reader');

    // Verify highlights are present
    await page.waitForSelector('.description-highlight');

    const highlightsCount = await page.locator('.description-highlight').count();

    // Book should have substantial highlights from ensemble
    expect(highlightsCount).toBeGreaterThan(50);

    // Check highlight metadata includes consensus info
    const firstHighlight = page.locator('.description-highlight').first();
    await firstHighlight.click();

    await expect(page.locator('[data-testid="image-modal"]')).toBeVisible();

    // Check consensus badge
    const consensusBadge = page.locator('[data-testid="consensus-badge"]');
    await expect(consensusBadge).toBeVisible();

    const consensusText = await consensusBadge.textContent();
    expect(consensusText).toMatch(/\d+ processors agree/i);
  });
});
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–°–∏—Å—Ç–µ–º–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è BookReader AI –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:

- **–ü–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ** –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
- **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ** –≤ CI/CD pipeline
- **Performance testing** –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç
- **E2E —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö user journeys
- **–õ–µ–≥–∫–æ—Å—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏** —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫–∏ –∏ —É—Ç–∏–ª–∏—Ç—ã
- **–ë—ã—Å—Ç—Ä–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ** —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **Multi-NLP System Testing** - comprehensive —Ç–µ—Å—Ç—ã –¥–ª—è –≤—Å–µ—Ö 3 –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ (–æ–∫—Ç—è–±—Ä—å 2025)
- **CFI System Testing** - backend –∏ frontend —Ç–µ—Å—Ç—ã –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞ (–æ–∫—Ç—è–±—Ä—å 2025)
- **E2E Reading Flow Tests** - –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —á—Ç–µ–Ω–∏—è —Å CFI restore (–æ–∫—Ç—è–±—Ä—å 2025)
- **Smart Highlights Testing** - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å epub.js highlights (–æ–∫—Ç—è–±—Ä—å 2025)

–í—Å–µ —Ç–µ—Å—Ç—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ continuous integration.
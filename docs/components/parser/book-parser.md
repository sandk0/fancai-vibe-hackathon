# Book Parser - BookReader AI

–°–∏—Å—Ç–µ–º–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö –∫–Ω–∏–≥ –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö EPUB –∏ FB2 —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≥–ª–∞–≤, –æ–±–ª–æ–∂–µ–∫ –∏ **CFI (Canonical Fragment Identifier) –¥–ª—è epub.js –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏**.

## üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï (23.10.2025)

**–ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** –ü–æ–¥–¥–µ—Ä–∂–∫–∞ CFI (Canonical Fragment Identifier) –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ epub.js.

### –ß—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ:
- **CFI Generation** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 2000 location —Ç–æ—á–µ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏
- **epub.js compatibility** - –ø–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å epub.js 0.3.93
- **Location-based progress** - —Ç–æ—á–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á—Ç–µ–Ω–∏—è (0-100%)
- **Hybrid restoration** - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è CFI + scroll offset –¥–ª—è pixel-perfect –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞—Ä—Å–µ—Ä–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã
- **Multi-format support** - EPUB 2.0/3.0, FB2.0/2.1
- **CFI support** - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è locations –¥–ª—è epub.js –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ (NEW)
- **Robust parsing** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
- **Metadata extraction** - –ø–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **Content normalization** - –æ—á–∏—Å—Ç–∫–∞ HTML –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
- **Error handling** - graceful degradation –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å —Ñ–∞–π–ª–∞–º–∏
- **Memory efficiency** - –ø–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
```
EPUB ‚Üí Electronic Publication (2.0, 3.0)
‚îú‚îÄ‚îÄ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ZIP –∞—Ä—Ö–∏–≤–∞
‚îú‚îÄ‚îÄ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ OPF (Dublin Core)
‚îú‚îÄ‚îÄ –ù–∞–≤–∏–≥–∞—Ü–∏—è —á–µ—Ä–µ–∑ NCX/NAV
‚îî‚îÄ‚îÄ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ XHTML/HTML

FB2 ‚Üí FictionBook 2.0/2.1
‚îú‚îÄ‚îÄ XML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
‚îú‚îÄ‚îÄ –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
‚îú‚îÄ‚îÄ –°–µ–∫—Ü–∏–æ–Ω–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
‚îî‚îÄ‚îÄ –ë–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–±–ª–æ–∂–∫–∏) –≤ base64
```

---

## –ö–ª–∞—Å—Å BookParser

**–§–∞–π–ª:** `backend/app/services/book_parser.py`

### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
```python
class BookParser:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö –∫–Ω–∏–≥.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ:
    - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (title, author, genre, language, etc.)
    - –°—Ç—Ä—É–∫—Ç—É—Ä—ã –≥–ª–∞–≤ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
    - –û–±–ª–æ–∂–µ–∫ –∫–Ω–∏–≥
    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (ISBN, publisher, etc.)
    """
    
    def __init__(self):
        self.supported_formats = [".epub", ".fb2", ".fb2.zip"]
        self.temp_dir = Path(tempfile.mkdtemp(prefix="bookreader_"))
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.max_file_size = 100 * 1024 * 1024  # 100MB
        self.max_chapters = 1000  # –ó–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏—Ö –∫–Ω–∏–≥
        self.encoding_fallbacks = ["utf-8", "windows-1251", "cp1252", "iso-8859-1"]
        
    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Cleanup –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
```

---

## –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã

### validate_book_file()
```python
def validate_book_file(self, file_path: str) -> ValidationResult:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –∫–Ω–∏–≥–∏ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞.
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–Ω–∏–≥–∏
        
    Returns:
        ValidationResult —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞
        
    Checks:
    - –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
    - –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    - –§–∞–π–ª –Ω–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω
    - –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
    """
    
    file_path = Path(file_path)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    if not file_path.exists():
        return ValidationResult(
            is_valid=False,
            errors=["File does not exist"],
            file_info=None
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
    file_size = file_path.stat().st_size
    if file_size > self.max_file_size:
        return ValidationResult(
            is_valid=False,
            errors=[f"File too large: {file_size / 1024 / 1024:.1f}MB (max: {self.max_file_size / 1024 / 1024}MB)"]
        )
    
    if file_size < 1024:  # –ú–∏–Ω–∏–º—É–º 1KB
        return ValidationResult(
            is_valid=False,
            errors=["File too small, likely corrupted"]
        )
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
    file_format = self._detect_format(file_path)
    if not file_format:
        return ValidationResult(
            is_valid=False,
            errors=["Unsupported file format"]
        )
    
    # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    try:
        if file_format == "epub":
            validation_info = self._validate_epub_structure(file_path)
        elif file_format == "fb2":
            validation_info = self._validate_fb2_structure(file_path)
        else:
            return ValidationResult(is_valid=False, errors=["Unknown format"])
            
        return ValidationResult(
            is_valid=True,
            errors=[],
            file_info=FileInfo(
                filename=file_path.name,
                file_size=file_size,
                format=file_format,
                **validation_info
            )
        )
        
    except Exception as e:
        return ValidationResult(
            is_valid=False,
            errors=[f"Validation failed: {str(e)}"],
            file_info=FileInfo(
                filename=file_path.name,
                file_size=file_size,
                format=file_format
            )
        )

def _detect_format(self, file_path: Path) -> Optional[str]:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –∏ magic bytes."""
    
    extension = file_path.suffix.lower()
    
    # –ü–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
    if extension == ".epub":
        return "epub"
    elif extension == ".fb2":
        return "fb2"
    elif extension == ".zip":
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ EPUB –∏–ª–∏ FB2 –≤ ZIP
        if self._is_epub_zip(file_path):
            return "epub"
        elif self._is_fb2_zip(file_path):
            return "fb2"
    
    # –ü–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É (magic bytes)
    try:
        with open(file_path, "rb") as f:
            header = f.read(512)
            
        if header.startswith(b"PK\x03\x04"):  # ZIP signature
            if b"mimetype" in header and b"application/epub+zip" in header[:100]:
                return "epub"
        elif header.startswith(b"<?xml") or header.startswith(b"\xef\xbb\xbf<?xml"):
            if b"<FictionBook" in header[:200]:
                return "fb2"
                
    except Exception:
        pass
        
    return None
```

### parse_book()
```python
def parse_book(self, file_path: str, user_id: UUID) -> BookParsingResult:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–Ω–∏–≥–∏.
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–Ω–∏–≥–∏
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        
    Returns:
        BookParsingResult —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        
    Process:
    1. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
    2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –≤—ã–±–æ—Ä –ø–∞—Ä—Å–µ—Ä–∞
    3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    4. –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –≥–ª–∞–≤
    5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–ª–æ–∂–∫–∏
    6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    """
    
    parsing_start = time.time()
    
    # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è
    validation = self.validate_book_file(file_path)
    if not validation.is_valid:
        return BookParsingResult(
            success=False,
            errors=validation.errors,
            parsing_time=time.time() - parsing_start
        )
    
    file_format = validation.file_info.format
    logger.info(f"Starting {file_format.upper()} parsing for user {user_id}")
    
    try:
        # 2. –í—ã–±–æ—Ä –ø–∞—Ä—Å–µ—Ä–∞ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É
        if file_format == "epub":
            result = self._parse_epub(file_path)
        elif file_format == "fb2":
            result = self._parse_fb2(file_path)
        else:
            raise ValueError(f"Unsupported format: {file_format}")
            
        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not result.metadata.title:
            result.metadata.title = Path(file_path).stem
            
        if not result.chapters:
            raise ValueError("No chapters found in book")
            
        # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        result = self._normalize_parsing_result(result)
        
        parsing_time = time.time() - parsing_start
        
        return BookParsingResult(
            success=True,
            metadata=result.metadata,
            chapters=result.chapters,
            cover_image=result.cover_image,
            parsing_time=parsing_time,
            file_info=validation.file_info,
            stats=ParsingStats(
                chapters_found=len(result.chapters),
                total_words=sum(ch.word_count for ch in result.chapters),
                has_cover=bool(result.cover_image),
                format=file_format
            )
        )
        
    except Exception as e:
        logger.error(f"Parsing failed for {file_path}: {str(e)}")
        return BookParsingResult(
            success=False,
            errors=[str(e)],
            parsing_time=time.time() - parsing_start,
            file_info=validation.file_info
        )
```

---

## EPUB Parser

### _parse_epub()
```python
def _parse_epub(self, file_path: str) -> EpubParsingResult:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ EPUB —Ñ–∞–π–ª–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–µ—Ä—Å–∏–π 2.0 –∏ 3.0.
    
    EPUB Structure:
    - META-INF/container.xml ‚Üí —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ OPF —Ñ–∞–π–ª
    - *.opf ‚Üí Package file —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ manifest
    - toc.ncx –∏–ª–∏ nav.xhtml ‚Üí –Ω–∞–≤–∏–≥–∞—Ü–∏—è
    - OEBPS/ –∏–ª–∏ –∞–Ω–∞–ª–æ–≥ ‚Üí —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤
    """
    
    import zipfile
    from xml.etree import ElementTree as ET
    from bs4 import BeautifulSoup
    
    with zipfile.ZipFile(file_path, 'r') as epub_zip:
        # 1. –ü–æ–∏—Å–∫ OPF —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ container.xml
        try:
            container_xml = epub_zip.read("META-INF/container.xml")
            container_tree = ET.fromstring(container_xml)
            
            # Namespace handling –¥–ª—è EPUB
            ns = {'container': 'urn:oasis:names:tc:opendocument:xmlns:container'}
            opf_path = container_tree.find('.//container:rootfile', ns).get('full-path')
            
        except Exception as e:
            raise ValueError(f"Invalid EPUB structure: cannot find OPF file ({str(e)})")
        
        # 2. –ü–∞—Ä—Å–∏–Ω–≥ OPF —Ñ–∞–π–ª–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        try:
            opf_content = epub_zip.read(opf_path)
            opf_tree = ET.fromstring(opf_content)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º namespace –¥–ª—è OPF
            opf_ns = {'opf': 'http://www.idpf.org/2007/opf'}
            dc_ns = {'dc': 'http://purl.org/dc/elements/1.1/'}
            
            metadata = self._extract_epub_metadata(opf_tree, opf_ns, dc_ns)
            
        except Exception as e:
            logger.warning(f"Failed to parse EPUB metadata: {str(e)}")
            metadata = BookMetadata(title="Unknown")
        
        # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ –∏ spine
        try:
            manifest_items = {}
            for item in opf_tree.findall('.//opf:item', opf_ns):
                item_id = item.get('id')
                item_href = item.get('href')
                item_type = item.get('media-type')
                
                # Resolve –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                base_dir = str(Path(opf_path).parent)
                full_path = str(Path(base_dir) / item_href) if base_dir != '.' else item_href
                
                manifest_items[item_id] = {
                    'href': full_path,
                    'media_type': item_type
                }
            
            # Spine –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —á—Ç–µ–Ω–∏—è
            spine_items = []
            for itemref in opf_tree.findall('.//opf:itemref', opf_ns):
                idref = itemref.get('idref')
                if idref in manifest_items:
                    spine_items.append(manifest_items[idref])
                    
        except Exception as e:
            raise ValueError(f"Failed to parse EPUB manifest: {str(e)}")
        
        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–∞–≤ –∏–∑ spine
        chapters = []
        chapter_number = 1
        
        for spine_item in spine_items:
            if spine_item['media_type'] in ['application/xhtml+xml', 'text/html']:
                try:
                    chapter_content = epub_zip.read(spine_item['href'])
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ encoding
                    encoding = self._detect_encoding(chapter_content)
                    chapter_html = chapter_content.decode(encoding, errors='replace')
                    
                    # –ü–∞—Ä—Å–∏–Ω–≥ HTML —Å BeautifulSoup
                    soup = BeautifulSoup(chapter_html, 'html.parser')
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≥–ª–∞–≤—ã
                    chapter_title = self._extract_chapter_title(soup)
                    
                    # –û—á–∏—Å—Ç–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                    cleaned_content = self._clean_html_content(soup)
                    
                    if len(cleaned_content.strip()) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–ª–∞–≤—ã
                        chapters.append(ChapterData(
                            number=chapter_number,
                            title=chapter_title or f"–ì–ª–∞–≤–∞ {chapter_number}",
                            content=cleaned_content,
                            word_count=len(cleaned_content.split()),
                            source_file=spine_item['href']
                        ))
                        chapter_number += 1
                        
                except Exception as e:
                    logger.warning(f"Failed to parse chapter {spine_item['href']}: {str(e)}")
                    continue
        
        # 5. –ü–æ–∏—Å–∫ –æ–±–ª–æ–∂–∫–∏
        cover_image = self._extract_epub_cover(epub_zip, opf_tree, manifest_items, opf_ns)
        
        return EpubParsingResult(
            metadata=metadata,
            chapters=chapters,
            cover_image=cover_image
        )

def _extract_epub_metadata(self, opf_tree, opf_ns: dict, dc_ns: dict) -> BookMetadata:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ OPF —Ñ–∞–π–ª–∞."""
    
    def get_dc_element(name: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ Dublin Core —ç–ª–µ–º–µ–Ω—Ç–∞."""
        elements = opf_tree.findall(f'.//dc:{name}', dc_ns)
        return elements[0].text.strip() if elements and elements[0].text else None
    
    def get_meta_content(name: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ meta –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ name/property."""
        # EPUB 2.0 style
        meta = opf_tree.find(f'.//opf:meta[@name="{name}"]', opf_ns)
        if meta is not None:
            return meta.get('content')
            
        # EPUB 3.0 style
        meta = opf_tree.find(f'.//opf:meta[@property="{name}"]', opf_ns)
        if meta is not None:
            return meta.text
            
        return None
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    title = get_dc_element('title') or 'Untitled'
    
    # –ê–≤—Ç–æ—Ä—ã (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)
    creators = opf_tree.findall('.//dc:creator', dc_ns)
    authors = []
    for creator in creators:
        if creator.text:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º role (author, editor, translator, etc.)
            role = creator.get(f'{{{opf_ns["opf"]}}}role', 'author')
            if role in ['author', 'aut']:
                authors.append(creator.text.strip())
    
    author = ', '.join(authors) if authors else None
    
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    language = get_dc_element('language') or 'unknown'
    publisher = get_dc_element('publisher')
    description = get_dc_element('description')
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
    isbn = get_dc_element('identifier')
    publication_date = get_dc_element('date')
    
    # –ñ–∞–Ω—Ä/subject
    subjects = opf_tree.findall('.//dc:subject', dc_ns)
    genres = [subj.text.strip() for subj in subjects if subj.text]
    genre = genres[0] if genres else None
    
    return BookMetadata(
        title=title,
        author=author,
        language=language,
        publisher=publisher,
        description=description,
        isbn=isbn,
        publication_date=publication_date,
        genre=genre,
        subjects=genres
    )

def _extract_epub_cover(
    self, 
    epub_zip: zipfile.ZipFile, 
    opf_tree, 
    manifest_items: dict, 
    opf_ns: dict
) -> Optional[bytes]:
    """–ü–æ–∏—Å–∫ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–ª–æ–∂–∫–∏ EPUB –∫–Ω–∏–≥–∏."""
    
    cover_candidates = []
    
    # –ú–µ—Ç–æ–¥ 1: —á–µ—Ä–µ–∑ metadata cover
    cover_meta = opf_tree.find('.//opf:meta[@name="cover"]', opf_ns)
    if cover_meta is not None:
        cover_id = cover_meta.get('content')
        if cover_id in manifest_items:
            cover_candidates.append(manifest_items[cover_id]['href'])
    
    # –ú–µ—Ç–æ–¥ 2: –ø–æ–∏—Å–∫ –ø–æ ID —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "cover"
    for item_id, item_data in manifest_items.items():
        if 'cover' in item_id.lower() and item_data['media_type'].startswith('image/'):
            cover_candidates.append(item_data['href'])
    
    # –ú–µ—Ç–æ–¥ 3: –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ —Å –∏–º–µ–Ω–∞–º–∏ cover.*
    for filename in epub_zip.namelist():
        if 'cover' in Path(filename).stem.lower() and filename.lower().endswith(('.jpg', '.jpeg', '.png')):
            cover_candidates.append(filename)
    
    # –ú–µ—Ç–æ–¥ 4: –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ
    for filename in epub_zip.namelist():
        if filename.lower().endswith(('.jpg', '.jpeg', '.png')) and '/' not in filename:
            cover_candidates.append(filename)
    
    # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±–ª–æ–∂–∫–∏
    for cover_path in cover_candidates:
        try:
            cover_data = epub_zip.read(cover_path)
            if len(cover_data) > 1000:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                return cover_data
        except Exception:
            continue
    
    return None
```

---

## FB2 Parser

### _parse_fb2()
```python
def _parse_fb2(self, file_path: str) -> FB2ParsingResult:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ FB2 —Ñ–∞–π–ª–æ–≤ (FictionBook format).
    
    FB2 Structure:
    - <FictionBook> –∫–æ—Ä–Ω–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç
    - <description> –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
    - <body> –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    - <binary> –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (base64)
    """
    
    from xml.etree import ElementTree as ET
    import base64
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ encoding –∏ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    with open(file_path, 'rb') as f:
        raw_content = f.read()
    
    encoding = self._detect_encoding(raw_content)
    content = raw_content.decode(encoding, errors='replace')
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ BOM –µ—Å–ª–∏ –µ—Å—Ç—å
    if content.startswith('\ufeff'):
        content = content[1:]
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ XML
        root = ET.fromstring(content)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        if not root.tag.endswith('FictionBook'):
            raise ValueError("Not a valid FB2 file: missing FictionBook root element")
            
    except ET.ParseError as e:
        raise ValueError(f"Invalid FB2 XML structure: {str(e)}")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = self._extract_fb2_metadata(root)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–∞–≤
    chapters = self._extract_fb2_chapters(root)
    
    # –ü–æ–∏—Å–∫ –æ–±–ª–æ–∂–∫–∏
    cover_image = self._extract_fb2_cover(root)
    
    return FB2ParsingResult(
        metadata=metadata,
        chapters=chapters,
        cover_image=cover_image
    )

def _extract_fb2_metadata(self, root) -> BookMetadata:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ FB2 description —Å–µ–∫—Ü–∏–∏."""
    
    def find_text(element, path: str) -> Optional[str]:
        """–ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ –ø–æ XPath –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        found = element.find(path)
        return found.text.strip() if found is not None and found.text else None
    
    # –ü–æ–∏—Å–∫ —Å–µ–∫—Ü–∏–∏ description
    description_elem = root.find('description')
    if description_elem is None:
        return BookMetadata(title='Unknown FB2 Book')
    
    # title-info —Å–µ–∫—Ü–∏—è
    title_info = description_elem.find('title-info')
    if title_info is None:
        return BookMetadata(title='Unknown FB2 Book')
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    title_elem = title_info.find('book-title')
    title = title_elem.text.strip() if title_elem is not None and title_elem.text else 'Untitled'
    
    # –ê–≤—Ç–æ—Ä—ã
    authors = []
    for author_elem in title_info.findall('author'):
        author_parts = []
        
        first_name = find_text(author_elem, 'first-name')
        middle_name = find_text(author_elem, 'middle-name') 
        last_name = find_text(author_elem, 'last-name')
        
        if first_name:
            author_parts.append(first_name)
        if middle_name:
            author_parts.append(middle_name)
        if last_name:
            author_parts.append(last_name)
            
        if author_parts:
            authors.append(' '.join(author_parts))
    
    author = ', '.join(authors) if authors else None
    
    # –ñ–∞–Ω—Ä—ã
    genres = []
    for genre_elem in title_info.findall('genre'):
        if genre_elem.text:
            genres.append(genre_elem.text.strip())
    genre = genres[0] if genres else None
    
    # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è
    annotation_elem = title_info.find('annotation')
    description = None
    if annotation_elem is not None:
        # FB2 –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = []
        for p in annotation_elem.findall('p'):
            if p.text:
                paragraphs.append(p.text.strip())
        description = '\n'.join(paragraphs) if paragraphs else None
    
    # –Ø–∑—ã–∫
    language = find_text(title_info, 'lang') or 'ru'
    
    # –î–∞—Ç–∞
    date_elem = title_info.find('date')
    publication_date = date_elem.get('value') if date_elem is not None else None
    
    # publish-info —Å–µ–∫—Ü–∏—è (–∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
    publish_info = description_elem.find('publish-info')
    publisher = None
    isbn = None
    
    if publish_info is not None:
        publisher = find_text(publish_info, 'publisher')
        isbn = find_text(publish_info, 'isbn')
    
    return BookMetadata(
        title=title,
        author=author,
        language=language,
        publisher=publisher,
        description=description,
        isbn=isbn,
        publication_date=publication_date,
        genre=genre,
        subjects=genres
    )

def _extract_fb2_chapters(self, root) -> List[ChapterData]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–∞–≤ –∏–∑ FB2 body —Å–µ–∫—Ü–∏–π."""
    
    chapters = []
    chapter_number = 1
    
    # FB2 –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ body —Å–µ–∫—Ü–∏–π
    for body in root.findall('body'):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º notes body
        if body.get('name') in ['notes', 'comments']:
            continue
            
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ body (–µ—Å–ª–∏ –µ—Å—Ç—å)
        body_title = None
        title_elem = body.find('title')
        if title_elem is not None:
            body_title = self._extract_fb2_text(title_elem)
        
        # –°–µ–∫—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ body
        sections = body.findall('section')
        
        if not sections:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–∫—Ü–∏–π, –≤—Å—è body —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –≥–ª–∞–≤–æ–π
            content = self._extract_fb2_text(body)
            if content and len(content.strip()) > 50:
                chapters.append(ChapterData(
                    number=chapter_number,
                    title=body_title or f"–ì–ª–∞–≤–∞ {chapter_number}",
                    content=content,
                    word_count=len(content.split())
                ))
                chapter_number += 1
        else:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—Ü–∏—é –∫–∞–∫ –≥–ª–∞–≤—É
            for section in sections:
                chapter_title = body_title
                
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
                section_title = section.find('title')
                if section_title is not None:
                    section_title_text = self._extract_fb2_text(section_title)
                    chapter_title = section_title_text or chapter_title
                
                # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–µ–∫—Ü–∏–∏
                content = self._extract_fb2_section_content(section)
                
                if content and len(content.strip()) > 50:
                    chapters.append(ChapterData(
                        number=chapter_number,
                        title=chapter_title or f"–ì–ª–∞–≤–∞ {chapter_number}",
                        content=content,
                        word_count=len(content.split())
                    ))
                    chapter_number += 1
    
    return chapters

def _extract_fb2_text(self, element) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ FB2 —ç–ª–µ–º–µ–Ω—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤."""
    
    def extract_recursive(elem) -> List[str]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞."""
        parts = []
        
        # –¢–µ–∫—Å—Ç —Å–∞–º–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        if elem.text:
            parts.append(elem.text.strip())
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ—á–µ—Ä–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        for child in elem:
            if child.tag == 'p':
                # –ü–∞—Ä–∞–≥—Ä–∞—Ñ - –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
                child_text = extract_recursive(child)
                if child_text:
                    parts.extend(child_text)
                    parts.append('\n')
            elif child.tag in ['emphasis', 'strong', 'strikethrough']:
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ —Ç–µ–≥–æ–≤
                child_text = extract_recursive(child)
                parts.extend(child_text)
            elif child.tag == 'empty-line':
                parts.append('\n')
            else:
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏ - —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                child_text = extract_recursive(child)
                parts.extend(child_text)
            
            # Tail —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç–µ–≥–∞
            if child.tail:
                parts.append(child.tail.strip())
    
    return ''.join(parts)

def _extract_fb2_cover(self, root) -> Optional[bytes]:
    """–ü–æ–∏—Å–∫ –æ–±–ª–æ–∂–∫–∏ –≤ FB2 —Ñ–∞–π–ª–µ."""
    
    # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±–ª–æ–∂–∫—É –≤ description
    description = root.find('description')
    if description is not None:
        title_info = description.find('title-info')
        if title_info is not None:
            coverpage = title_info.find('coverpage')
            if coverpage is not None:
                image_ref = coverpage.find('image')
                if image_ref is not None:
                    href = image_ref.get('{http://www.w3.org/1999/xlink}href')
                    if href and href.startswith('#'):
                        image_id = href[1:]  # –£–±–∏—Ä–∞–µ–º #
                        
                        # –ü–æ–∏—Å–∫ binary —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º id
                        for binary in root.findall('binary'):
                            if binary.get('id') == image_id:
                                try:
                                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64
                                    image_data = base64.b64decode(binary.text.strip())
                                    return image_data
                                except Exception:
                                    continue
    
    # Fallback: –ø–µ—Ä–≤–æ–µ binary –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    for binary in root.findall('binary'):
        content_type = binary.get('content-type', '')
        if content_type.startswith('image/'):
            try:
                image_data = base64.b64decode(binary.text.strip())
                return image_data
            except Exception:
                continue
    
    return None
```

---

## –£—Ç–∏–ª–∏—Ç—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

### Content Cleaning
```python
def _clean_html_content(self, soup: BeautifulSoup) -> str:
    """
    –û—á–∏—Å—Ç–∫–∞ HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ readable —Ñ–æ—Ä–º–∞—Ç—É.
    
    Removes:
    - HTML —Ç–µ–≥–∏ (–∫—Ä–æ–º–µ –∑–Ω–∞—á–∏–º—ã—Ö –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
    - JavaScript –∏ CSS
    - –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    - –õ–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    
    Preserves:
    - –°—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    - –ü–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≥–¥–µ –Ω—É–∂–Ω–æ
    - –ë–∞–∑–æ–≤–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    """
    
    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
    for script in soup(["script", "style", "noscript"]):
        script.decompose()
    
    # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    from bs4 import Comment
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()
    
    # –ó–∞–º–µ–Ω–∞ —Ç–µ–≥–æ–≤ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç
    for br in soup.find_all("br"):
        br.replace_with("\n")
    
    for hr in soup.find_all("hr"):
        hr.replace_with("\n---\n")
    
    # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    for tag in soup.find_all(["p", "div", "h1", "h2", "h3", "h4", "h5", "h6"]):
        if tag.string:
            tag.string = f"\n{tag.get_text()}\n"
    
    # –ü–æ–ª—É—á–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    text = soup.get_text()
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
    lines = []
    for line in text.split('\n'):
        cleaned_line = ' '.join(line.split())  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        if cleaned_line:
            lines.append(cleaned_line)
        elif lines and lines[-1]:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            lines.append('')
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    result_lines = []
    prev_empty = False
    
    for line in lines:
        if line:
            result_lines.append(line)
            prev_empty = False
        elif not prev_empty:
            result_lines.append('')
            prev_empty = True
    
    return '\n'.join(result_lines).strip()

def _extract_chapter_title(self, soup: BeautifulSoup) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≥–ª–∞–≤—ã –∏–∑ HTML."""
    
    # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    title_selectors = [
        'h1', 'h2', 'h3',
        '.chapter-title', '.title', 
        '[class*="title"]', '[class*="heading"]'
    ]
    
    for selector in title_selectors:
        elements = soup.select(selector)
        for element in elements:
            title_text = element.get_text().strip()
            if title_text and len(title_text) < 200:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                return title_text
    
    # Fallback: –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –µ—Å–ª–∏ –æ–Ω–∞ –∫–æ—Ä–æ—Ç–∫–∞—è
    text = soup.get_text().strip()
    if text:
        first_line = text.split('\n')[0].strip()
        if len(first_line) < 100 and not first_line.endswith('.'):
            return first_line
    
    return None

def _detect_encoding(self, content: bytes) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–∞ —Å fallback –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏."""
    
    # –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ chardet
    try:
        import chardet
        detected = chardet.detect(content[:10000])  # –ü–µ—Ä–≤—ã–µ 10KB –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if detected and detected['confidence'] > 0.7:
            return detected['encoding']
    except ImportError:
        pass
    
    # –ü–æ–∏—Å–∫ –≤ XML declaration
    content_str = content[:1000].decode('ascii', errors='ignore')
    encoding_match = re.search(r'encoding=["\']([^"\']+)["\']', content_str, re.IGNORECASE)
    if encoding_match:
        encoding = encoding_match.group(1).lower()
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∏—Ä–æ–≤–æ–∫
        encoding_map = {
            'windows-1251': 'cp1251',
            'win-1251': 'cp1251',
            'utf8': 'utf-8'
        }
        return encoding_map.get(encoding, encoding)
    
    # Fallback —á–µ—Ä–µ–∑ –ø–æ–ø—ã—Ç–∫–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    for encoding in self.encoding_fallbacks:
        try:
            content.decode(encoding)
            return encoding
        except UnicodeDecodeError:
            continue
    
    return 'utf-8'  # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback
```

### File Management
```python
async def save_parsed_book(
    self,
    parsing_result: BookParsingResult,
    user_id: UUID,
    session: AsyncSession
) -> Book:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        parsing_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        session: –°–µ—Å—Å–∏—è –ë–î
        
    Returns:
        –°–æ–∑–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Book
    """
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∫–Ω–∏–≥–∏
    book = Book(
        user_id=user_id,
        title=parsing_result.metadata.title,
        author=parsing_result.metadata.author,
        language=parsing_result.metadata.language,
        description=parsing_result.metadata.description,
        file_format=BookFormat(parsing_result.file_info.format),
        file_size=parsing_result.file_info.file_size,
        is_parsed=True,
        parsing_progress=100,
        book_metadata={
            'publisher': parsing_result.metadata.publisher,
            'isbn': parsing_result.metadata.isbn,
            'publication_date': parsing_result.metadata.publication_date,
            'subjects': parsing_result.metadata.subjects,
            'parsing_stats': parsing_result.stats.__dict__
        }
    )
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–Ω—Ä–∞ –ø–æ keywords
    if parsing_result.metadata.genre:
        book.genre = self._map_genre(parsing_result.metadata.genre)
    
    session.add(book)
    await session.flush()  # –ü–æ–ª—É—á–∞–µ–º ID –∫–Ω–∏–≥–∏
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–∞–≤
    for chapter_data in parsing_result.chapters:
        chapter = Chapter(
            book_id=book.id,
            chapter_number=chapter_data.number,
            title=chapter_data.title,
            content=chapter_data.content,
            word_count=chapter_data.word_count,
            estimated_reading_time=chapter_data.word_count // 200  # ~200 WPM
        )
        session.add(chapter)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–ª–æ–∂–∫–∏
    if parsing_result.cover_image:
        cover_path = await self._save_cover_image(
            parsing_result.cover_image,
            book.id
        )
        book.cover_image = cover_path
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–Ω–∏–≥–∏
    book.total_pages = sum(ch.word_count // 250 for ch in parsing_result.chapters)  # ~250 —Å–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
    book.estimated_reading_time = sum(ch.word_count for ch in parsing_result.chapters) // 200  # –º–∏–Ω—É—Ç—ã
    
    await session.commit()
    
    logger.info(f"Book '{book.title}' parsed and saved successfully. Chapters: {len(parsing_result.chapters)}")
    
    return book

def _map_genre(self, genre_text: str) -> BookGenre:
    """–ú–∞–ø–ø–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∂–∞–Ω—Ä–∞ –≤ enum."""
    
    genre_mapping = {
        # –†—É—Å—Å–∫–∏–µ –∂–∞–Ω—Ä—ã
        '—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞': BookGenre.SCIFI,
        '–Ω–∞—É—á–Ω–∞—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞': BookGenre.SCIFI,
        '—Ñ—ç–Ω—Ç–µ–∑–∏': BookGenre.FANTASY,
        '–¥–µ—Ç–µ–∫—Ç–∏–≤': BookGenre.DETECTIVE,
        '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π': BookGenre.HISTORICAL,
        '—Ä–æ–º–∞–Ω': BookGenre.ROMANCE,
        '—Ç—Ä–∏–ª–ª–µ—Ä': BookGenre.THRILLER,
        '—É–∂–∞—Å—ã': BookGenre.HORROR,
        '–∫–ª–∞—Å—Å–∏–∫–∞': BookGenre.CLASSIC,
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∂–∞–Ω—Ä—ã
        'science fiction': BookGenre.SCIFI,
        'fantasy': BookGenre.FANTASY,
        'detective': BookGenre.DETECTIVE,
        'mystery': BookGenre.DETECTIVE,
        'historical': BookGenre.HISTORICAL,
        'romance': BookGenre.ROMANCE,
        'thriller': BookGenre.THRILLER,
        'horror': BookGenre.HORROR,
        'classic': BookGenre.CLASSIC
    }
    
    genre_lower = genre_text.lower()
    
    for key, value in genre_mapping.items():
        if key in genre_lower:
            return value
    
    return BookGenre.OTHER
```

---

## Error Handling –∏ Logging

### ParsingError Classes
```python
class ParsingError(Exception):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    pass

class UnsupportedFormatError(ParsingError):
    """–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞."""
    pass

class CorruptedFileError(ParsingError):
    """–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª."""
    pass

class EncodingError(ParsingError):
    """–û—à–∏–±–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∏."""
    pass

class StructureError(ParsingError):
    """–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–Ω–∏–≥–∏."""
    pass

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
def log_parsing_context(func):
    @wraps(func)
    def wrapper(self, file_path: str, *args, **kwargs):
        file_name = Path(file_path).name
        
        logger.info(f"Starting parsing: {file_name}")
        
        try:
            result = func(self, file_path, *args, **kwargs)
            
            if hasattr(result, 'success') and result.success:
                logger.info(f"Parsing successful: {file_name}, chapters: {len(result.chapters)}")
            else:
                logger.warning(f"Parsing failed: {file_name}, errors: {result.errors}")
                
            return result
            
        except Exception as e:
            logger.error(f"Parsing exception for {file_name}: {str(e)}", exc_info=True)
            raise
            
    return wrapper
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

Book Parser —Å–∏—Å—Ç–µ–º—ã BookReader AI –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:

- **–ù–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥** EPUB –∏ FB2 —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
- **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** –≤–∫–ª—é—á–∞—è –∞–≤—Ç–æ—Ä–∞, –∂–∞–Ω—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ, –∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞** —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –≥–ª–∞–≤—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞
- **–û–±—Ä–∞–±–æ—Ç–∫—É –æ–±–ª–æ–∂–µ–∫** —Å –ø–æ–∏—Å–∫–æ–º –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç–æ–¥–∞–º
- **Error resilience** —Å graceful degradation –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
- **Production –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å** —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–∞

–ü–∞—Ä—Å–µ—Ä –≥–æ—Ç–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ production —Å—Ä–µ–¥—É –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö –∫–Ω–∏–≥.
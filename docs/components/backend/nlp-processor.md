# Advanced Multi-NLP System - BookReader AI

–î–µ—Ç–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π Multi-NLP —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –∫–Ω–∏–≥. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–∞.

## ü§ñ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï (03.09.2025)

**–ü—Ä–æ–±–ª–µ–º–∞:** –û–¥–∏–Ω–æ—á–Ω—ã–π spaCy –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–ª –∫–∞—á–µ—Å—Ç–≤–æ –∏ –ø–æ–ª–Ω–æ—Ç—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π.

**–†–ï–®–ï–ù–ò–ï:** –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É.

### üöÄ –†–ï–ó–£–õ–¨–¢–ê–¢–´:
- **2171 –æ–ø–∏—Å–∞–Ω–∏–µ** –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –∫–Ω–∏–≥–µ (25 –≥–ª–∞–≤) –∑–∞ **4 —Å–µ–∫—É–Ω–¥—ã**
- **300%+ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ** –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
- **Ensemble voting** —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
- **Adaptive —Ä–µ–∂–∏–º** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

---

## –ù–û–í–ê–Ø –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Multi-NLP –°–∏—Å—Ç–µ–º—ã

### Multi-NLP Manager (multi_nlp_manager.py)

–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏:

```python
class MultiNLPManager:
    def __init__(self):
        self.processors: Dict[str, EnhancedNLPProcessor] = {}
        self.processor_configs: Dict[str, ProcessorConfig] = {}
        self.processing_mode = ProcessingMode.SINGLE
        self.global_config = {
            'max_parallel_processors': 3,
            'ensemble_voting_threshold': 0.6,
            'adaptive_text_analysis': True
        }
        
    async def extract_descriptions(
        self, text: str, chapter_id: str = None,
        mode: ProcessingMode = None
    ) -> ProcessingResult
```

### –¢—Ä–∏ NLP –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞

1. **EnhancedSpacyProcessor** (enhanced_nlp_system.py)
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
   - Entity types: ['PERSON', 'LOC', 'GPE', 'FAC', 'ORG']
   - Character detection boost: 1.2x, Location detection boost: 1.1x
   - Atmosphere keywords –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è

2. **EnhancedNatashaProcessor** (natasha_processor.py)
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–∏—Ö –∏–º–µ–Ω–∞—Ö –∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è—Ö
   - Weight 1.2 (–≤—ã—à–µ —á–µ–º SpaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
   - Literary boost 1.3x –¥–ª—è —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

3. **EnhancedStanzaProcessor** (stanza_processor.py)
   - –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
   - –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á–µ–Ω (enabled=false)
   - Weight 0.8, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Å–æ–±–æ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

---

## üîÑ –ü—è—Ç—å –†–µ–∂–∏–º–æ–≤ –û–±—Ä–∞–±–æ—Ç–∫–∏

### ProcessingMode Enum
```python
class ProcessingMode(Enum):
    SINGLE = "single"           # –û–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    PARALLEL = "parallel"       # –ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    SEQUENTIAL = "sequential"   # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    ENSEMBLE = "ensemble"       # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    ADAPTIVE = "adaptive"       # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä
```

#### 1. Single Mode
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é spacy)
- –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

#### 2. Parallel Mode
- –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å asyncio.gather
- –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å deduplication

#### 3. Sequential Mode
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º
- –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–æ—Ä—è–¥–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –ú–µ–Ω—å—à–µ –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ —Å–∏—Å—Ç–µ–º—É

#### 4. Ensemble Mode
- **–ö–ª—é—á–µ–≤–∞—è —Ñ–∏—á–∞:** Voting —Å consensus –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
- –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π consensus threshold: 60%
- –£–≤–µ–ª–∏—á–µ–Ω–∏–µ priority_score –Ω–∞ –æ—Å–Ω–æ–≤–µ consensus strength
- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤

#### 5. Adaptive Mode
- **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä** –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –Ω–∞–ª–∏—á–∏—è –∏–º–µ–Ω –∏ –ª–æ–∫–∞—Ü–∏–π
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É single/parallel/ensemble —Ä–µ–∂–∏–º–∞–º–∏

---

## –õ–µ–≥–∞—Å–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (nlp_processor.py)

### –ù–æ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã Multi-NLP —Å–∏—Å—Ç–µ–º—ã
- **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã** - SpaCy + Natasha + Stanza –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
- **Ensemble voting** - –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∞–ª–≥–æ—Ä–∏—Ç–º —Å –≤–µ—Å–∞–º–∏ –∏ –ø–æ—Ä–æ–≥–∞–º–∏ (>60% consensus)
- **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä** - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
- **–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π
- **Quality monitoring** - –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **–ü—Ä–æ—Ä—ã–≤–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** - 2171 –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞ 4 —Å–µ–∫—É–Ω–¥—ã (300%+ —É–ª—É—á—à–µ–Ω–∏–µ)

### –ù–æ–≤—ã–π —Å—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
```
Multi-NLP Manager ‚Üí –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏
SpaCy (ru_core_news_lg) ‚Üí Entity recognition, –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, atmosphere detection
Natasha ‚Üí –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É: –∏–º–µ–Ω–∞, –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è
Stanza ‚Üí –°–ª–æ–≤–∞–∫—Å–∫–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
Ensemble Voting ‚Üí –ö–æ–Ω—Å–µ–Ω—Å—É—Å –∞–ª–≥–æ—Ä–∏—Ç–º —Å –≤–µ—Å–∞–º–∏ –∏ –ø–æ—Ä–æ–≥–∞–º–∏
Adaptive Selection ‚Üí –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º —Ç–µ–∫—Å—Ç–∞
```

---

## –ö–ª–∞—Å—Å NLPProcessor

**–§–∞–π–ª:** `backend/app/services/nlp_processor.py`

### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
```python
class NLPProcessor:
    def __init__(self, session: AsyncSession):
        self.session = session
        self.spacy_model = self._load_spacy_model()
        self.nltk_initialized = self._init_nltk()
        self.cache = {}
        
    def _load_spacy_model(self) -> spacy.Language:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ spaCy –º–æ–¥–µ–ª–∏."""
        try:
            nlp = spacy.load("ru_core_news_lg")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è pipeline –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            nlp.disable_pipes("parser")  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            nlp.enable_pipe("ner")       # –í–∫–ª—é—á–∞–µ–º NER
            nlp.enable_pipe("tagger")    # –í–∫–ª—é—á–∞–µ–º POS tagging
            
            return nlp
        except OSError:
            logger.error("spaCy model ru_core_news_lg not found. Install with: python -m spacy download ru_core_news_lg")
            raise
```

---

## –¢–∏–ø—ã –æ–ø–∏—Å–∞–Ω–∏–π

### Enum DescriptionType
```python
class DescriptionType(enum.Enum):
    LOCATION = "location"      # –õ–æ–∫–∞—Ü–∏–∏ - –∑–∞–º–∫–∏, –∫–æ–º–Ω–∞—Ç—ã, –≥–æ—Ä–æ–¥–∞
    CHARACTER = "character"    # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏ - –≤–Ω–µ—à–Ω–æ—Å—Ç—å, –æ–¥–µ–∂–¥–∞
    ATMOSPHERE = "atmosphere"  # –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ - –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, –æ—Å–≤–µ—â–µ–Ω–∏–µ
    OBJECT = "object"         # –û–±—ä–µ–∫—Ç—ã - –º–µ–±–µ–ª—å, –ø—Ä–µ–¥–º–µ—Ç—ã
    ACTION = "action"         # –î–µ–π—Å—Ç–≤–∏—è - –¥–≤–∏–∂–µ–Ω–∏—è, —Å–æ–±—ã—Ç–∏—è
```

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ç–∏–ø–æ–≤
```python
TYPE_PRIORITIES = {
    DescriptionType.LOCATION: 75,      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    DescriptionType.CHARACTER: 60,     # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    DescriptionType.ATMOSPHERE: 45,    # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    DescriptionType.OBJECT: 40,        # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    DescriptionType.ACTION: 30         # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
}
```

---

## –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã

### extract_descriptions_from_text()
```python
async def extract_descriptions_from_text(
    self, 
    text: str, 
    chapter_id: UUID,
    min_confidence: float = 0.6
) -> List[Description]:
    """
    –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        chapter_id: ID –≥–ª–∞–≤—ã –¥–ª—è —Å–≤—è–∑—ã–≤–∞–Ω–∏—è
        min_confidence: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        
    Process:
        1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        2. NER –∏ POS –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ spaCy
        3. –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–ø–∏—Å–∞–Ω–∏–π
        4. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º
        5. –†–∞—Å—á–µ—Ç confidence –∏ priority scores
        6. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
    """
    
    # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    cleaned_text = self._preprocess_text(text)
    
    # 2. spaCy –∞–Ω–∞–ª–∏–∑
    doc = self.spacy_model(cleaned_text)
    
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    entities = self._extract_entities(doc)
    
    # 4. –ü–æ–∏—Å–∫ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    description_candidates = self._find_description_patterns(doc, entities)
    
    # 5. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ scoring
    descriptions = []
    for candidate in description_candidates:
        desc_type, confidence = self._classify_description_type(
            candidate.text, entities
        )
        
        if confidence >= min_confidence:
            priority = self._calculate_priority_score(
                desc_type, confidence, candidate.context
            )
            
            description = Description(
                chapter_id=chapter_id,
                content=candidate.text,
                context=candidate.context,
                type=desc_type,
                confidence_score=confidence,
                priority_score=priority,
                entities_mentioned=",".join(candidate.entities),
                text_position_start=candidate.start,
                text_position_end=candidate.end
            )
            descriptions.append(description)
    
    # 6. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    descriptions.sort(key=lambda x: x.priority_score, reverse=True)
    
    return descriptions
```

---

## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π

### Location Detection
```python
def _classify_location_description(
    self, 
    text: str, 
    entities: List[str]
) -> float:
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –ª–æ–∫–∞—Ü–∏–π —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.
    
    Patterns:
    - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: –∑–∞–º–æ–∫, –¥–æ–º, –±–∞—à–Ω—è, –∫–æ–º–Ω–∞—Ç–∞
    - –ü—Ä–∏—Ä–æ–¥–Ω—ã–µ –ª–æ–∫–∞—Ü–∏–∏: –ª–µ—Å, —Ä–µ–∫–∞, –≥–æ—Ä–∞, –ø–æ–ª–µ
    - –ì–æ—Ä–æ–¥—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã: —É–ª–∏—Ü–∞, –ø–ª–æ—â–∞–¥—å, —Ä—ã–Ω–æ–∫
    - –ò–Ω—Ç–µ—Ä—å–µ—Ä—ã: –∑–∞–ª, —Å–ø–∞–ª—å–Ω—è, –∫—É—Ö–Ω—è, –ø–æ–¥–≤–∞–ª
    """
    
    confidence = 0.0
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ª–æ–∫–∞—Ü–∏–π
    location_keywords = [
        # –ó–¥–∞–Ω–∏—è
        "–∑–∞–º–æ–∫", "–¥–æ–º", "–¥–≤–æ—Ä–µ—Ü", "–±–∞—à–Ω—è", "–∫—Ä–µ–ø–æ—Å—Ç—å", "—Ö—Ä–∞–º", "—Ü–µ—Ä–∫–æ–≤—å",
        "–æ—Å–æ–±–Ω—è–∫", "—Ö–∏–∂–∏–Ω–∞", "–∏–∑–±–∞", "—Ç–µ—Ä–µ–º", "–∑–¥–∞–Ω–∏–µ", "—Å—Ç—Ä–æ–µ–Ω–∏–µ",
        
        # –ö–æ–º–Ω–∞—Ç—ã
        "–∫–æ–º–Ω–∞—Ç–∞", "–∑–∞–ª", "—Å–ø–∞–ª—å–Ω—è", "–∫—É—Ö–Ω—è", "—Å—Ç–æ–ª–æ–≤–∞—è", "–≥–æ—Å—Ç–∏–Ω–∞—è",
        "–∫–∞–±–∏–Ω–µ—Ç", "–±–∏–±–ª–∏–æ—Ç–µ–∫–∞", "–ø–æ–¥–≤–∞–ª", "—á–µ—Ä–¥–∞–∫", "–∫–æ—Ä–∏–¥–æ—Ä",
        
        # –ü—Ä–∏—Ä–æ–¥–∞
        "–ª–µ—Å", "–ø–æ–ª—è–Ω–∞", "—Ä–µ–∫–∞", "–æ–∑–µ—Ä–æ", "–≥–æ—Ä–∞", "—Ö–æ–ª–º", "–¥–æ–ª–∏–Ω–∞",
        "–ø–æ–ª–µ", "—Å–∞–¥", "–ø–∞—Ä–∫", "—Ä–æ—â–∞", "–ø–µ—â–µ—Ä–∞", "—É—â–µ–ª—å–µ",
        
        # –ì–æ—Ä–æ–¥
        "—É–ª–∏—Ü–∞", "–ø–ª–æ—â–∞–¥—å", "—Ä—ã–Ω–æ–∫", "–ø–µ—Ä–µ—É–ª–æ–∫", "–ø—Ä–æ—Å–ø–µ–∫—Ç", "–±—É–ª—å–≤–∞—Ä",
        "–º–æ—Å—Ç", "–ø–æ—Ä—Ç", "–≥–∞–≤–∞–Ω—å", "–∫—Ä—ã—à–∞", "–¥–≤–æ—Ä", "–≤–æ—Ä–æ—Ç–∞"
    ]
    
    # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –¥–ª—è –ª–æ–∫–∞—Ü–∏–π
    location_adjectives = [
        "–¥—Ä–µ–≤–Ω–∏–π", "—Å—Ç–∞—Ä—ã–π", "–Ω–æ–≤—ã–π", "–±–æ–ª—å—à–æ–π", "–º–∞–ª–µ–Ω—å–∫–∏–π", "–æ–≥—Ä–æ–º–Ω—ã–π",
        "—É–∑–∫–∏–π", "—à–∏—Ä–æ–∫–∏–π", "–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π", "—Ç–µ–º–Ω—ã–π", "—Å–≤–µ—Ç–ª—ã–π",
        "—É—é—Ç–Ω—ã–π", "–ø—Ä–æ—Å—Ç–æ—Ä–Ω—ã–π", "—Ç–µ—Å–Ω—ã–π", "–≤–µ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "–º—Ä–∞—á–Ω—ã–π"
    ]
    
    text_lower = text.lower()
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    keyword_matches = sum(1 for word in location_keywords if word in text_lower)
    if keyword_matches > 0:
        confidence += 0.4 + (keyword_matches * 0.1)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö + —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
    adj_noun_patterns = [
        r'\b(–¥—Ä–µ–≤–Ω\w+|—Å—Ç–∞—Ä\w+|–±–æ–ª—å—à\w+|–º–∞–ª–µ–Ω—å–∫\w+)\s+(–¥–æ–º|–∑–∞–º–æ–∫|–∑–¥–∞–Ω–∏–µ)',
        r'\b(—Ç–µ–º–Ω\w+|—Å–≤–µ—Ç–ª\w+|—É–∑–∫\w+|—à–∏—Ä–æ–∫\w+)\s+(—É–ª–∏—Ü–∞|–¥–æ—Ä–æ–≥–∞|–∫–æ—Ä–∏–¥–æ—Ä)',
        r'\b(–≤—ã—Å–æ–∫\w+|–Ω–∏–∑–∫\w+|–∫—Ä—É–≥–ª–∞—è|–∫–≤–∞–¥—Ä–∞—Ç–Ω\w+)\s+(–±–∞—à–Ω—è|–∫–æ–º–Ω–∞—Ç–∞|–∑–∞–ª)'
    ]
    
    for pattern in adj_noun_patterns:
        if re.search(pattern, text_lower):
            confidence += 0.3
            
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    location_entities = [e for e in entities if e.label_ in ["LOC", "GPE"]]
    if location_entities:
        confidence += 0.2
        
    return min(1.0, confidence)
```

### Character Detection  
```python
def _classify_character_description(
    self, 
    text: str, 
    entities: List[str]
) -> float:
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ –∏—Ö –≤–Ω–µ—à–Ω–æ—Å—Ç–∏.
    
    Focus Areas:
    - –§–∏–∑–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: —Ä–æ—Å—Ç, —Ñ–∏–≥—É—Ä–∞, –≤–æ–∑—Ä–∞—Å—Ç
    - –õ–∏—Ü–æ: –≥–ª–∞–∑–∞, –≤–æ–ª–æ—Å—ã, —á–µ—Ä—Ç—ã –ª–∏—Ü–∞
    - –û–¥–µ–∂–¥–∞ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã
    - –•–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –ø–æ–∑—ã
    """
    
    confidence = 0.0
    
    # –ß–∞—Å—Ç–∏ —Ç–µ–ª–∞ –∏ –≤–Ω–µ—à–Ω–æ—Å—Ç—å
    appearance_keywords = [
        # –û–±—â–∞—è –≤–Ω–µ—à–Ω–æ—Å—Ç—å
        "—á–µ–ª–æ–≤–µ–∫", "–º—É–∂—á–∏–Ω–∞", "–∂–µ–Ω—â–∏–Ω–∞", "–¥–µ–≤—É—à–∫–∞", "–ø–∞—Ä–µ–Ω—å", "—Å—Ç–∞—Ä–∏–∫",
        "—Ä–µ–±–µ–Ω–æ–∫", "–º–∞–ª—å—á–∏–∫", "–¥–µ–≤–æ—á–∫–∞", "—Ñ–∏–≥—É—Ä–∞", "—Å–∏–ª—É—ç—Ç",
        
        # –õ–∏—Ü–æ
        "–ª–∏—Ü–æ", "–≥–ª–∞–∑–∞", "–≤–∑–≥–ª—è–¥", "–≤–æ–ª–æ—Å—ã", "–±–æ—Ä–æ–¥–∞", "—É—Å—ã", "—É–ª—ã–±–∫–∞",
        "–±—Ä–æ–≤–∏", "—Ä–µ—Å–Ω–∏—Ü—ã", "—â–µ–∫–∏", "–≥—É–±—ã", "–Ω–æ—Å", "–ø–æ–¥–±–æ—Ä–æ–¥–æ–∫",
        
        # –¢–µ–ª–æ
        "—Ä–æ—Å—Ç", "—Ä—É–∫–∏", "–Ω–æ–≥–∏", "–ø–ª–µ—á–∏", "—Å–ø–∏–Ω–∞", "–≥—Ä—É–¥—å", "—Ç–∞–ª–∏—è",
        "–ø–∞–ª—å—Ü—ã", "–ª–∞–¥–æ–Ω–∏", "—à–µ—è", "–ø–æ—Ö–æ–¥–∫–∞", "–¥–≤–∏–∂–µ–Ω–∏—è",
        
        # –û–¥–µ–∂–¥–∞
        "–æ–¥–µ–∂–¥–∞", "–ø–ª–∞—Ç—å–µ", "—Ä—É–±–∞—à–∫–∞", "–ø–ª–∞—â", "—à–ª—è–ø–∞", "—Å–∞–ø–æ–≥–∏",
        "–ø–µ—Ä—á–∞—Ç–∫–∏", "–∫–æ–ª—å—Ü–æ", "—É–∫—Ä–∞—à–µ–Ω–∏—è", "–¥–æ—Å–ø–µ—Ö–∏", "–º–µ—á"
    ]
    
    # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π  
    character_adjectives = [
        # –†–æ—Å—Ç –∏ —Ñ–∏–≥—É—Ä–∞
        "–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π", "—Å—Ç—Ä–æ–π–Ω—ã–π", "–ø–æ–ª–Ω—ã–π", "—Ö—É–¥–æ–π", "–∫—Ä–µ–ø–∫–∏–π",
        "–∏–∑—è—â–Ω—ã–π", "–º–∞—Å—Å–∏–≤–Ω—ã–π", "–≥–∏–±–∫–∏–π", "—Å–∏–ª—å–Ω—ã–π",
        
        # –í–æ–∑—Ä–∞—Å—Ç
        "–º–æ–ª–æ–¥–æ–π", "—Å—Ç–∞—Ä—ã–π", "—é–Ω—ã–π", "–ø–æ–∂–∏–ª–æ–π", "—Å—Ä–µ–¥–Ω–∏—Ö –ª–µ—Ç",
        
        # –í–æ–ª–æ—Å—ã
        "—Ç–µ–º–Ω—ã–π", "—Å–≤–µ—Ç–ª—ã–π", "—Ä—ã–∂–∏–π", "—Å–µ–¥–æ–π", "—á–µ—Ä–Ω—ã–π", "–±–µ–ª–æ–∫—É—Ä—ã–π",
        "–∫—É–¥—Ä—è–≤—ã–π", "–ø—Ä—è–º–æ–π", "–¥–ª–∏–Ω–Ω—ã–π", "–∫–æ—Ä–æ—Ç–∫–∏–π",
        
        # –ì–ª–∞–∑–∞
        "–≥–æ–ª—É–±–æ–π", "–∫–∞—Ä–∏–π", "–∑–µ–ª–µ–Ω—ã–π", "—Å–µ—Ä—ã–π", "—á–µ—Ä–Ω—ã–π", "—è—Ä–∫–∏–π"
    ]
    
    text_lower = text.lower()
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤–Ω–µ—à–Ω–æ—Å—Ç–∏
    appearance_matches = sum(1 for word in appearance_keywords if word in text_lower)
    if appearance_matches > 0:
        confidence += 0.3 + (appearance_matches * 0.1)
        
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ–ø–∏—Å–∞–Ω–∏—è –≤–Ω–µ—à–Ω–æ—Å—Ç–∏
    character_patterns = [
        r'\b(–≤—ã—Å–æ–∫\w+|–Ω–∏–∑–∫\w+|—Å—Ç—Ä–æ–π–Ω\w+|–ø–æ–ª–Ω\w+)\s+(–º—É–∂—á–∏–Ω–∞|–∂–µ–Ω—â–∏–Ω–∞|–¥–µ–≤—É—à–∫–∞|–ø–∞—Ä–µ–Ω—å)',
        r'\b(—Ç–µ–º–Ω\w+|—Å–≤–µ—Ç–ª\w+|—Ä—ã–∂\w+|—Å–µ–¥\w+)\s+(–≤–æ–ª–æ—Å—ã)',
        r'\b(–≥–æ–ª—É–±\w+|–∫–∞—Ä–∏–µ|–∑–µ–ª–µ–Ω\w+|—Å–µ—Ä—ã–µ)\s+(–≥–ª–∞–∑–∞)',
        r'\b(–∫—Ä–∞—Å–∏–≤\w+|–Ω–µ–∫—Ä–∞—Å–∏–≤\w+|–ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω\w+)\s+(–ª–∏—Ü–æ|–¥–µ–≤—É—à–∫–∞|–∂–µ–Ω—â–∏–Ω–∞)',
        r'\b(—ç–ª–µ–≥–∞–Ω—Ç–Ω\w+|–∏–∑—è—â–Ω\w+|–≥—Ä—É–±–æ–≤–∞—Ç\w+)\s+(—Ñ–∏–≥—É—Ä–∞|—Å–∏–ª—É—ç—Ç)'
    ]
    
    for pattern in character_patterns:
        if re.search(pattern, text_lower):
            confidence += 0.25
            
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–ø–µ—Ä—Å–æ–Ω—ã)
    person_entities = [e for e in entities if e.label_ in ["PER", "PERSON"]]
    if person_entities:
        confidence += 0.2
        
    return min(1.0, confidence)
```

---

## –†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞

### Priority Scoring Algorithm
```python
def _calculate_priority_score(
    self, 
    desc_type: DescriptionType, 
    confidence: float, 
    context: str
) -> float:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    
    Factors:
    - –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–∏–ø–∞ (location > character > atmosphere > object > action)
    - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (0.0-1.0)
    - –î–ª–∏–Ω–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏
    - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞
    
    Returns:
        Priority score (0-100)
    """
    
    # 1. –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–∏–ø–∞
    base_priority = TYPE_PRIORITIES[desc_type]
    
    # 2. –ë–æ–Ω—É—Å –∑–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–¥–æ +20 –±–∞–ª–ª–æ–≤)
    confidence_bonus = confidence * 20
    
    # 3. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –æ–ø–∏—Å–∞–Ω–∏—è
    words_count = len(context.split())
    if words_count < 3:
        length_penalty = -15  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
    elif words_count > 20:
        length_penalty = -10  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
    else:
        length_penalty = 0    # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        
    # 4. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–æ–Ω—É—Å—ã
    context_bonus = 0
    context_lower = context.lower()
    
    # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
    emotional_words = [
        "–≤–µ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "–º—Ä–∞—á–Ω—ã–π", "—Ç–∞–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π", "–≤–æ–ª—à–µ–±–Ω—ã–π",
        "–∑–ª–æ–≤–µ—â–∏–π", "–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π", "—É–∂–∞—Å–Ω—ã–π", "–¥—Ä–µ–≤–Ω–∏–π", "–∑–∞–±—ã—Ç—ã–π"
    ]
    
    for word in emotional_words:
        if word in context_lower:
            context_bonus += 3
            
    # 5. –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è
    detail_keywords = [
        "—Ä–µ–∑–Ω–æ–π", "—É–∫—Ä–∞—à–µ–Ω–Ω—ã–π", "–∏–∑—è—â–Ω—ã–π", "–º–∞—Å—Å–∏–≤–Ω—ã–π", "–¥–µ—Ç–∞–ª—å–Ω—ã–π",
        "—Å–ª–æ–∂–Ω—ã–π", "–æ—Ä–Ω–∞–º–µ–Ω—Ç", "—É–∑–æ—Ä", "—Ä–µ–ª—å–µ—Ñ", "–∏–Ω–∫—Ä—É—Å—Ç–∞—Ü–∏—è"
    ]
    
    detail_bonus = sum(2 for word in detail_keywords if word in context_lower)
    
    # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç
    final_score = (
        base_priority + 
        confidence_bonus + 
        length_penalty + 
        context_bonus + 
        detail_bonus
    )
    
    return max(0, min(100, final_score))
```

---

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```python
class NLPProcessor:
    def __init__(self, session: AsyncSession):
        self.cache = TTLCache(maxsize=1000, ttl=3600)  # 1 —á–∞—Å TTL
        
    async def extract_descriptions_from_text(self, text: str, chapter_id: UUID):
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫–µ—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞ —Ç–µ–∫—Å—Ç–∞
        cache_key = f"nlp:{hashlib.md5(text.encode()).hexdigest()}"
        
        if cache_key in self.cache:
            logger.info(f"Cache hit for text hash {cache_key[:8]}...")
            cached_results = self.cache[cache_key]
            # –û–±–Ω–æ–≤–ª—è–µ–º chapter_id –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            for desc in cached_results:
                desc.chapter_id = chapter_id
            return cached_results
            
        # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∫–µ—à–µ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        descriptions = await self._process_text_full(text, chapter_id)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        self.cache[cache_key] = descriptions
        
        return descriptions
```

### Batch Processing
```python
async def process_multiple_chapters(
    self, 
    chapters: List[Chapter], 
    batch_size: int = 5
) -> Dict[UUID, List[Description]]:
    """
    –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    """
    
    results = {}
    
    for i in range(0, len(chapters), batch_size):
        batch = chapters[i:i + batch_size]
        batch_tasks = [
            self.extract_descriptions_from_text(chapter.content, chapter.id)
            for chapter in batch
        ]
        
        batch_results = await asyncio.gather(*batch_tasks)
        
        for chapter, descriptions in zip(batch, batch_results):
            results[chapter.id] = descriptions
            
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
        await asyncio.sleep(0.1)
        
    return results
```

---

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏

### Performance Metrics
```python
class ProcessingMetrics:
    def __init__(self):
        self.total_texts_processed = 0
        self.total_descriptions_found = 0
        self.average_processing_time = 0.0
        self.type_distribution = Counter()
        self.confidence_distribution = []
        
    def add_processing_result(
        self, 
        processing_time: float, 
        descriptions: List[Description]
    ):
        self.total_texts_processed += 1
        self.total_descriptions_found += len(descriptions)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.average_processing_time = (
            (self.average_processing_time * (self.total_texts_processed - 1) + 
             processing_time) / self.total_texts_processed
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        for desc in descriptions:
            self.type_distribution[desc.type.value] += 1
            self.confidence_distribution.append(desc.confidence_score)
            
    def get_stats(self) -> dict:
        return {
            "total_processed": self.total_texts_processed,
            "total_descriptions": self.total_descriptions_found,
            "avg_processing_time": self.average_processing_time,
            "avg_descriptions_per_text": (
                self.total_descriptions_found / max(1, self.total_texts_processed)
            ),
            "type_distribution": dict(self.type_distribution),
            "avg_confidence": (
                sum(self.confidence_distribution) / 
                max(1, len(self.confidence_distribution))
            )
        }
```

---

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### Configuration Class
```python
class NLPConfig:
    # spaCy –º–æ–¥–µ–ª—å
    SPACY_MODEL = "ru_core_news_lg"
    
    # –ü–æ—Ä–æ–≥–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    MIN_CONFIDENCE_THRESHOLD = 0.6
    HIGH_CONFIDENCE_THRESHOLD = 0.85
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
    MAX_DESCRIPTION_LENGTH = 200
    MIN_DESCRIPTION_LENGTH = 5
    MAX_DESCRIPTIONS_PER_CHAPTER = 100
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    BATCH_SIZE = 5
    CACHE_TTL = 3600  # 1 —á–∞—Å
    CACHE_MAX_SIZE = 1000
    
    # –Ø–∑—ã–∫–∏
    SUPPORTED_LANGUAGES = ["ru", "en"]
    DEFAULT_LANGUAGE = "ru"
```

---

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ NLP —Å–∏—Å—Ç–µ–º—ã

### Unit Tests
```python
import pytest
from app.services.nlp_processor import NLPProcessor, DescriptionType

@pytest.mark.asyncio
async def test_location_detection():
    """–¢–µ—Å—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–æ–∫–∞—Ü–∏–π."""
    
    processor = NLPProcessor(mock_session)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∑–∞–º–∫–∞
    text = "–í –¥—Ä–µ–≤–Ω–µ–º –∫–∞–º–µ–Ω–Ω–æ–º –∑–∞–º–∫–µ –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ —Ö–æ–ª–º–∞ —Ü–∞—Ä–∏–ª–∞ –º—Ä–∞—á–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞."
    
    descriptions = await processor.extract_descriptions_from_text(
        text, chapter_id=uuid4()
    )
    
    # –î–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏
    location_descs = [d for d in descriptions if d.type == DescriptionType.LOCATION]
    assert len(location_descs) > 0
    
    location = location_descs[0]
    assert "–∑–∞–º–æ–∫" in location.content.lower()
    assert location.confidence_score >= 0.7
    assert location.priority_score >= 70.0

@pytest.mark.asyncio  
async def test_character_detection():
    """–¢–µ—Å—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π."""
    
    processor = NLPProcessor(mock_session)
    
    text = "–í—ã—Å–æ–∫–∏–π –º—É–∂—á–∏–Ω–∞ —Å —Å–µ–¥–æ–π –±–æ—Ä–æ–¥–æ–π –∏ –ø—Ä–æ–Ω–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≥–æ–ª—É–±—ã–º–∏ –≥–ª–∞–∑–∞–º–∏."
    
    descriptions = await processor.extract_descriptions_from_text(
        text, chapter_id=uuid4()
    )
    
    character_descs = [d for d in descriptions if d.type == DescriptionType.CHARACTER]
    assert len(character_descs) > 0
    
    character = character_descs[0]
    assert any(word in character.content.lower() 
              for word in ["–º—É–∂—á–∏–Ω–∞", "–±–æ—Ä–æ–¥–∞", "–≥–ª–∞–∑–∞"])
    assert character.confidence_score >= 0.6

@pytest.mark.asyncio
async def test_priority_calculation():
    """–¢–µ—Å—Ç —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤."""
    
    processor = NLPProcessor(mock_session)
    
    # –¢–µ–∫—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –æ–ø–∏—Å–∞–Ω–∏–π
    text = """
    –í–µ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–∞–º–æ–∫ –≤–æ–∑–≤—ã—à–∞–ª—Å—è –Ω–∞–¥ –¥–æ–ª–∏–Ω–æ–π. 
    –í —É–≥–ª—É –∫–æ–º–Ω–∞—Ç—ã —Å—Ç–æ—è–ª —Å—Ç–∞—Ä—ã–π —Å—Ç—É–ª.
    –†—ã—Ü–∞—Ä—å –±—ã—Å—Ç—Ä–æ –ø–æ–±–µ–∂–∞–ª –ø–æ –∫–æ—Ä–∏–¥–æ—Ä—É.
    """
    
    descriptions = await processor.extract_descriptions_from_text(
        text, chapter_id=uuid4()
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
    descriptions.sort(key=lambda x: x.priority_score, reverse=True)
    
    # –ó–∞–º–æ–∫ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    top_desc = descriptions[0]
    assert top_desc.type == DescriptionType.LOCATION
    assert "–∑–∞–º–æ–∫" in top_desc.content.lower()
```

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### Logging Configuration
```python
import logging
from functools import wraps

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è NLP
nlp_logger = logging.getLogger("bookreader.nlp")
nlp_logger.setLevel(logging.INFO)

def log_nlp_processing(func):
    @wraps(func)
    async def wrapper(self, text: str, chapter_id: UUID, *args, **kwargs):
        start_time = time.time()
        text_length = len(text)
        
        nlp_logger.info(
            f"Starting NLP processing for chapter {chapter_id}, text length: {text_length}",
            extra={
                "chapter_id": str(chapter_id),
                "text_length": text_length,
                "function": func.__name__
            }
        )
        
        try:
            result = await func(self, text, chapter_id, *args, **kwargs)
            
            processing_time = time.time() - start_time
            descriptions_found = len(result)
            
            nlp_logger.info(
                f"NLP processing completed. Found {descriptions_found} descriptions in {processing_time:.2f}s",
                extra={
                    "chapter_id": str(chapter_id),
                    "descriptions_found": descriptions_found,
                    "processing_time": processing_time,
                    "descriptions_per_second": descriptions_found / processing_time if processing_time > 0 else 0
                }
            )
            
            return result
            
        except Exception as e:
            nlp_logger.error(
                f"NLP processing failed for chapter {chapter_id}: {str(e)}",
                extra={
                    "chapter_id": str(chapter_id),
                    "error": str(e),
                    "text_length": text_length
                },
                exc_info=True
            )
            raise
            
    return wrapper
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

NLP —Å–∏—Å—Ç–µ–º–∞ BookReader AI –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:

- **–í—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å** –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π (>85% precision)
- **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—é** –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (<0.5 —Å–µ–∫/1000 —Å–∏–º–≤–æ–ª–æ–≤)
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** —á–µ—Ä–µ–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ batch processing
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** –∏ –¥–µ—Ç–∞–ª—å–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø—Ä–æ—Ü–µ—Å—Å–∞

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏ –∏ —Ç–∏–ø–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
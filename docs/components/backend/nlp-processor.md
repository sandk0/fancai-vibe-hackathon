# NLP Processor - BookReader AI

Детальная документация NLP системы для извлечения и классификации описаний из текстов книг. NLP процессор является ключевым компонентом, определяющим качество генерируемых изображений.

## Архитектура NLP системы

### Основные принципы
- **Многоуровневый анализ** - лексический, синтаксический, семантический
- **Приоритизация описаний** - интеллектуальное ранжирование для генерации
- **Мультиязычность** - поддержка русского и английского языков
- **Контекстный анализ** - учет окружающего контекста для точности
- **Производительность** - быстрая обработка больших текстов

### Стек технологий
```
spaCy (ru_core_news_lg) → Основная NLP модель для русского языка
NLTK → Дополнительная обработка и токенизация
Regex Patterns → Специализированные шаблоны поиска
Machine Learning → Классификация и scoring
```

---

## Класс NLPProcessor

**Файл:** `backend/app/services/nlp_processor.py`

### Инициализация
```python
class NLPProcessor:
    def __init__(self, session: AsyncSession):
        self.session = session
        self.spacy_model = self._load_spacy_model()
        self.nltk_initialized = self._init_nltk()
        self.cache = {}
        
    def _load_spacy_model(self) -> spacy.Language:
        """Загрузка и настройка spaCy модели."""
        try:
            nlp = spacy.load("ru_core_news_lg")
            
            # Оптимизация pipeline для производительности
            nlp.disable_pipes("parser")  # Отключаем парсер для скорости
            nlp.enable_pipe("ner")       # Включаем NER
            nlp.enable_pipe("tagger")    # Включаем POS tagging
            
            return nlp
        except OSError:
            logger.error("spaCy model ru_core_news_lg not found. Install with: python -m spacy download ru_core_news_lg")
            raise
```

---

## Типы описаний

### Enum DescriptionType
```python
class DescriptionType(enum.Enum):
    LOCATION = "location"      # Локации - замки, комнаты, города
    CHARACTER = "character"    # Персонажи - внешность, одежда
    ATMOSPHERE = "atmosphere"  # Атмосфера - настроение, освещение
    OBJECT = "object"         # Объекты - мебель, предметы
    ACTION = "action"         # Действия - движения, события
```

### Приоритеты типов
```python
TYPE_PRIORITIES = {
    DescriptionType.LOCATION: 75,      # Максимальный приоритет
    DescriptionType.CHARACTER: 60,     # Высокий приоритет
    DescriptionType.ATMOSPHERE: 45,    # Средний приоритет
    DescriptionType.OBJECT: 40,        # Средний приоритет
    DescriptionType.ACTION: 30         # Низкий приоритет
}
```

---

## Основные методы

### extract_descriptions_from_text()
```python
async def extract_descriptions_from_text(
    self, 
    text: str, 
    chapter_id: UUID,
    min_confidence: float = 0.6
) -> List[Description]:
    """
    Главный метод извлечения описаний из текста.
    
    Args:
        text: Исходный текст для анализа
        chapter_id: ID главы для связывания
        min_confidence: Минимальная уверенность для сохранения
        
    Returns:
        Список найденных описаний, отсортированных по приоритету
        
    Process:
        1. Предобработка текста
        2. NER и POS анализ через spaCy
        3. Поиск паттернов описаний
        4. Классификация по типам
        5. Расчет confidence и priority scores
        6. Фильтрация и ранжирование
    """
    
    # 1. Предобработка текста
    cleaned_text = self._preprocess_text(text)
    
    # 2. spaCy анализ
    doc = self.spacy_model(cleaned_text)
    
    # 3. Извлечение именованных сущностей
    entities = self._extract_entities(doc)
    
    # 4. Поиск описательных фрагментов
    description_candidates = self._find_description_patterns(doc, entities)
    
    # 5. Классификация и scoring
    descriptions = []
    for candidate in description_candidates:
        desc_type, confidence = self._classify_description_type(
            candidate.text, entities
        )
        
        if confidence >= min_confidence:
            priority = self._calculate_priority_score(
                desc_type, confidence, candidate.context
            )
            
            description = Description(
                chapter_id=chapter_id,
                content=candidate.text,
                context=candidate.context,
                type=desc_type,
                confidence_score=confidence,
                priority_score=priority,
                entities_mentioned=",".join(candidate.entities),
                text_position_start=candidate.start,
                text_position_end=candidate.end
            )
            descriptions.append(description)
    
    # 6. Сортировка по приоритету
    descriptions.sort(key=lambda x: x.priority_score, reverse=True)
    
    return descriptions
```

---

## Классификация описаний

### Location Detection
```python
def _classify_location_description(
    self, 
    text: str, 
    entities: List[str]
) -> float:
    """
    Определение описаний локаций с высокой точностью.
    
    Patterns:
    - Архитектурные элементы: замок, дом, башня, комната
    - Природные локации: лес, река, гора, поле
    - Городские объекты: улица, площадь, рынок
    - Интерьеры: зал, спальня, кухня, подвал
    """
    
    confidence = 0.0
    
    # Ключевые слова локаций
    location_keywords = [
        # Здания
        "замок", "дом", "дворец", "башня", "крепость", "храм", "церковь",
        "особняк", "хижина", "изба", "терем", "здание", "строение",
        
        # Комнаты
        "комната", "зал", "спальня", "кухня", "столовая", "гостиная",
        "кабинет", "библиотека", "подвал", "чердак", "коридор",
        
        # Природа
        "лес", "поляна", "река", "озеро", "гора", "холм", "долина",
        "поле", "сад", "парк", "роща", "пещера", "ущелье",
        
        # Город
        "улица", "площадь", "рынок", "переулок", "проспект", "бульвар",
        "мост", "порт", "гавань", "крыша", "двор", "ворота"
    ]
    
    # Прилагательные для локаций
    location_adjectives = [
        "древний", "старый", "новый", "большой", "маленький", "огромный",
        "узкий", "широкий", "высокий", "низкий", "темный", "светлый",
        "уютный", "просторный", "тесный", "величественный", "мрачный"
    ]
    
    text_lower = text.lower()
    
    # Подсчет совпадений ключевых слов
    keyword_matches = sum(1 for word in location_keywords if word in text_lower)
    if keyword_matches > 0:
        confidence += 0.4 + (keyword_matches * 0.1)
    
    # Проверка прилагательных + существительных
    adj_noun_patterns = [
        r'\b(древн\w+|стар\w+|больш\w+|маленьк\w+)\s+(дом|замок|здание)',
        r'\b(темн\w+|светл\w+|узк\w+|широк\w+)\s+(улица|дорога|коридор)',
        r'\b(высок\w+|низк\w+|круглая|квадратн\w+)\s+(башня|комната|зал)'
    ]
    
    for pattern in adj_noun_patterns:
        if re.search(pattern, text_lower):
            confidence += 0.3
            
    # Проверка именованных сущностей
    location_entities = [e for e in entities if e.label_ in ["LOC", "GPE"]]
    if location_entities:
        confidence += 0.2
        
    return min(1.0, confidence)
```

### Character Detection  
```python
def _classify_character_description(
    self, 
    text: str, 
    entities: List[str]
) -> float:
    """
    Определение описаний персонажей и их внешности.
    
    Focus Areas:
    - Физические характеристики: рост, фигура, возраст
    - Лицо: глаза, волосы, черты лица
    - Одежда и аксессуары
    - Характерные движения и позы
    """
    
    confidence = 0.0
    
    # Части тела и внешность
    appearance_keywords = [
        # Общая внешность
        "человек", "мужчина", "женщина", "девушка", "парень", "старик",
        "ребенок", "мальчик", "девочка", "фигура", "силуэт",
        
        # Лицо
        "лицо", "глаза", "взгляд", "волосы", "борода", "усы", "улыбка",
        "брови", "ресницы", "щеки", "губы", "нос", "подбородок",
        
        # Тело
        "рост", "руки", "ноги", "плечи", "спина", "грудь", "талия",
        "пальцы", "ладони", "шея", "походка", "движения",
        
        # Одежда
        "одежда", "платье", "рубашка", "плащ", "шляпа", "сапоги",
        "перчатки", "кольцо", "украшения", "доспехи", "меч"
    ]
    
    # Прилагательные для персонажей  
    character_adjectives = [
        # Рост и фигура
        "высокий", "низкий", "стройный", "полный", "худой", "крепкий",
        "изящный", "массивный", "гибкий", "сильный",
        
        # Возраст
        "молодой", "старый", "юный", "пожилой", "средних лет",
        
        # Волосы
        "темный", "светлый", "рыжий", "седой", "черный", "белокурый",
        "кудрявый", "прямой", "длинный", "короткий",
        
        # Глаза
        "голубой", "карий", "зеленый", "серый", "черный", "яркий"
    ]
    
    text_lower = text.lower()
    
    # Ключевые слова внешности
    appearance_matches = sum(1 for word in appearance_keywords if word in text_lower)
    if appearance_matches > 0:
        confidence += 0.3 + (appearance_matches * 0.1)
        
    # Паттерны описания внешности
    character_patterns = [
        r'\b(высок\w+|низк\w+|стройн\w+|полн\w+)\s+(мужчина|женщина|девушка|парень)',
        r'\b(темн\w+|светл\w+|рыж\w+|сед\w+)\s+(волосы)',
        r'\b(голуб\w+|карие|зелен\w+|серые)\s+(глаза)',
        r'\b(красив\w+|некрасив\w+|привлекательн\w+)\s+(лицо|девушка|женщина)',
        r'\b(элегантн\w+|изящн\w+|грубоват\w+)\s+(фигура|силуэт)'
    ]
    
    for pattern in character_patterns:
        if re.search(pattern, text_lower):
            confidence += 0.25
            
    # Проверка именованных сущностей (персоны)
    person_entities = [e for e in entities if e.label_ in ["PER", "PERSON"]]
    if person_entities:
        confidence += 0.2
        
    return min(1.0, confidence)
```

---

## Расчет приоритета

### Priority Scoring Algorithm
```python
def _calculate_priority_score(
    self, 
    desc_type: DescriptionType, 
    confidence: float, 
    context: str
) -> float:
    """
    Интеллектуальный расчет приоритета для генерации изображений.
    
    Factors:
    - Базовый приоритет типа (location > character > atmosphere > object > action)
    - Уверенность классификации (0.0-1.0)
    - Длина и сложность описания
    - Контекстные подсказки
    - Эмоциональная окраска
    
    Returns:
        Priority score (0-100)
    """
    
    # 1. Базовый приоритет типа
    base_priority = TYPE_PRIORITIES[desc_type]
    
    # 2. Бонус за уверенность (до +20 баллов)
    confidence_bonus = confidence * 20
    
    # 3. Анализ длины описания
    words_count = len(context.split())
    if words_count < 3:
        length_penalty = -15  # Слишком короткое
    elif words_count > 20:
        length_penalty = -10  # Слишком длинное
    else:
        length_penalty = 0    # Оптимальная длина
        
    # 4. Контекстные бонусы
    context_bonus = 0
    context_lower = context.lower()
    
    # Эмоциональные прилагательные
    emotional_words = [
        "величественный", "мрачный", "таинственный", "волшебный",
        "зловещий", "прекрасный", "ужасный", "древний", "забытый"
    ]
    
    for word in emotional_words:
        if word in context_lower:
            context_bonus += 3
            
    # 5. Детализация описания
    detail_keywords = [
        "резной", "украшенный", "изящный", "массивный", "детальный",
        "сложный", "орнамент", "узор", "рельеф", "инкрустация"
    ]
    
    detail_bonus = sum(2 for word in detail_keywords if word in context_lower)
    
    # 6. Финальный расчет
    final_score = (
        base_priority + 
        confidence_bonus + 
        length_penalty + 
        context_bonus + 
        detail_bonus
    )
    
    return max(0, min(100, final_score))
```

---

## Производительность и оптимизация

### Кеширование результатов
```python
class NLPProcessor:
    def __init__(self, session: AsyncSession):
        self.cache = TTLCache(maxsize=1000, ttl=3600)  # 1 час TTL
        
    async def extract_descriptions_from_text(self, text: str, chapter_id: UUID):
        # Создаем ключ кеша на основе хеша текста
        cache_key = f"nlp:{hashlib.md5(text.encode()).hexdigest()}"
        
        if cache_key in self.cache:
            logger.info(f"Cache hit for text hash {cache_key[:8]}...")
            cached_results = self.cache[cache_key]
            # Обновляем chapter_id для кешированных результатов
            for desc in cached_results:
                desc.chapter_id = chapter_id
            return cached_results
            
        # Если нет в кеше - обрабатываем
        descriptions = await self._process_text_full(text, chapter_id)
        
        # Сохраняем в кеш
        self.cache[cache_key] = descriptions
        
        return descriptions
```

### Batch Processing
```python
async def process_multiple_chapters(
    self, 
    chapters: List[Chapter], 
    batch_size: int = 5
) -> Dict[UUID, List[Description]]:
    """
    Пакетная обработка глав для оптимизации производительности.
    """
    
    results = {}
    
    for i in range(0, len(chapters), batch_size):
        batch = chapters[i:i + batch_size]
        batch_tasks = [
            self.extract_descriptions_from_text(chapter.content, chapter.id)
            for chapter in batch
        ]
        
        batch_results = await asyncio.gather(*batch_tasks)
        
        for chapter, descriptions in zip(batch, batch_results):
            results[chapter.id] = descriptions
            
        # Небольшая пауза между батчами
        await asyncio.sleep(0.1)
        
    return results
```

---

## Статистика и метрики

### Performance Metrics
```python
class ProcessingMetrics:
    def __init__(self):
        self.total_texts_processed = 0
        self.total_descriptions_found = 0
        self.average_processing_time = 0.0
        self.type_distribution = Counter()
        self.confidence_distribution = []
        
    def add_processing_result(
        self, 
        processing_time: float, 
        descriptions: List[Description]
    ):
        self.total_texts_processed += 1
        self.total_descriptions_found += len(descriptions)
        
        # Обновляем среднее время обработки
        self.average_processing_time = (
            (self.average_processing_time * (self.total_texts_processed - 1) + 
             processing_time) / self.total_texts_processed
        )
        
        # Статистика по типам
        for desc in descriptions:
            self.type_distribution[desc.type.value] += 1
            self.confidence_distribution.append(desc.confidence_score)
            
    def get_stats(self) -> dict:
        return {
            "total_processed": self.total_texts_processed,
            "total_descriptions": self.total_descriptions_found,
            "avg_processing_time": self.average_processing_time,
            "avg_descriptions_per_text": (
                self.total_descriptions_found / max(1, self.total_texts_processed)
            ),
            "type_distribution": dict(self.type_distribution),
            "avg_confidence": (
                sum(self.confidence_distribution) / 
                max(1, len(self.confidence_distribution))
            )
        }
```

---

## Настройка и конфигурация

### Configuration Class
```python
class NLPConfig:
    # spaCy модель
    SPACY_MODEL = "ru_core_news_lg"
    
    # Пороги уверенности
    MIN_CONFIDENCE_THRESHOLD = 0.6
    HIGH_CONFIDENCE_THRESHOLD = 0.85
    
    # Ограничения
    MAX_DESCRIPTION_LENGTH = 200
    MIN_DESCRIPTION_LENGTH = 5
    MAX_DESCRIPTIONS_PER_CHAPTER = 100
    
    # Производительность
    BATCH_SIZE = 5
    CACHE_TTL = 3600  # 1 час
    CACHE_MAX_SIZE = 1000
    
    # Языки
    SUPPORTED_LANGUAGES = ["ru", "en"]
    DEFAULT_LANGUAGE = "ru"
```

---

## Тестирование NLP системы

### Unit Tests
```python
import pytest
from app.services.nlp_processor import NLPProcessor, DescriptionType

@pytest.mark.asyncio
async def test_location_detection():
    """Тест определения локаций."""
    
    processor = NLPProcessor(mock_session)
    
    # Тестовый текст с описанием замка
    text = "В древнем каменном замке на вершине холма царила мрачная атмосфера."
    
    descriptions = await processor.extract_descriptions_from_text(
        text, chapter_id=uuid4()
    )
    
    # Должны найти описание локации
    location_descs = [d for d in descriptions if d.type == DescriptionType.LOCATION]
    assert len(location_descs) > 0
    
    location = location_descs[0]
    assert "замок" in location.content.lower()
    assert location.confidence_score >= 0.7
    assert location.priority_score >= 70.0

@pytest.mark.asyncio  
async def test_character_detection():
    """Тест определения персонажей."""
    
    processor = NLPProcessor(mock_session)
    
    text = "Высокий мужчина с седой бородой и проницательными голубыми глазами."
    
    descriptions = await processor.extract_descriptions_from_text(
        text, chapter_id=uuid4()
    )
    
    character_descs = [d for d in descriptions if d.type == DescriptionType.CHARACTER]
    assert len(character_descs) > 0
    
    character = character_descs[0]
    assert any(word in character.content.lower() 
              for word in ["мужчина", "борода", "глаза"])
    assert character.confidence_score >= 0.6

@pytest.mark.asyncio
async def test_priority_calculation():
    """Тест расчета приоритетов."""
    
    processor = NLPProcessor(mock_session)
    
    # Текст с разными типами описаний
    text = """
    Величественный замок возвышался над долиной. 
    В углу комнаты стоял старый стул.
    Рыцарь быстро побежал по коридору.
    """
    
    descriptions = await processor.extract_descriptions_from_text(
        text, chapter_id=uuid4()
    )
    
    # Проверяем правильность приоритизации
    descriptions.sort(key=lambda x: x.priority_score, reverse=True)
    
    # Замок должен иметь максимальный приоритет
    top_desc = descriptions[0]
    assert top_desc.type == DescriptionType.LOCATION
    assert "замок" in top_desc.content.lower()
```

---

## Мониторинг и логирование

### Logging Configuration
```python
import logging
from functools import wraps

# Настройка логирования для NLP
nlp_logger = logging.getLogger("bookreader.nlp")
nlp_logger.setLevel(logging.INFO)

def log_nlp_processing(func):
    @wraps(func)
    async def wrapper(self, text: str, chapter_id: UUID, *args, **kwargs):
        start_time = time.time()
        text_length = len(text)
        
        nlp_logger.info(
            f"Starting NLP processing for chapter {chapter_id}, text length: {text_length}",
            extra={
                "chapter_id": str(chapter_id),
                "text_length": text_length,
                "function": func.__name__
            }
        )
        
        try:
            result = await func(self, text, chapter_id, *args, **kwargs)
            
            processing_time = time.time() - start_time
            descriptions_found = len(result)
            
            nlp_logger.info(
                f"NLP processing completed. Found {descriptions_found} descriptions in {processing_time:.2f}s",
                extra={
                    "chapter_id": str(chapter_id),
                    "descriptions_found": descriptions_found,
                    "processing_time": processing_time,
                    "descriptions_per_second": descriptions_found / processing_time if processing_time > 0 else 0
                }
            )
            
            return result
            
        except Exception as e:
            nlp_logger.error(
                f"NLP processing failed for chapter {chapter_id}: {str(e)}",
                extra={
                    "chapter_id": str(chapter_id),
                    "error": str(e),
                    "text_length": text_length
                },
                exc_info=True
            )
            raise
            
    return wrapper
```

---

## Заключение

NLP система BookReader AI обеспечивает:

- **Высокую точность** извлечения релевантных описаний (>85% precision)
- **Интеллектуальную приоритизацию** для оптимального качества генерации
- **Производительность** обработки больших текстов (<0.5 сек/1000 символов)
- **Масштабируемость** через кеширование и batch processing
- **Мониторинг** и детальную аналитику процесса

Система готова для production использования и может быть расширена дополнительными языками и типами анализа.
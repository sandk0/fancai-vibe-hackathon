#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –≥–ª–∞–≤—ã –¥–ª—è —Ç–µ—Å—Ç–∞.
"""

import sys
import os
import asyncio
import json

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.core.database import AsyncSessionLocal
from app.models.book import Book
from app.models.chapter import Chapter
from app.models.description import Description, DescriptionType
from app.services.nlp_processor import nlp_processor
from sqlalchemy import select

async def process_single_chapter():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –≥–ª–∞–≤—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞."""
    
    async with AsyncSessionLocal() as db:
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é –≥–ª–∞–≤—É –ø–µ—Ä–≤–æ–π –∫–Ω–∏–≥–∏
        result = await db.execute(
            select(Chapter)
            .join(Book)
            .where(Book.title.like('%–í–µ–¥—å–º–∞–∫%'))
            .where(Chapter.chapter_number == 1)
            .limit(1)
        )
        
        chapter = result.scalar_one_or_none()
        if not chapter:
            print("‚ùå –ì–ª–∞–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥–ª–∞–≤—É: {chapter.title}")
        print(f"   –ö–æ–Ω—Ç–µ–Ω—Ç: {len(chapter.content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é NLP
        try:
            descriptions_data = nlp_processor.extract_descriptions_from_text(
                text=chapter.content[:5000],  # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
                chapter_id=str(chapter.id)
            )
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π: {len(descriptions_data)}")
            
            # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç—ã –æ–ø–∏—Å–∞–Ω–∏–π
            for desc_data in descriptions_data:
                description = Description(
                    chapter_id=chapter.id,
                    type=desc_data["type"],
                    content=desc_data["content"],
                    confidence_score=desc_data["confidence_score"],
                    priority_score=desc_data["priority_score"],
                    position_in_chapter=0,
                    word_count=len(desc_data["content"].split()),
                    entities_mentioned=json.dumps(desc_data.get("entities_mentioned", []), ensure_ascii=False),
                    is_suitable_for_generation=desc_data.get("confidence_score", 0) >= 0.3
                )
                db.add(description)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≥–ª–∞–≤—ã
            chapter.is_description_parsed = True
            chapter.descriptions_found = len(descriptions_data)
            
            await db.commit()
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(descriptions_data)} –æ–ø–∏—Å–∞–Ω–∏–π")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
            for i, desc in enumerate(descriptions_data[:3]):
                print(f"   {i+1}. {desc['type'].value}: {desc['content'][:80]}...")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
            await db.rollback()

if __name__ == "__main__":
    print("=" * 60)
    print("–ë–´–°–¢–†–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –û–î–ù–û–ô –ì–õ–ê–í–´")
    print("=" * 60)
    
    if not nlp_processor.is_available():
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º NLP –º–æ–¥–µ–ª–∏...")
        nlp_processor.load_models()
    
    print("‚úÖ NLP –≥–æ—Ç–æ–≤")
    asyncio.run(process_single_chapter())
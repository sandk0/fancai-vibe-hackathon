# Testing Action Plan - Week 1

**–ü–µ—Ä–∏–æ–¥:** 2025-10-30 –¥–æ 2025-11-06 (7 –¥–Ω–µ–π)
**–¶–µ–ª—å:** Fix critical issues + Start P0 tests
**Target:** +70 tests, fix 3 issues

---

## üéØ Week 1 Goals

- [x] ‚úÖ Complete testing audit
- [ ] üîÑ Fix 1 failing test
- [ ] üîÑ Fix 2 collection errors
- [ ] üîÑ Create 30 NLP Strategy tests
- [ ] üîÑ Create 40 EpubReader tests
- [ ] üîÑ Setup coverage gates

**Total:** 3 fixes + 70 tests

---

## üìÖ Day-by-Day Plan

### Day 1 (Monday) - Investigation & Fixes

**Morning (4 hours):**
1. ‚úÖ **Complete Audit Report** (DONE)
   - Generated 3 reports
   - Identified critical gaps
   - Created action plans

2. **Investigate Failing Test** (1 hour)
   ```bash
   # Run failing test with verbose output
   docker-compose exec backend pytest tests/test_book_service.py::TestBookRetrieval::test_get_user_books_pagination -vv

   # Check pagination logic
   # Likely issue: skip/limit calculation or data setup
   ```

3. **Fix Failing Test** (1 hour)
   - Debug pagination logic
   - Fix issue
   - Verify test passes
   - Commit fix

**Afternoon (4 hours):**
4. **Fix Collection Errors** (2 hours)
   ```bash
   # Error 1: integration/test_reading_sessions_flow.py
   # Error 2: performance/test_reading_sessions_load.py

   # Common issues:
   # - Import errors
   # - Missing dependencies
   # - Permission issues
   ```

5. **Investigate NLP Utils Paradox** (2 hours)
   ```bash
   # 130+ tests exist but coverage 10-20%
   # Possible causes:
   # 1. Tests import wrong modules
   # 2. Tests don't run (skipped/excluded)
   # 3. Coverage config issue

   # Check:
   docker-compose exec backend pytest tests/services/nlp/utils/ -v --collect-only
   docker-compose exec backend pytest tests/services/nlp/utils/ --cov=app/services/nlp/utils --cov-report=term-missing
   ```

**Deliverables:**
- ‚úÖ All tests passing (621/621)
- ‚úÖ No collection errors
- üìù Investigation report on NLP utils

---

### Day 2 (Tuesday) - NLP Strategy Tests (Part 1)

**Goal:** Create 3 strategy test files (18 tests)

**File 1: test_single_strategy.py** (6 tests, 2 hours)
```python
# backend/tests/services/nlp/strategies/test_single_strategy.py

import pytest
from app.services.nlp.strategies.single_strategy import SingleStrategy

class TestSingleStrategy:
    """Tests for SingleStrategy - simplest execution mode."""

    @pytest.fixture
    def strategy(self):
        return SingleStrategy()

    @pytest.mark.asyncio
    async def test_process_with_spacy(self, strategy):
        """Test processing with SpaCy processor."""
        text = "–ö—Ä–∞—Å–∏–≤—ã–π –ª–µ—Å —Å –≤—ã—Å–æ–∫–∏–º–∏ –¥–µ—Ä–µ–≤—å—è–º–∏."
        result = await strategy.process(text, processor_name="spacy")

        assert result is not None
        assert len(result) > 0
        assert result[0]["text"]
        assert result[0]["description_type"] in ["location", "character", "atmosphere", "object"]

    @pytest.mark.asyncio
    async def test_process_with_natasha(self, strategy):
        """Test processing with Natasha processor."""
        text = "–ê–Ω–Ω–∞ –ö–∞—Ä–µ–Ω–∏–Ω–∞ —Å—Ç–æ—è–ª–∞ —É –æ–∫–Ω–∞."
        result = await strategy.process(text, processor_name="natasha")

        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_process_with_stanza(self, strategy):
        """Test processing with Stanza processor."""
        text = "–°—Ç–∞—Ä–∏–Ω–Ω—ã–π –∑–∞–º–æ–∫ –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ —Ö–æ–ª–º–∞."
        result = await strategy.process(text, processor_name="stanza")

        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_process_with_invalid_processor(self, strategy):
        """Test error handling for invalid processor."""
        text = "Test text"

        with pytest.raises(ValueError, match="Unknown processor"):
            await strategy.process(text, processor_name="invalid")

    @pytest.mark.asyncio
    async def test_process_empty_text(self, strategy):
        """Test processing empty text."""
        result = await strategy.process("", processor_name="spacy")

        assert result == []

    @pytest.mark.asyncio
    async def test_process_with_config(self, strategy):
        """Test processing with custom config."""
        text = "–¢—ë–º–Ω—ã–π –ª–µ—Å."
        config = {
            "min_confidence": 0.7,
            "max_descriptions": 5
        }
        result = await strategy.process(text, processor_name="spacy", config=config)

        assert len(result) <= 5
        for desc in result:
            assert desc.get("confidence", 1.0) >= 0.7
```

**File 2: test_parallel_strategy.py** (7 tests, 2 hours)
```python
# backend/tests/services/nlp/strategies/test_parallel_strategy.py

import pytest
import asyncio
from app.services.nlp.strategies.parallel_strategy import ParallelStrategy

class TestParallelStrategy:
    """Tests for ParallelStrategy - concurrent execution."""

    @pytest.fixture
    def strategy(self):
        return ParallelStrategy()

    @pytest.mark.asyncio
    async def test_process_all_processors(self, strategy):
        """Test parallel processing with all processors."""
        text = "–í—ã—Å–æ–∫–∞—è –±–∞—à–Ω—è –≤ —Ü–µ–Ω—Ç—Ä–µ –≥–æ—Ä–æ–¥–∞."
        result = await strategy.process(text)

        # Should combine results from all processors
        assert result is not None
        assert len(result) > 0

        # Check that results from multiple processors are merged
        # (each processor should contribute some descriptions)
        assert len(result) >= 1

    @pytest.mark.asyncio
    async def test_process_selected_processors(self, strategy):
        """Test with specific processor list."""
        text = "–ö—Ä–∞—Å–∏–≤—ã–π —Å–∞–¥."
        processors = ["spacy", "natasha"]
        result = await strategy.process(text, processors=processors)

        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_deduplication(self, strategy):
        """Test that duplicate descriptions are removed."""
        text = "–ë–æ–ª—å—à–æ–π –¥–æ–º. –ë–æ–ª—å—à–æ–π –¥–æ–º."
        result = await strategy.process(text)

        # Check no exact duplicates
        texts = [d["text"] for d in result]
        assert len(texts) == len(set(texts))

    @pytest.mark.asyncio
    async def test_concurrent_execution(self, strategy):
        """Test that processors run concurrently (performance)."""
        text = "–¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."

        start = asyncio.get_event_loop().time()
        result = await strategy.process(text)
        duration = asyncio.get_event_loop().time() - start

        # Parallel should be faster than 3x sequential
        # (exact timing depends on hardware)
        assert result is not None
        assert duration < 10  # Reasonable timeout

    @pytest.mark.asyncio
    async def test_error_handling_one_processor_fails(self, strategy, monkeypatch):
        """Test resilience when one processor fails."""
        async def failing_processor(*args, **kwargs):
            raise Exception("Processor failed")

        # Mock one processor to fail
        # Strategy should continue with other processors
        text = "–¢–µ—Å—Ç"
        result = await strategy.process(text)

        # Should still get results from working processors
        assert result is not None

    @pytest.mark.asyncio
    async def test_empty_text(self, strategy):
        """Test with empty input."""
        result = await strategy.process("")
        assert result == []

    @pytest.mark.asyncio
    async def test_merge_priority(self, strategy):
        """Test that higher priority descriptions come first."""
        text = "–°—Ç–∞—Ä—ã–π –∑–∞–º–æ–∫ –∏ –º–æ–ª–æ–¥–∞—è –¥–µ–≤—É—à–∫–∞."
        result = await strategy.process(text)

        if len(result) > 1:
            # Check that results are sorted by priority_score
            scores = [d.get("priority_score", 0) for d in result]
            assert scores == sorted(scores, reverse=True)
```

**File 3: test_sequential_strategy.py** (6 tests, 2 hours)
```python
# backend/tests/services/nlp/strategies/test_sequential_strategy.py

import pytest
from app.services.nlp.strategies.sequential_strategy import SequentialStrategy

class TestSequentialStrategy:
    """Tests for SequentialStrategy - ordered execution."""

    @pytest.fixture
    def strategy(self):
        return SequentialStrategy()

    @pytest.mark.asyncio
    async def test_process_default_order(self, strategy):
        """Test sequential processing with default order."""
        text = "–¢—ë–º–Ω—ã–π –ª–µ—Å —Å –≤—ã—Å–æ–∫–∏–º–∏ —Å–æ—Å–Ω–∞–º–∏."
        result = await strategy.process(text)

        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_process_custom_order(self, strategy):
        """Test with custom processor order."""
        text = "–ê–Ω–Ω–∞ –≤ —Å–∞–¥—É."
        processors = ["natasha", "spacy", "stanza"]
        result = await strategy.process(text, processors=processors)

        assert result is not None

    @pytest.mark.asyncio
    async def test_context_enrichment(self, strategy):
        """Test that later processors enrich earlier results."""
        text = "–í—ã—Å–æ–∫–∞—è –≥–æ—Ä–∞."
        result = await strategy.process(text)

        # Sequential mode should allow context from earlier processors
        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_stop_on_sufficient_results(self, strategy):
        """Test early stopping when enough descriptions found."""
        text = "–¢–µ–∫—Å—Ç —Å –º–Ω–æ–≥–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏."
        config = {"max_descriptions": 3}
        result = await strategy.process(text, config=config)

        # Should stop early if first processor provides enough
        assert len(result) <= 3

    @pytest.mark.asyncio
    async def test_empty_text(self, strategy):
        """Test with empty input."""
        result = await strategy.process("")
        assert result == []

    @pytest.mark.asyncio
    async def test_error_recovery(self, strategy):
        """Test recovery when processor fails."""
        text = "–¢–µ—Å—Ç"
        # Even if one processor fails, should continue with next
        result = await strategy.process(text)
        assert result is not None
```

**Deliverables:**
- ‚úÖ 3 test files created
- ‚úÖ 18 tests passing
- üìù Coverage report for strategies

---

### Day 3 (Wednesday) - NLP Strategy Tests (Part 2)

**Goal:** Create 3 more strategy test files (17 tests)

**File 4: test_ensemble_strategy.py** (8 tests, 3 hours)
```python
# backend/tests/services/nlp/strategies/test_ensemble_strategy.py

import pytest
from app.services.nlp.strategies.ensemble_strategy import EnsembleStrategy

class TestEnsembleStrategy:
    """Tests for EnsembleStrategy - voting and consensus."""

    @pytest.fixture
    def strategy(self):
        return EnsembleStrategy()

    @pytest.mark.asyncio
    async def test_voting_consensus(self, strategy):
        """Test that voting produces consensus results."""
        text = "–í—ã—Å–æ–∫–∞—è –±–∞—à–Ω—è –≤ —Ü–µ–Ω—Ç—Ä–µ –≥–æ—Ä–æ–¥–∞."
        result = await strategy.process(text)

        assert result is not None
        # Ensemble should return high-confidence results
        for desc in result:
            assert desc.get("confidence", 0) >= 0.6  # Consensus threshold

    @pytest.mark.asyncio
    async def test_weighted_voting(self, strategy):
        """Test processor weights in voting."""
        text = "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤ —Å—Ç–æ—è–ª —É –¥–æ–º–∞."
        result = await strategy.process(text)

        # Should use processor weights (e.g., Natasha 0.8, SpaCy 1.0, Stanza 0.7)
        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_consensus_threshold(self, strategy):
        """Test consensus threshold filtering."""
        text = "–†–µ–¥–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
        result = await strategy.process(text, consensus_threshold=0.8)

        # Only descriptions with 80%+ agreement
        for desc in result:
            assert desc.get("consensus_score", 0) >= 0.8

    @pytest.mark.asyncio
    async def test_conflict_resolution(self, strategy):
        """Test resolution when processors disagree."""
        text = "–ú–æ—Å–∫–≤–∞ - —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏."
        result = await strategy.process(text)

        # Should resolve conflicts intelligently
        assert result is not None

    @pytest.mark.asyncio
    async def test_quality_boost(self, strategy):
        """Test that ensemble improves quality."""
        text = "–°—Ç–∞—Ä–∏–Ω–Ω—ã–π –∑–∞–º–æ–∫ –Ω–∞ —Ö–æ–ª–º–µ."
        result = await strategy.process(text)

        # Ensemble should have higher quality than single processor
        if len(result) > 0:
            avg_confidence = sum(d.get("confidence", 0) for d in result) / len(result)
            assert avg_confidence >= 0.5

    @pytest.mark.asyncio
    async def test_minority_opinions(self, strategy):
        """Test handling of minority processor opinions."""
        text = "–°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
        result = await strategy.process(text, include_minority=False)

        # Minority opinions should be filtered
        assert result is not None

    @pytest.mark.asyncio
    async def test_empty_text(self, strategy):
        """Test with empty input."""
        result = await strategy.process("")
        assert result == []

    @pytest.mark.asyncio
    async def test_voting_metadata(self, strategy):
        """Test that voting metadata is included."""
        text = "–ë–æ–ª—å—à–æ–π –≥–æ—Ä–æ–¥."
        result = await strategy.process(text, include_metadata=True)

        if len(result) > 0:
            # Should include voting details
            assert "consensus_score" in result[0] or "votes" in result[0]
```

**File 5: test_adaptive_strategy.py** (6 tests, 2 hours)
```python
# backend/tests/services/nlp/strategies/test_adaptive_strategy.py

import pytest
from app.services.nlp.strategies.adaptive_strategy import AdaptiveStrategy

class TestAdaptiveStrategy:
    """Tests for AdaptiveStrategy - intelligent mode selection."""

    @pytest.fixture
    def strategy(self):
        return AdaptiveStrategy()

    @pytest.mark.asyncio
    async def test_short_text_uses_single(self, strategy):
        """Test that short texts use single processor (fast)."""
        text = "–õ–µ—Å."  # Very short
        result = await strategy.process(text)

        # Should use single strategy for efficiency
        assert result is not None

    @pytest.mark.asyncio
    async def test_medium_text_uses_parallel(self, strategy):
        """Test that medium texts use parallel."""
        text = "–ö—Ä–∞—Å–∏–≤—ã–π –ª–µ—Å —Å –≤—ã—Å–æ–∫–∏–º–∏ –¥–µ—Ä–µ–≤—å—è–º–∏ –∏ –ø–µ–Ω–∏–µ–º –ø—Ç–∏—Ü." * 3
        result = await strategy.process(text)

        # Should use parallel for good coverage
        assert result is not None
        assert len(result) > 0

    @pytest.mark.asyncio
    async def test_complex_text_uses_ensemble(self, strategy):
        """Test that complex texts use ensemble (highest quality)."""
        text = "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á —à—ë–ª –ø–æ —Å—Ç–∞—Ä–æ–º—É –ê—Ä–±–∞—Ç—É..." * 10
        result = await strategy.process(text)

        # Should use ensemble for maximum quality
        assert result is not None
        if len(result) > 0:
            # Ensemble produces high-confidence results
            avg_conf = sum(d.get("confidence", 0) for d in result) / len(result)
            assert avg_conf >= 0.5

    @pytest.mark.asyncio
    async def test_strategy_selection_metadata(self, strategy):
        """Test that selected strategy is recorded."""
        text = "–°—Ä–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."
        result = await strategy.process(text, include_metadata=True)

        # Should include info about which strategy was used
        assert result is not None

    @pytest.mark.asyncio
    async def test_fallback_on_error(self, strategy):
        """Test fallback when primary strategy fails."""
        text = "–¢–µ—Å—Ç –Ω–∞ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å."
        result = await strategy.process(text)

        # Should gracefully handle errors
        assert result is not None

    @pytest.mark.asyncio
    async def test_empty_text(self, strategy):
        """Test with empty input."""
        result = await strategy.process("")
        assert result == []
```

**File 6: test_strategy_factory.py** (5 tests, 1 hour)
```python
# backend/tests/services/nlp/strategies/test_strategy_factory.py

import pytest
from app.services.nlp.strategies.strategy_factory import StrategyFactory
from app.services.nlp.strategies.single_strategy import SingleStrategy
from app.services.nlp.strategies.parallel_strategy import ParallelStrategy
from app.services.nlp.strategies.sequential_strategy import SequentialStrategy
from app.services.nlp.strategies.ensemble_strategy import EnsembleStrategy
from app.services.nlp.strategies.adaptive_strategy import AdaptiveStrategy

class TestStrategyFactory:
    """Tests for StrategyFactory - strategy creation."""

    def test_create_single_strategy(self):
        """Test creating SingleStrategy."""
        strategy = StrategyFactory.create("single")
        assert isinstance(strategy, SingleStrategy)

    def test_create_parallel_strategy(self):
        """Test creating ParallelStrategy."""
        strategy = StrategyFactory.create("parallel")
        assert isinstance(strategy, ParallelStrategy)

    def test_create_sequential_strategy(self):
        """Test creating SequentialStrategy."""
        strategy = StrategyFactory.create("sequential")
        assert isinstance(strategy, SequentialStrategy)

    def test_create_ensemble_strategy(self):
        """Test creating EnsembleStrategy."""
        strategy = StrategyFactory.create("ensemble")
        assert isinstance(strategy, EnsembleStrategy)

    def test_create_adaptive_strategy(self):
        """Test creating AdaptiveStrategy."""
        strategy = StrategyFactory.create("adaptive")
        assert isinstance(strategy, AdaptiveStrategy)

    def test_invalid_strategy_name(self):
        """Test error for invalid strategy."""
        with pytest.raises(ValueError, match="Unknown strategy"):
            StrategyFactory.create("invalid")

    def test_default_strategy(self):
        """Test default strategy (if no name provided)."""
        strategy = StrategyFactory.create()
        # Should return adaptive by default
        assert isinstance(strategy, AdaptiveStrategy)
```

**Deliverables:**
- ‚úÖ 3 more test files
- ‚úÖ 17 tests passing
- ‚úÖ Total 35/30 strategy tests complete

---

### Day 4 (Thursday) - EpubReader Tests (Part 1)

**Goal:** Create EpubReader test file with 20 tests

**File: EpubReader.test.tsx** (20 tests, 6 hours)
```typescript
// frontend/src/components/Reader/__tests__/EpubReader.test.tsx

import { describe, it, expect, vi, beforeEach } from 'vitest';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { EpubReader } from '../EpubReader';
import type { BookDetail } from '@/types/api';

// Mock hooks
vi.mock('@/hooks/epub/useEpubLoader', () => ({
  useEpubLoader: vi.fn(() => ({
    book: null,
    rendition: null,
    isLoading: false,
    error: null,
  })),
}));

vi.mock('@/hooks/epub/useBookMetadata', () => ({
  useBookMetadata: vi.fn(() => ({
    metadata: { title: 'Test Book', author: 'Test Author' },
    cover: null,
  })),
}));

// ... more mocks for all 16 hooks

const mockBook: BookDetail = {
  id: '123',
  title: 'Test Book',
  author: 'Test Author',
  // ... full book data
};

describe('EpubReader', () => {
  beforeEach(() => {
    vi.clearAllMocks();
  });

  describe('Rendering', () => {
    it('should render without crashing', () => {
      render(<EpubReader bookId="123" />);
      expect(screen.getByTestId('epub-reader')).toBeInTheDocument();
    });

    it('should show loading state', () => {
      vi.mocked(useEpubLoader).mockReturnValue({
        book: null,
        rendition: null,
        isLoading: true,
        error: null,
      });

      render(<EpubReader bookId="123" />);
      expect(screen.getByText(/loading/i)).toBeInTheDocument();
    });

    it('should show error state', () => {
      vi.mocked(useEpubLoader).mockReturnValue({
        book: null,
        rendition: null,
        isLoading: false,
        error: 'Failed to load',
      });

      render(<EpubReader bookId="123" />);
      expect(screen.getByText(/failed to load/i)).toBeInTheDocument();
    });

    it('should render reader when book loaded', async () => {
      vi.mocked(useEpubLoader).mockReturnValue({
        book: mockEpubBook,
        rendition: mockRendition,
        isLoading: false,
        error: null,
      });

      render(<EpubReader bookId="123" />);
      await waitFor(() => {
        expect(screen.getByTestId('epub-content')).toBeInTheDocument();
      });
    });
  });

  describe('Navigation', () => {
    it('should navigate to next page', async () => {
      const mockNext = vi.fn();
      vi.mocked(useEpubNavigation).mockReturnValue({
        goNext: mockNext,
        goPrev: vi.fn(),
        goToLocation: vi.fn(),
      });

      render(<EpubReader bookId="123" />);

      const nextButton = screen.getByLabelText(/next page/i);
      fireEvent.click(nextButton);

      expect(mockNext).toHaveBeenCalled();
    });

    // ... more navigation tests
  });

  // ... more test suites (20 total)
});
```

**Deliverables:**
- ‚úÖ 20 EpubReader tests
- üìù Mock setup documentation

---

### Day 5 (Friday) - EpubReader Tests (Part 2)

**Goal:** Complete remaining 20 EpubReader tests

**Continued: EpubReader.test.tsx** (20 more tests, 6 hours)
```typescript
  describe('State Management', () => {
    it('should track current location', () => {
      // Test CFI tracking
    });

    it('should sync progress to backend', () => {
      // Test progress sync
    });

    // ... 6 more state tests
  });

  describe('Theme Switching', () => {
    it('should apply light theme', () => {
      // Test theme application
    });

    it('should apply dark theme', () => {
      // Test theme switching
    });

    // ... 4 more theme tests
  });

  describe('Error Handling', () => {
    it('should handle render errors gracefully', () => {
      // Test error boundaries
    });

    // ... 3 more error tests
  });

  describe('Accessibility', () => {
    it('should be keyboard navigable', () => {
      // Test keyboard nav
    });

    it('should have proper ARIA labels', () => {
      // Test accessibility
    });

    // ... 3 more a11y tests
  });
```

**Deliverables:**
- ‚úÖ 40 total EpubReader tests
- ‚úÖ All P0 tests complete (70 total)

---

### Day 6 (Saturday) - Coverage Gates Setup

**Goal:** Setup automated quality gates

**Morning (4 hours):**

1. **Pre-commit Hooks** (2 hours)
```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: pytest-coverage
        name: Backend Coverage Check
        entry: bash -c 'cd backend && pytest --cov=app --cov-fail-under=50'
        language: system
        pass_filenames: false
        stages: [commit]

      - id: vitest-coverage
        name: Frontend Coverage Check
        entry: bash -c 'cd frontend && npm test -- --coverage --run'
        language: system
        pass_filenames: false
        stages: [commit]
```

2. **CI/CD Pipeline** (2 hours)
```yaml
# .github/workflows/tests.yml
name: Tests

on: [push, pull_request]

jobs:
  backend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: |
          docker-compose exec -T backend pytest --cov=app --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./backend/coverage.xml
          fail_ci_if_error: true

  frontend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: |
          cd frontend && npm test -- --coverage --run
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

**Afternoon (4 hours):**

3. **Coverage Badges** (1 hour)
```markdown
# README.md
[![Backend Coverage](https://codecov.io/gh/user/repo/branch/main/graph/badge.svg?flag=backend)](https://codecov.io/gh/user/repo)
[![Frontend Coverage](https://codecov.io/gh/user/repo/branch/main/graph/badge.svg?flag=frontend)](https://codecov.io/gh/user/repo)
```

4. **Documentation** (3 hours)
   - Test strategy document
   - Coverage goals document
   - Testing best practices guide

**Deliverables:**
- ‚úÖ Pre-commit hooks enabled
- ‚úÖ CI/CD pipeline configured
- ‚úÖ Coverage tracking setup
- üìù Testing documentation

---

### Day 7 (Sunday) - Review & Planning

**Goal:** Review progress and plan next week

**Tasks:**
1. Run full test suite (1 hour)
   ```bash
   # Backend
   docker-compose exec backend pytest --cov=app --cov-report=html

   # Frontend
   cd frontend && npm test -- --coverage
   ```

2. Generate coverage reports (1 hour)
   - Backend HTML report
   - Frontend HTML report
   - Coverage comparison

3. Update documentation (2 hours)
   - Update TESTING_AUDIT_REPORT.md
   - Update progress tracking
   - Document learnings

4. Plan Week 2 (2 hours)
   - Book Services tests (60 tests)
   - Core Hooks tests (30 tests)
   - Books Router tests (25 tests)

**Deliverables:**
- üìä Coverage reports
- üìù Updated documentation
- üìÖ Week 2 plan

---

## üìä Expected Outcomes

### Tests Created
- ‚úÖ NLP Strategies: 30 tests (6 files)
- ‚úÖ EpubReader: 40 tests (1 file)
- **Total:** 70 new tests

### Issues Fixed
- ‚úÖ 1 failing test
- ‚úÖ 2 collection errors
- ‚úÖ Investigation complete

### Coverage Impact
```
Backend:
  Before: 34% (2557/7607 lines)
  After:  38%+ (estimated +4% from strategy tests)

Frontend:
  Before: ~15%
  After:  25%+ (estimated +10% from EpubReader)
```

### Total Tests
```
Before: 621 tests
After:  691 tests (+70)
```

---

## üéØ Success Criteria

- [ ] All tests passing (691/691)
- [ ] No collection errors
- [ ] NLP Strategies coverage >70%
- [ ] EpubReader coverage >70%
- [ ] Pre-commit hooks working
- [ ] CI/CD pipeline passing
- [ ] Documentation updated

---

## üöß Risks & Mitigations

### Risk 1: Tests take longer than estimated
**Mitigation:** Focus on P0 critical tests first, defer nice-to-haves

### Risk 2: Collection errors difficult to fix
**Mitigation:** Skip problematic test files, fix in Week 2

### Risk 3: EpubReader tests require complex mocking
**Mitigation:** Use minimal viable mocks, focus on critical paths

### Risk 4: Coverage gates break CI
**Mitigation:** Start with warnings only, enforce after stabilization

---

## üìû Support Resources

**Documentation:**
- pytest docs: https://docs.pytest.org
- vitest docs: https://vitest.dev
- testing-library: https://testing-library.com

**Internal:**
- `/backend/TESTING_AUDIT_REPORT.md` - Full audit
- `/backend/TESTING_AUDIT_SUMMARY.md` - Quick reference
- `/backend/tests/COMPREHENSIVE_TEST_SUMMARY.md` - Previous work

**Team:**
- QA Lead: Review test quality
- Tech Lead: Approve coverage gates
- Developers: Code review

---

## üìù Daily Checklist

Each day:
- [ ] Run tests locally before committing
- [ ] Update test count in tracking doc
- [ ] Document any blockers
- [ ] Push code to feature branch
- [ ] Review coverage report

End of week:
- [ ] All 70 tests passing
- [ ] Coverage gates setup
- [ ] Documentation updated
- [ ] Week 2 planned

---

**Created by:** Testing & QA Specialist Agent
**Date:** 2025-10-30
**Status:** üü¢ READY TO START

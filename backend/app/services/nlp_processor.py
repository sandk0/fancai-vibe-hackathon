"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç spaCy, Natasha –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º.
"""

import spacy
import re
from typing import List, Dict, Any, Optional
from enum import Enum
import asyncio

from ..models.description import DescriptionType
from .settings_manager import get_nlp_settings


class NLPProcessorType(Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""
    SPACY = "spacy"
    NATASHA = "natasha"
    HYBRID = "hybrid"


class BaseNLPProcessor:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""
    
    def __init__(self):
        self.processor_type = None
        self.loaded = False
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ë–î)
        self.min_description_length = 50
        self.max_description_length = 1000
        self.min_word_count = 10
        self.min_sentence_length = 30
        self.confidence_threshold = 0.3
        
    async def load_settings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
        try:
            settings = await get_nlp_settings()
            self.min_description_length = settings.get('min_description_length', 50)
            self.max_description_length = settings.get('max_description_length', 1000)
            self.min_word_count = settings.get('min_word_count', 10)
            self.min_sentence_length = settings.get('min_sentence_length', 30)
            self.confidence_threshold = settings.get('confidence_threshold', 0.3)
            print(f"‚úÖ NLP settings loaded: {settings}")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load NLP settings, using defaults: {e}")
    
    async def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        raise NotImplementedError
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return self.loaded
    
    def extract_descriptions(self, text: str, chapter_id: str = None) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        raise NotImplementedError
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ä—É—Å—Å–∫–∏–µ, –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s\.!\?,;:\-‚Äî¬´¬ª()\[\]]+', '', text)
        return text.strip()
    
    def _filter_and_prioritize(self, descriptions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è."""
        filtered = []
        
        for desc in descriptions:
            content = desc.get('content', '')
            word_count = len(content.split())
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –±–∞–∑–æ–≤—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
            if (len(content) >= self.min_description_length and
                len(content) <= self.max_description_length and
                word_count >= self.min_word_count and
                desc.get('confidence_score', 0) >= self.confidence_threshold):
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                desc['priority_score'] = self._calculate_priority_score(
                    desc.get('type'), 
                    desc.get('confidence_score', 0), 
                    len(content)
                )
                filtered.append(desc)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        filtered.sort(key=lambda x: x.get('priority_score', 0), reverse=True)
        return filtered
    
    def _calculate_priority_score(self, desc_type: str, confidence: float, text_length: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Å—á–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è."""
        type_priorities = {
            DescriptionType.LOCATION.value: 75,
            DescriptionType.CHARACTER.value: 60,
            DescriptionType.ATMOSPHERE.value: 45,
            DescriptionType.OBJECT.value: 40,
            DescriptionType.ACTION.value: 30
        }
        
        base_priority = type_priorities.get(desc_type, 30)
        confidence_bonus = confidence * 20
        
        # –ë–æ–Ω—É—Å –∑–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        length_bonus = 0
        if 50 <= text_length <= 400:
            length_bonus = 10
        elif text_length < 50:
            length_bonus = max(0, (text_length - 20) / 5)
        else:
            length_bonus = max(0, 10 - (text_length - 400) / 100)
        
        return base_priority + confidence_bonus + length_bonus


class SpacyProcessor(BaseNLPProcessor):
    """spaCy –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    
    def __init__(self):
        super().__init__()
        self.processor_type = NLPProcessorType.SPACY
        self.nlp = None
        self._model_loading = False
        self.model_name = 'ru_core_news_lg'
    
    async def load_model(self, model_name: str = None):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç spaCy –º–æ–¥–µ–ª—å."""
        if self._model_loading:
            return
        
        self._model_loading = True
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
            await self.load_settings()
            
            if model_name:
                self.model_name = model_name
            
            print(f"üîÑ Loading spaCy model {self.model_name}...")
            self.nlp = spacy.load(self.model_name)
            self.loaded = True
            print(f"‚úÖ spaCy model {self.model_name} loaded successfully")
        except OSError as e:
            print(f"‚ö†Ô∏è Warning: spaCy model {self.model_name} not found: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
            try:
                fallback_model = 'ru_core_news_md'
                print(f"üîÑ Trying fallback model {fallback_model}...")
                self.nlp = spacy.load(fallback_model)
                self.model_name = fallback_model
                self.loaded = True
                print(f"‚úÖ Fallback spaCy model {fallback_model} loaded successfully")
            except OSError:
                print("‚ùå No spaCy models available")
                self.nlp = None
                self.loaded = False
        except Exception as e:
            print(f"‚ùå Error loading spaCy model: {e}")
            self.nlp = None
            self.loaded = False
        finally:
            self._model_loading = False
    
    def extract_descriptions(self, text: str, chapter_id: str = None) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è spaCy."""
        if not self.is_available():
            return []
        
        print(f"üîç SpaCy: extracting descriptions from text (length: {len(text)})")
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = self._clean_text(text)
        
        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        doc = self.nlp(cleaned_text)
        sentences = [
            sent.text.strip() 
            for sent in doc.sents 
            if len(sent.text.strip()) >= self.min_sentence_length
        ]
        
        descriptions = []
        
        for i, sentence in enumerate(sentences):
            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è
            sentence_descriptions = self._analyze_sentence_spacy(sentence, i, cleaned_text)
            descriptions.extend(sentence_descriptions)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
        filtered_descriptions = self._filter_and_prioritize(descriptions)
        
        print(f"‚úÖ SpaCy: extracted {len(filtered_descriptions)} descriptions")
        return filtered_descriptions
    
    def _analyze_sentence_spacy(self, sentence: str, position: int, full_text: str) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—è spaCy."""
        doc = self.nlp(sentence)
        descriptions = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        for ent in doc.ents:
            desc_type = None
            confidence = 0.5
            
            if ent.label_ in ['LOC', 'GPE', 'FAC']:  # –õ–æ–∫–∞—Ü–∏–∏
                desc_type = DescriptionType.LOCATION.value
                confidence = 0.8
            elif ent.label_ in ['PERSON']:  # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏
                desc_type = DescriptionType.CHARACTER.value
                confidence = 0.7
            elif ent.label_ in ['ORG']:  # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ –æ–±—ä–µ–∫—Ç—ã
                desc_type = DescriptionType.OBJECT.value
                confidence = 0.6
            
            if desc_type:
                # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—É—â–Ω–æ—Å—Ç–∏
                extended_context = self._get_extended_context(sentence, ent.text, full_text)
                
                descriptions.append({
                    'content': extended_context,
                    'context': sentence,
                    'type': desc_type,
                    'confidence_score': confidence,
                    'entities_mentioned': ent.text,
                    'text_position_start': ent.start_char,
                    'text_position_end': ent.end_char,
                    'position': position
                })
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        pattern_descriptions = self._extract_by_patterns(sentence, position)
        descriptions.extend(pattern_descriptions)
        
        return descriptions
    
    def _extract_by_patterns(self, sentence: str, position: int) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."""
        descriptions = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ª–æ–∫–∞—Ü–∏–π
        location_patterns = [
            r'(?:–≤|–Ω–∞|–æ–∫–æ–ª–æ|–≤–æ–∑–ª–µ|—Ä—è–¥–æ–º —Å|–ø–µ—Ä–µ–¥|–∑–∞|–Ω–∞–¥|–ø–æ–¥)\s+([^,.!?]{10,100})',
            r'([^,.!?]{5,50})\s+(?:—Å—Ç–æ—è–ª|—Å—Ç–æ—è–ª–∞|—Å—Ç–æ—è–ª–æ|–Ω–∞—Ö–æ–¥–∏–ª—Å—è|–Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å|–Ω–∞—Ö–æ–¥–∏–ª–æ—Å—å)',
            r'(?:–¥–æ–º|–∑–¥–∞–Ω–∏–µ|–∑–∞–º–æ–∫|—Ö—Ä–∞–º|–¥–≤–æ—Ä–µ—Ü|–±–∞—à–Ω—è|–º–æ—Å—Ç|–ª–µ—Å|–ø–æ–ª–µ|–≥–æ—Ä—ã?|—Ä–µ–∫–∞|–º–æ—Ä–µ|–æ–∑–µ—Ä–æ)\s+([^,.!?]{10,100})',
        ]
        
        for pattern in location_patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                content = match.group(0).strip()
                if len(content) >= self.min_description_length:
                    descriptions.append({
                        'content': content,
                        'context': sentence,
                        'type': DescriptionType.LOCATION.value,
                        'confidence_score': 0.6,
                        'entities_mentioned': match.group(1) if match.lastindex >= 1 else content,
                        'text_position_start': match.start(),
                        'text_position_end': match.end(),
                        'position': position
                    })
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        character_patterns = [
            r'(?:–æ–Ω|–æ–Ω–∞|–æ–Ω–æ|–æ–Ω–∏)\s+(?:–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏)\s+([^,.!?]{10,100})',
            r'(?:–º—É–∂—á–∏–Ω–∞|–∂–µ–Ω—â–∏–Ω–∞|–¥–µ–≤—É—à–∫–∞|–ø–∞—Ä–µ–Ω—å|—Å—Ç–∞—Ä–∏–∫|—Å—Ç–∞—Ä—É—Ö–∞)\s+([^,.!?]{10,100})',
        ]
        
        for pattern in character_patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                content = match.group(0).strip()
                if len(content) >= self.min_description_length:
                    descriptions.append({
                        'content': content,
                        'context': sentence,
                        'type': DescriptionType.CHARACTER.value,
                        'confidence_score': 0.5,
                        'entities_mentioned': match.group(1) if match.lastindex >= 1 else content,
                        'text_position_start': match.start(),
                        'text_position_end': match.end(),
                        'position': position
                    })
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã
        atmosphere_patterns = [
            r'(?:–±—ã–ª–æ|—Å—Ç–∞–ª–æ)\s+(?:—Ç–µ–º–Ω–æ|—Å–≤–µ—Ç–ª–æ|—Ö–æ–ª–æ–¥–Ω–æ|–∂–∞—Ä–∫–æ|—Ç–∏—Ö–æ|—à—É–º–Ω–æ|—Ç—É–º–∞–Ω–Ω–æ|—è—Å–Ω–æ)\s*([^,.!?]{0,50})',
            r'(?:–Ω–∞—Å—Ç—É–ø–∏–ª|–Ω–∞—Å—Ç—É–ø–∏–ª–∞|–Ω–∞—Å—Ç—É–ø–∏–ª–æ)\s+(?:–≤–µ—á–µ—Ä|—É—Ç—Ä–æ|–Ω–æ—á—å|–¥–µ–Ω—å|—Ä–∞—Å—Å–≤–µ—Ç|–∑–∞–∫–∞—Ç)\s*([^,.!?]{0,50})',
        ]
        
        for pattern in atmosphere_patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                content = match.group(0).strip()
                if len(content) >= self.min_description_length:
                    descriptions.append({
                        'content': content,
                        'context': sentence,
                        'type': DescriptionType.ATMOSPHERE.value,
                        'confidence_score': 0.7,
                        'entities_mentioned': match.group(1) if match.lastindex >= 1 else content,
                        'text_position_start': match.start(),
                        'text_position_end': match.end(),
                        'position': position
                    })
        
        return descriptions
    
    def _get_extended_context(self, sentence: str, entity: str, full_text: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—É—â–Ω–æ—Å—Ç–∏."""
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π –≤–æ–∫—Ä—É–≥ —Å—É—â–Ω–æ—Å—Ç–∏
        entity_pos = sentence.find(entity)
        if entity_pos == -1:
            return sentence
        
        # –ë–µ—Ä–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º, —ç—Ç–æ –∏ –µ—Å—Ç—å –Ω–∞—à –∫–æ–Ω—Ç–µ–∫—Å—Ç
        return sentence


class NatashaProcessor(BaseNLPProcessor):
    """Natasha –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    
    def __init__(self):
        super().__init__()
        self.processor_type = NLPProcessorType.NATASHA
        self.morph = None
        self.segmenter = None
        self.emb = None
        self.ner_tagger = None
    
    async def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Natasha –º–æ–¥–µ–ª–∏."""
        try:
            print("üîÑ Loading Natasha models...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
            await self.load_settings()
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ Natasha
            from natasha import (
                Segmenter, MorphVocab, 
                NewsEmbedding, NewsNERTagger,
                Doc
            )
            
            self.segmenter = Segmenter()
            morph_vocab = MorphVocab()
            self.emb = NewsEmbedding()
            self.ner_tagger = NewsNERTagger(self.emb)
            
            self.loaded = True
            print("‚úÖ Natasha models loaded successfully")
            
        except ImportError as e:
            print(f"‚ö†Ô∏è Natasha not available: {e}")
            self.loaded = False
        except Exception as e:
            print(f"‚ùå Error loading Natasha: {e}")
            self.loaded = False
    
    def extract_descriptions(self, text: str, chapter_id: str = None) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è Natasha."""
        if not self.is_available():
            return []
        
        print(f"üîç Natasha: extracting descriptions from text (length: {len(text)})")
        
        try:
            from natasha import Doc
            
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = self._clean_text(text)
            
            # –ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é Natasha
            doc = Doc(cleaned_text)
            doc.segment(self.segmenter)
            doc.tag_ner(self.ner_tagger)
            
            descriptions = []
            
            for i, sent in enumerate(doc.sents):
                sentence = sent.text
                if len(sentence) < self.min_sentence_length:
                    continue
                
                # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                sentence_descriptions = self._analyze_sentence_natasha(sentence, i, sent.spans)
                descriptions.extend(sentence_descriptions)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
            filtered_descriptions = self._filter_and_prioritize(descriptions)
            
            print(f"‚úÖ Natasha: extracted {len(filtered_descriptions)} descriptions")
            return filtered_descriptions
            
        except Exception as e:
            print(f"‚ùå Error in Natasha processing: {e}")
            return []
    
    def _analyze_sentence_natasha(self, sentence: str, position: int, spans) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Natasha NER."""
        descriptions = []
        
        for span in spans:
            desc_type = None
            confidence = 0.5
            
            if span.type == 'LOC':  # –õ–æ–∫–∞—Ü–∏–∏
                desc_type = DescriptionType.LOCATION.value
                confidence = 0.8
            elif span.type == 'PER':  # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏
                desc_type = DescriptionType.CHARACTER.value
                confidence = 0.7
            elif span.type == 'ORG':  # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                desc_type = DescriptionType.OBJECT.value
                confidence = 0.6
            
            if desc_type:
                entity_text = sentence[span.start:span.stop]
                descriptions.append({
                    'content': sentence,  # –ë–µ—Ä–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    'context': sentence,
                    'type': desc_type,
                    'confidence_score': confidence,
                    'entities_mentioned': entity_text,
                    'text_position_start': span.start,
                    'text_position_end': span.stop,
                    'position': position
                })
        
        return descriptions


class NLPProcessor:
    """–ì–ª–∞–≤–Ω—ã–π NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤."""
    
    def __init__(self):
        self.processors = {
            NLPProcessorType.SPACY: SpacyProcessor(),
            NLPProcessorType.NATASHA: NatashaProcessor(),
        }
        self.current_processor = None
        self.current_type = NLPProcessorType.SPACY  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    async def initialize(self, processor_type: NLPProcessorType = None, model_name: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
        if processor_type:
            self.current_type = processor_type
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        try:
            settings = await get_nlp_settings()
            processor_type_str = settings.get('processor_type', 'spacy')
            self.current_type = NLPProcessorType(processor_type_str)
            model_name = settings.get('spacy_model', 'ru_core_news_lg')
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load processor type from settings: {e}")
        
        self.current_processor = self.processors[self.current_type]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        if self.current_type == NLPProcessorType.SPACY:
            await self.current_processor.load_model(model_name)
        else:
            await self.current_processor.load_model()
        
        print(f"‚úÖ NLP Processor initialized: {self.current_type.value}")
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return (self.current_processor and 
                self.current_processor.is_available())
    
    def extract_descriptions_from_text(self, text: str, chapter_id: str = None) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
        if not self.is_available():
            print("‚ùå NLP processor not available")
            return []
        
        return self.current_processor.extract_descriptions(text, chapter_id)
    
    async def switch_processor(self, processor_type: NLPProcessorType, model_name: str = None):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç —Ç–∏–ø –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        if processor_type not in self.processors:
            raise ValueError(f"Unsupported processor type: {processor_type}")
        
        self.current_type = processor_type
        self.current_processor = self.processors[processor_type]
        
        if not self.current_processor.loaded:
            if processor_type == NLPProcessorType.SPACY:
                await self.current_processor.load_model(model_name)
            else:
                await self.current_processor.load_model()
        
        print(f"‚úÖ Switched to NLP processor: {processor_type.value}")
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return {
            'spacy': ['ru_core_news_lg', 'ru_core_news_md', 'ru_core_news_sm'],
            'natasha': ['default']
        }
    
    def get_current_processor_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ."""
        if not self.current_processor:
            return {'type': None, 'loaded': False, 'available': False}
        
        info = {
            'type': self.current_type.value,
            'loaded': self.current_processor.loaded,
            'available': self.current_processor.is_available()
        }
        
        if self.current_type == NLPProcessorType.SPACY and hasattr(self.current_processor, 'model_name'):
            info['model'] = self.current_processor.model_name
        
        return info
    
    async def update_settings(self, settings: Dict[str, Any]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        print(f"üîß [NLP] Updating settings: {settings}")
        
        # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        processor_type = settings.get('processor_type')
        if processor_type:
            try:
                if processor_type == 'spacy':
                    new_type = NLPProcessorType.SPACY
                elif processor_type == 'natasha':
                    new_type = NLPProcessorType.NATASHA
                elif processor_type == 'hybrid':
                    new_type = NLPProcessorType.SPACY  # Hybrid –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SpaCy –∫–∞–∫ –æ—Å–Ω–æ–≤—É
                else:
                    print(f"‚ö†Ô∏è Unknown processor type: {processor_type}")
                    return
                
                if new_type != self.current_type:
                    await self.switch_processor(new_type)
                    
            except Exception as e:
                print(f"‚ùå Failed to switch processor: {e}")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ spaCy –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        spacy_model = settings.get('spacy_model')
        if spacy_model and self.current_type == NLPProcessorType.SPACY and hasattr(self.current_processor, 'model_name'):
            if self.current_processor.model_name != spacy_model:
                try:
                    await self.initialize(NLPProcessorType.SPACY, spacy_model)
                    print(f"‚úÖ Switched to spaCy model: {spacy_model}")
                except Exception as e:
                    print(f"‚ùå Failed to switch spaCy model: {e}")
        
        print(f"‚úÖ [NLP] Settings updated successfully")


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
nlp_processor = NLPProcessor()


async def initialize_nlp_processor():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
    await nlp_processor.initialize()
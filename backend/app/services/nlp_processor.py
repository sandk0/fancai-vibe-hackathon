"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç spaCy, Natasha –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º.
"""

import spacy
import re
from typing import List, Dict, Any
from enum import Enum

from ..models.description import DescriptionType
from .nlp.utils.text_cleaner import clean_text
from .nlp.utils.description_filter import filter_and_prioritize_descriptions
from .nlp.utils.type_mapper import (
    map_spacy_entity_to_description_type,
    map_natasha_entity_to_description_type,
)


class NLPProcessorType(Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""

    SPACY = "spacy"
    NATASHA = "natasha"
    HYBRID = "hybrid"


class BaseNLPProcessor:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""

    def __init__(self):
        self.processor_type = None
        self.loaded = False
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ë–î)
        self.min_description_length = 50
        self.max_description_length = 1000
        self.min_word_count = 10
        self.min_sentence_length = 30
        self.confidence_threshold = 0.3

    async def load_settings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
        # NOTE: settings_manager removed as it depended on orphaned AdminSettings model
        # Using default values instead
        print("‚ö†Ô∏è Using default NLP settings (AdminSettings removed)")

    async def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        raise NotImplementedError

    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return self.loaded

    def extract_descriptions(
        self, text: str, chapter_id: str = None
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        raise NotImplementedError

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç shared utility)."""
        return clean_text(text)

    def _filter_and_prioritize(
        self, descriptions: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç shared utility)."""
        return filter_and_prioritize_descriptions(
            descriptions,
            min_description_length=self.min_description_length,
            max_description_length=self.max_description_length,
            min_word_count=self.min_word_count,
            confidence_threshold=self.confidence_threshold,
        )


class SpacyProcessor(BaseNLPProcessor):
    """spaCy –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏."""

    def __init__(self):
        super().__init__()
        self.processor_type = NLPProcessorType.SPACY
        self.nlp = None
        self._model_loading = False
        self.model_name = "ru_core_news_lg"

    async def load_model(self, model_name: str = None):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç spaCy –º–æ–¥–µ–ª—å."""
        if self._model_loading:
            return

        self._model_loading = True
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
            await self.load_settings()

            if model_name:
                self.model_name = model_name

            print(f"üîÑ Loading spaCy model {self.model_name}...")
            self.nlp = spacy.load(self.model_name)
            self.loaded = True
            print(f"‚úÖ spaCy model {self.model_name} loaded successfully")
        except OSError as e:
            print(f"‚ö†Ô∏è Warning: spaCy model {self.model_name} not found: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
            try:
                fallback_model = "ru_core_news_md"
                print(f"üîÑ Trying fallback model {fallback_model}...")
                self.nlp = spacy.load(fallback_model)
                self.model_name = fallback_model
                self.loaded = True
                print(f"‚úÖ Fallback spaCy model {fallback_model} loaded successfully")
            except OSError:
                print("‚ùå No spaCy models available")
                self.nlp = None
                self.loaded = False
        except Exception as e:
            print(f"‚ùå Error loading spaCy model: {e}")
            self.nlp = None
            self.loaded = False
        finally:
            self._model_loading = False

    def extract_descriptions(
        self, text: str, chapter_id: str = None
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è spaCy."""
        if not self.is_available():
            return []

        print(f"üîç SpaCy: extracting descriptions from text (length: {len(text)})")

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = self._clean_text(text)

        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        doc = self.nlp(cleaned_text)
        sentences = [
            sent.text.strip()
            for sent in doc.sents
            if len(sent.text.strip()) >= self.min_sentence_length
        ]

        descriptions = []

        for i, sentence in enumerate(sentences):
            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è
            sentence_descriptions = self._analyze_sentence_spacy(
                sentence, i, cleaned_text
            )
            descriptions.extend(sentence_descriptions)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
        filtered_descriptions = self._filter_and_prioritize(descriptions)

        print(f"‚úÖ SpaCy: extracted {len(filtered_descriptions)} descriptions")
        return filtered_descriptions

    def _analyze_sentence_spacy(
        self, sentence: str, position: int, full_text: str
    ) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—è spaCy."""
        doc = self.nlp(sentence)
        descriptions = []

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        for ent in doc.ents:
            desc_type = None
            confidence = 0.5

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º shared type mapper
            desc_type = map_spacy_entity_to_description_type(ent.label_)

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º confidence –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞
            if ent.label_ in ["LOC", "GPE", "FAC"]:
                confidence = 0.8
            elif ent.label_ in ["PERSON"]:
                confidence = 0.7
            elif ent.label_ in ["ORG"]:
                confidence = 0.6

            if desc_type:
                # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—É—â–Ω–æ—Å—Ç–∏
                extended_context = self._get_extended_context(
                    sentence, ent.text, full_text
                )

                descriptions.append(
                    {
                        "content": extended_context,
                        "context": sentence,
                        "type": desc_type,
                        "confidence_score": confidence,
                        "entities_mentioned": ent.text,
                        "text_position_start": ent.start_char,
                        "text_position_end": ent.end_char,
                        "position": position,
                    }
                )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        pattern_descriptions = self._extract_by_patterns(sentence, position)
        descriptions.extend(pattern_descriptions)

        return descriptions

    def _extract_by_patterns(
        self, sentence: str, position: int
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."""
        descriptions = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ª–æ–∫–∞—Ü–∏–π
        location_patterns = [
            r"(?:–≤|–Ω–∞|–æ–∫–æ–ª–æ|–≤–æ–∑–ª–µ|—Ä—è–¥–æ–º —Å|–ø–µ—Ä–µ–¥|–∑–∞|–Ω–∞–¥|–ø–æ–¥)\s+([^,.!?]{10,100})",
            r"([^,.!?]{5,50})\s+(?:—Å—Ç–æ—è–ª|—Å—Ç–æ—è–ª–∞|—Å—Ç–æ—è–ª–æ|–Ω–∞—Ö–æ–¥–∏–ª—Å—è|–Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å|–Ω–∞—Ö–æ–¥–∏–ª–æ—Å—å)",
            r"(?:–¥–æ–º|–∑–¥–∞–Ω–∏–µ|–∑–∞–º–æ–∫|—Ö—Ä–∞–º|–¥–≤–æ—Ä–µ—Ü|–±–∞—à–Ω—è|–º–æ—Å—Ç|–ª–µ—Å|–ø–æ–ª–µ|–≥–æ—Ä—ã?|—Ä–µ–∫–∞|–º–æ—Ä–µ|–æ–∑–µ—Ä–æ)\s+([^,.!?]{10,100})",
        ]

        for pattern in location_patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                content = match.group(0).strip()
                if len(content) >= self.min_description_length:
                    descriptions.append(
                        {
                            "content": content,
                            "context": sentence,
                            "type": DescriptionType.LOCATION.value,
                            "confidence_score": 0.6,
                            "entities_mentioned": (
                                match.group(1) if match.lastindex >= 1 else content
                            ),
                            "text_position_start": match.start(),
                            "text_position_end": match.end(),
                            "position": position,
                        }
                    )

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        character_patterns = [
            r"(?:–æ–Ω|–æ–Ω–∞|–æ–Ω–æ|–æ–Ω–∏)\s+(?:–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏)\s+([^,.!?]{10,100})",
            r"(?:–º—É–∂—á–∏–Ω–∞|–∂–µ–Ω—â–∏–Ω–∞|–¥–µ–≤—É—à–∫–∞|–ø–∞—Ä–µ–Ω—å|—Å—Ç–∞—Ä–∏–∫|—Å—Ç–∞—Ä—É—Ö–∞)\s+([^,.!?]{10,100})",
        ]

        for pattern in character_patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                content = match.group(0).strip()
                if len(content) >= self.min_description_length:
                    descriptions.append(
                        {
                            "content": content,
                            "context": sentence,
                            "type": DescriptionType.CHARACTER.value,
                            "confidence_score": 0.5,
                            "entities_mentioned": (
                                match.group(1) if match.lastindex >= 1 else content
                            ),
                            "text_position_start": match.start(),
                            "text_position_end": match.end(),
                            "position": position,
                        }
                    )

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã
        atmosphere_patterns = [
            r"(?:–±—ã–ª–æ|—Å—Ç–∞–ª–æ)\s+(?:—Ç–µ–º–Ω–æ|—Å–≤–µ—Ç–ª–æ|—Ö–æ–ª–æ–¥–Ω–æ|–∂–∞—Ä–∫–æ|—Ç–∏—Ö–æ|—à—É–º–Ω–æ|—Ç—É–º–∞–Ω–Ω–æ|—è—Å–Ω–æ)\s*([^,.!?]{0,50})",
            r"(?:–Ω–∞—Å—Ç—É–ø–∏–ª|–Ω–∞—Å—Ç—É–ø–∏–ª–∞|–Ω–∞—Å—Ç—É–ø–∏–ª–æ)\s+(?:–≤–µ—á–µ—Ä|—É—Ç—Ä–æ|–Ω–æ—á—å|–¥–µ–Ω—å|—Ä–∞—Å—Å–≤–µ—Ç|–∑–∞–∫–∞—Ç)\s*([^,.!?]{0,50})",
        ]

        for pattern in atmosphere_patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                content = match.group(0).strip()
                if len(content) >= self.min_description_length:
                    descriptions.append(
                        {
                            "content": content,
                            "context": sentence,
                            "type": DescriptionType.ATMOSPHERE.value,
                            "confidence_score": 0.7,
                            "entities_mentioned": (
                                match.group(1) if match.lastindex >= 1 else content
                            ),
                            "text_position_start": match.start(),
                            "text_position_end": match.end(),
                            "position": position,
                        }
                    )

        return descriptions

    def _get_extended_context(self, sentence: str, entity: str, full_text: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—É—â–Ω–æ—Å—Ç–∏."""
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π –≤–æ–∫—Ä—É–≥ —Å—É—â–Ω–æ—Å—Ç–∏
        entity_pos = sentence.find(entity)
        if entity_pos == -1:
            return sentence

        # –ë–µ—Ä–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º, —ç—Ç–æ –∏ –µ—Å—Ç—å –Ω–∞—à –∫–æ–Ω—Ç–µ–∫—Å—Ç
        return sentence


class NatashaProcessor(BaseNLPProcessor):
    """Natasha –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏."""

    def __init__(self):
        super().__init__()
        self.processor_type = NLPProcessorType.NATASHA
        self.morph = None
        self.segmenter = None
        self.emb = None
        self.ner_tagger = None

    async def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Natasha –º–æ–¥–µ–ª–∏."""
        try:
            print("üîÑ Loading Natasha models...")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
            await self.load_settings()

            # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ Natasha
            from natasha import (
                Segmenter,
                MorphVocab,
                NewsEmbedding,
                NewsNERTagger,
                Doc,  # noqa: F401
            )

            self.segmenter = Segmenter()
            MorphVocab()
            self.emb = NewsEmbedding()
            self.ner_tagger = NewsNERTagger(self.emb)

            self.loaded = True
            print("‚úÖ Natasha models loaded successfully")

        except ImportError as e:
            print(f"‚ö†Ô∏è Natasha not available: {e}")
            self.loaded = False
        except Exception as e:
            print(f"‚ùå Error loading Natasha: {e}")
            self.loaded = False

    def extract_descriptions(
        self, text: str, chapter_id: str = None
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è Natasha."""
        if not self.is_available():
            return []

        print(f"üîç Natasha: extracting descriptions from text (length: {len(text)})")

        try:
            from natasha import Doc

            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = self._clean_text(text)

            # –ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é Natasha
            doc = Doc(cleaned_text)
            doc.segment(self.segmenter)
            doc.tag_ner(self.ner_tagger)

            descriptions = []

            for i, sent in enumerate(doc.sents):
                sentence = sent.text
                if len(sentence) < self.min_sentence_length:
                    continue

                # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                sentence_descriptions = self._analyze_sentence_natasha(
                    sentence, i, sent.spans
                )
                descriptions.extend(sentence_descriptions)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
            filtered_descriptions = self._filter_and_prioritize(descriptions)

            print(f"‚úÖ Natasha: extracted {len(filtered_descriptions)} descriptions")
            return filtered_descriptions

        except Exception as e:
            print(f"‚ùå Error in Natasha processing: {e}")
            return []

    def _analyze_sentence_natasha(
        self, sentence: str, position: int, spans
    ) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Natasha NER."""
        descriptions = []

        for span in spans:
            desc_type = None
            confidence = 0.5

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º shared type mapper
            desc_type = map_natasha_entity_to_description_type(span.type)

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º confidence –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞
            if span.type == "LOC":
                confidence = 0.8
            elif span.type == "PER":
                confidence = 0.7
            elif span.type == "ORG":
                confidence = 0.6

            if desc_type:
                entity_text = sentence[span.start : span.stop]
                descriptions.append(
                    {
                        "content": sentence,  # –ë–µ—Ä–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        "context": sentence,
                        "type": desc_type,
                        "confidence_score": confidence,
                        "entities_mentioned": entity_text,
                        "text_position_start": span.start,
                        "text_position_end": span.stop,
                        "position": position,
                    }
                )

        return descriptions


class NLPProcessor:
    """–ì–ª–∞–≤–Ω—ã–π NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤."""

    def __init__(self):
        self.processors = {
            NLPProcessorType.SPACY: SpacyProcessor(),
            NLPProcessorType.NATASHA: NatashaProcessor(),
        }
        self.current_processor = None
        self.current_type = NLPProcessorType.SPACY  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

    async def initialize(
        self, processor_type: NLPProcessorType = None, model_name: str = None
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
        if processor_type:
            self.current_type = processor_type

        # NOTE: settings_manager removed (depended on orphaned AdminSettings)
        # Using defaults or parameters
        if not model_name:
            model_name = "ru_core_news_lg"

        self.current_processor = self.processors[self.current_type]

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        if self.current_type == NLPProcessorType.SPACY:
            await self.current_processor.load_model(model_name)
        else:
            await self.current_processor.load_model()

        print(f"‚úÖ NLP Processor initialized: {self.current_type.value}")

    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return self.current_processor and self.current_processor.is_available()

    def extract_descriptions_from_text(
        self, text: str, chapter_id: str = None
    ) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
        if not self.is_available():
            print("‚ùå NLP processor not available")
            return []

        return self.current_processor.extract_descriptions(text, chapter_id)

    async def switch_processor(
        self, processor_type: NLPProcessorType, model_name: str = None
    ):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç —Ç–∏–ø –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        if processor_type not in self.processors:
            raise ValueError(f"Unsupported processor type: {processor_type}")

        self.current_type = processor_type
        self.current_processor = self.processors[processor_type]

        if not self.current_processor.loaded:
            if processor_type == NLPProcessorType.SPACY:
                await self.current_processor.load_model(model_name)
            else:
                await self.current_processor.load_model()

        print(f"‚úÖ Switched to NLP processor: {processor_type.value}")

    def get_available_models(self) -> Dict[str, List[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        return {
            "spacy": ["ru_core_news_lg", "ru_core_news_md", "ru_core_news_sm"],
            "natasha": ["default"],
        }

    def get_current_processor_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ."""
        if not self.current_processor:
            return {"type": None, "loaded": False, "available": False}

        info = {
            "type": self.current_type.value,
            "loaded": self.current_processor.loaded,
            "available": self.current_processor.is_available(),
        }

        if self.current_type == NLPProcessorType.SPACY and hasattr(
            self.current_processor, "model_name"
        ):
            info["model"] = self.current_processor.model_name

        return info

    async def update_settings(self, settings: Dict[str, Any]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        print(f"üîß [NLP] Updating settings: {settings}")

        # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        processor_type = settings.get("processor_type")
        if processor_type:
            try:
                if processor_type == "spacy":
                    new_type = NLPProcessorType.SPACY
                elif processor_type == "natasha":
                    new_type = NLPProcessorType.NATASHA
                elif processor_type == "hybrid":
                    new_type = (
                        NLPProcessorType.SPACY
                    )  # Hybrid –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SpaCy –∫–∞–∫ –æ—Å–Ω–æ–≤—É
                else:
                    print(f"‚ö†Ô∏è Unknown processor type: {processor_type}")
                    return

                if new_type != self.current_type:
                    await self.switch_processor(new_type)

            except Exception as e:
                print(f"‚ùå Failed to switch processor: {e}")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ spaCy –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        spacy_model = settings.get("spacy_model")
        if (
            spacy_model
            and self.current_type == NLPProcessorType.SPACY
            and hasattr(self.current_processor, "model_name")
        ):
            if self.current_processor.model_name != spacy_model:
                try:
                    await self.initialize(NLPProcessorType.SPACY, spacy_model)
                    print(f"‚úÖ Switched to spaCy model: {spacy_model}")
                except Exception as e:
                    print(f"‚ùå Failed to switch spaCy model: {e}")

        print("‚úÖ [NLP] Settings updated successfully")


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
nlp_processor = NLPProcessor()


async def initialize_nlp_processor():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
    await nlp_processor.initialize()

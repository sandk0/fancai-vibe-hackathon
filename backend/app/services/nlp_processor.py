"""
NLP –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥.

–†–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º—É –∑–∞–¥–∞–Ω–∏—é:
- LOCATION: 75% (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) - –ª–æ–∫–∞—Ü–∏–∏, –∏–Ω—Ç–µ—Ä—å–µ—Ä—ã, —ç–∫—Å—Ç–µ—Ä—å–µ—Ä—ã, –ø—Ä–∏—Ä–æ–¥–∞
- CHARACTER: 60% - –ø–µ—Ä—Å–æ–Ω–∞–∂–∏, –≤–Ω–µ—à–Ω–æ—Å—Ç—å, –æ–¥–µ–∂–¥–∞, —ç–º–æ—Ü–∏–∏  
- ATMOSPHERE: 45% - –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞, –≤—Ä–µ–º—è —Å—É—Ç–æ–∫, –ø–æ–≥–æ–¥–∞, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
- OBJECT: 40% - –æ–±—ä–µ–∫—Ç—ã, –æ—Ä—É–∂–∏–µ, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
- ACTION: 30% (–Ω–∏–∑—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) - –¥–µ–π—Å—Ç–≤–∏—è, –±–∏—Ç–≤—ã, —Ü–µ—Ä–µ–º–æ–Ω–∏–∏, —Å–æ–±—ã—Ç–∏—è
"""

import spacy
import re
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

from ..models.description import DescriptionType


class NLPProcessor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –∫–Ω–∏–≥."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å —Ä—É—Å—Å–∫–æ–π –º–æ–¥–µ–ª—å—é spaCy (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)."""
        self.nlp = None
        self.loaded = False
        self._model_loading = False
    
    def _load_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy."""
        if self._model_loading:
            return  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
        
        self._model_loading = True
        try:
            print("üîÑ Loading spaCy model ru_core_news_lg...")
            self.nlp = spacy.load("ru_core_news_lg")
            self.loaded = True
            print("‚úÖ spaCy model loaded successfully")
        except OSError:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å spaCy –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. NLP —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
            self.nlp = None
            self.loaded = False
        finally:
            self._model_loading = False
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        if not self.loaded and not self._model_loading:
            self._load_model()
        return self.loaded and self.nlp is not None
    
    def extract_descriptions_from_text(self, text: str, chapter_id: str = None) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã.
        
        Args:
            text: –¢–µ–∫—Å—Ç –≥–ª–∞–≤—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            chapter_id: ID –≥–ª–∞–≤—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.is_available():
            return []
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = self._clean_text(text)
        
        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        doc = self.nlp(cleaned_text)
        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]
        
        descriptions = []
        
        for i, sentence in enumerate(sentences):
            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–π
            sentence_doc = self.nlp(sentence)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ–ø–∏—Å–∞–Ω–∏–π
            location_desc = self._extract_location_description(sentence_doc, sentence)
            if location_desc:
                descriptions.append({
                    "type": DescriptionType.LOCATION,
                    "content": sentence,
                    "context": self._get_context(sentences, i),
                    "confidence_score": location_desc["confidence"],
                    "position_in_chapter": i,
                    "word_count": len(sentence.split()),
                    "entities_mentioned": location_desc["entities"],
                    "priority_score": self._calculate_priority_score(DescriptionType.LOCATION, location_desc["confidence"], len(sentence))
                })
            
            character_desc = self._extract_character_description(sentence_doc, sentence)
            if character_desc:
                descriptions.append({
                    "type": DescriptionType.CHARACTER,
                    "content": sentence,
                    "context": self._get_context(sentences, i),
                    "confidence_score": character_desc["confidence"],
                    "position_in_chapter": i,
                    "word_count": len(sentence.split()),
                    "entities_mentioned": character_desc["entities"],
                    "priority_score": self._calculate_priority_score(DescriptionType.CHARACTER, character_desc["confidence"], len(sentence))
                })
            
            atmosphere_desc = self._extract_atmosphere_description(sentence_doc, sentence)
            if atmosphere_desc:
                descriptions.append({
                    "type": DescriptionType.ATMOSPHERE,
                    "content": sentence,
                    "context": self._get_context(sentences, i),
                    "confidence_score": atmosphere_desc["confidence"],
                    "position_in_chapter": i,
                    "word_count": len(sentence.split()),
                    "entities_mentioned": atmosphere_desc["entities"],
                    "priority_score": self._calculate_priority_score(DescriptionType.ATMOSPHERE, atmosphere_desc["confidence"], len(sentence))
                })
            
            object_desc = self._extract_object_description(sentence_doc, sentence)
            if object_desc:
                descriptions.append({
                    "type": DescriptionType.OBJECT,
                    "content": sentence,
                    "context": self._get_context(sentences, i),
                    "confidence_score": object_desc["confidence"],
                    "position_in_chapter": i,
                    "word_count": len(sentence.split()),
                    "entities_mentioned": object_desc["entities"],
                    "priority_score": self._calculate_priority_score(DescriptionType.OBJECT, object_desc["confidence"], len(sentence))
                })
            
            action_desc = self._extract_action_description(sentence_doc, sentence)
            if action_desc:
                descriptions.append({
                    "type": DescriptionType.ACTION,
                    "content": sentence,
                    "context": self._get_context(sentences, i),
                    "confidence_score": action_desc["confidence"],
                    "position_in_chapter": i,
                    "word_count": len(sentence.split()),
                    "entities_mentioned": action_desc["entities"],
                    "priority_score": self._calculate_priority_score(DescriptionType.ACTION, action_desc["confidence"], len(sentence))
                })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç = –≤—ã—à–µ –≤ —Å–ø–∏—Å–∫–µ)
        descriptions.sort(key=lambda x: x["priority_score"], reverse=True)
        
        return descriptions
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s\.\!\?\,\;\:\-\‚Äî\¬´\¬ª\(\)\[\]]+', '', text)
        return text.strip()
    
    def _get_context(self, sentences: List[str], position: int, context_size: int = 1) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        start = max(0, position - context_size)
        end = min(len(sentences), position + context_size + 1)
        return " ".join(sentences[start:end])
    
    def _calculate_priority_score(self, desc_type: DescriptionType, confidence: float, text_length: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Å—á–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è."""
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –ø–æ —Ç–∏–ø–∞–º (–∏–∑ –¢–ó)
        type_priorities = {
            DescriptionType.LOCATION: 75,
            DescriptionType.CHARACTER: 60,
            DescriptionType.ATMOSPHERE: 45,
            DescriptionType.OBJECT: 40,
            DescriptionType.ACTION: 30
        }
        
        base_priority = type_priorities.get(desc_type, 30)
        confidence_bonus = confidence * 20  # 0-20 points
        
        # –ë–æ–Ω—É—Å –∑–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É (50-400 —Å–∏–º–≤–æ–ª–æ–≤)
        length_bonus = 0
        if 50 <= text_length <= 400:
            length_bonus = 10
        elif text_length < 50:
            length_bonus = max(0, text_length - 20) / 5
        else:
            length_bonus = max(0, 10 - (text_length - 400) / 100)
        
        return min(100.0, base_priority + confidence_bonus + length_bonus)
    
    def _extract_location_description(self, doc, sentence: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –ª–æ–∫–∞—Ü–∏–π."""
        location_keywords = [
            '–¥–æ–º', '–∑–∞–º–æ–∫', '–∫–æ–º–Ω–∞—Ç–∞', '–∑–∞–ª', '–¥–≤–æ—Ä–µ—Ü', '—Ö—Ä–∞–º', '—Ü–µ—Ä–∫–æ–≤—å',
            '–ª–µ—Å', '–ø–æ–ª–µ', '–≥–æ—Ä–∞', '—Ä–µ–∫–∞', '–æ–∑–µ—Ä–æ', '–º–æ—Ä–µ', '—Å–∞–¥', '–ø–∞—Ä–∫',
            '–≥–æ—Ä–æ–¥', '–¥–µ—Ä–µ–≤–Ω—è', '—É–ª–∏—Ü–∞', '–ø–ª–æ—â–∞–¥—å', '—Ä—ã–Ω–æ–∫', '—Ç–∞–≤–µ—Ä–Ω–∞',
            '–º–æ—Å—Ç', '–±–∞—à–Ω—è', '—Å—Ç–µ–Ω–∞', '–≤–æ—Ä–æ—Ç–∞', '–¥–æ—Ä–æ–≥–∞', '—Ç—Ä–æ–ø–∞'
        ]
        
        preposition_location_patterns = [
            '–≤ –¥–æ–º–µ', '–≤ –∑–∞–º–∫–µ', '–≤ –∫–æ–º–Ω–∞—Ç–µ', '–≤ –∑–∞–ª–µ', '–≤ –ª–µ—Å—É', '–≤ —Å–∞–¥—É',
            '–Ω–∞ –ø–ª–æ—â–∞–¥–∏', '–Ω–∞ —É–ª–∏—Ü–µ', '–Ω–∞ –ø–æ–ª–µ', '–Ω–∞ –≥–æ—Ä–µ', '—É —Ä–µ–∫–∏', '—É –æ–∑–µ—Ä–∞'
        ]
        
        confidence = 0.0
        entities = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ª–æ–∫–∞—Ü–∏–π
        text_lower = sentence.lower()
        for keyword in location_keywords:
            if keyword in text_lower:
                confidence += 0.3
                entities.append(keyword)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –ø—Ä–µ–¥–ª–æ–≥–∞–º–∏
        for pattern in preposition_location_patterns:
            if pattern in text_lower:
                confidence += 0.5
                entities.append(pattern)
        
        # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        for ent in doc.ents:
            if ent.label_ in ['LOC', 'GPE']:  # –õ–æ–∫–∞—Ü–∏–∏ –∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã
                confidence += 0.4
                entities.append(ent.text)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –º–µ—Å—Ç–∞
        descriptive_adjectives = [
            '—Å—Ç–∞—Ä—ã–π', '–¥—Ä–µ–≤–Ω–∏–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª–µ–Ω—å–∫–∏–π', '–≤—ã—Å–æ–∫–∏–π', '–Ω–∏–∑–∫–∏–π',
            '—Ç—ë–º–Ω—ã–π', '—Å–≤–µ—Ç–ª—ã–π', '—à–∏—Ä–æ–∫–∏–π', '—É–∑–∫–∏–π', '–¥–ª–∏–Ω–Ω—ã–π', '–∫–æ—Ä–æ—Ç–∫–∏–π'
        ]
        
        for token in doc:
            if token.pos_ == 'ADJ' and token.text.lower() in descriptive_adjectives:
                confidence += 0.2
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ª–æ–∫–∞—Ü–∏–π
        if confidence >= 0.4:
            return {
                "confidence": min(1.0, confidence),
                "entities": list(set(entities))
            }
        
        return None
    
    def _extract_character_description(self, doc, sentence: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π."""
        character_keywords = [
            '—á–µ–ª–æ–≤–µ–∫', '–º—É–∂—á–∏–Ω–∞', '–∂–µ–Ω—â–∏–Ω–∞', '–¥–µ–≤—É—à–∫–∞', '–ø–∞—Ä–µ–Ω—å', '—Å—Ç–∞—Ä–∏–∫', '—Å—Ç–∞—Ä—É—Ö–∞',
            '—Ä—ã—Ü–∞—Ä—å', '–≤–æ–∏–Ω', '–º–∞–≥', '–≤–æ–ª—à–µ–±–Ω–∏–∫', '–∫–æ—Ä–æ–ª—å', '–∫–æ—Ä–æ–ª–µ–≤–∞', '–ø—Ä–∏–Ω—Ü', '–ø—Ä–∏–Ω—Ü–µ—Å—Å–∞',
            '–≥–ª–∞–∑–∞', '–≤–æ–ª–æ—Å—ã', '–ª–∏—Ü–æ', '—Ä—É–∫–∏', '–æ–¥–µ–∂–¥–∞', '–ø–ª–∞—Ç—å–µ', '—Ä—É–±–∞—à–∫–∞', '–ø–ª–∞—â'
        ]
        
        appearance_adjectives = [
            '–∫—Ä–∞—Å–∏–≤—ã–π', '–∫—Ä–∞—Å–∏–≤–∞—è', '–º–æ–ª–æ–¥–æ–π', '–º–æ–ª–æ–¥–∞—è', '—Å—Ç–∞—Ä—ã–π', '—Å—Ç–∞—Ä–∞—è',
            '–≤—ã—Å–æ–∫–∏–π', '–≤—ã—Å–æ–∫–∞—è', '–Ω–∏–∑–∫–∏–π', '–Ω–∏–∑–∫–∞—è', '—Å–∏–ª—å–Ω—ã–π', '—Å–∏–ª—å–Ω–∞—è',
            '—Å–≤–µ—Ç–ª—ã–π', '—Ç—ë–º–Ω—ã–π', '—Ä—ã–∂–∏–π', '–±–ª–æ–Ω–¥–∏–Ω', '–±—Ä—é–Ω–µ—Ç'
        ]
        
        confidence = 0.0
        entities = []
        
        text_lower = sentence.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        for keyword in character_keywords:
            if keyword in text_lower:
                confidence += 0.4
                entities.append(keyword)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–Ω–µ—à–Ω–æ—Å—Ç–∏
        for adj in appearance_adjectives:
            if adj in text_lower:
                confidence += 0.3
        
        # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–ª—é–¥–∏)
        for ent in doc.ents:
            if ent.label_ in ['PER', 'PERSON']:
                confidence += 0.5
                entities.append(ent.text)
        
        # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –≤–Ω–µ—à–Ω–æ—Å—Ç–∏
        for token in doc:
            if token.pos_ == 'ADJ' and any(person_word in sentence.lower() for person_word in ['—á–µ–ª–æ–≤–µ–∫', '–º—É–∂—á–∏–Ω–∞', '–∂–µ–Ω—â–∏–Ω–∞']):
                confidence += 0.2
        
        if confidence >= 0.4:
            return {
                "confidence": min(1.0, confidence),
                "entities": list(set(entities))
            }
        
        return None
    
    def _extract_atmosphere_description(self, doc, sentence: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã."""
        atmosphere_keywords = [
            '—Ç—É–º–∞–Ω', '–¥–æ–∂–¥—å', '—Å–Ω–µ–≥', '–≤–µ—Ç–µ—Ä', '—Å–æ–ª–Ω—Ü–µ', '–ª—É–Ω–∞', '–∑–≤—ë–∑–¥—ã',
            '—É—Ç—Ä–æ', '–¥–µ–Ω—å', '–≤–µ—á–µ—Ä', '–Ω–æ—á—å', '—Ä–∞—Å—Å–≤–µ—Ç', '–∑–∞–∫–∞—Ç',
            '—Ç–∏—à–∏–Ω–∞', '—à—É–º', '–∫—Ä–∏–∫–∏', '–∑–≤—É–∫–∏', '–º—Ä–∞–∫', '—Å–≤–µ—Ç',
            '—Ö–æ–ª–æ–¥', '—Ç–µ–ø–ª–æ', '–∂–∞—Ä–∞', '–ø—Ä–æ—Ö–ª–∞–¥–∞'
        ]
        
        mood_adjectives = [
            '–º—Ä–∞—á–Ω—ã–π', '–≤–µ—Å—ë–ª—ã–π', '–≥—Ä—É—Å—Ç–Ω—ã–π', '—Ç–∞–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∑–ª–æ–≤–µ—â–∏–π',
            '—Å–ø–æ–∫–æ–π–Ω—ã–π', '—Ç—Ä–µ–≤–æ–∂–Ω—ã–π', '–ø–µ—á–∞–ª—å–Ω—ã–π', '—Ä–∞–¥–æ—Å—Ç–Ω—ã–π', '—É–≥—Ä—é–º—ã–π'
        ]
        
        confidence = 0.0
        entities = []
        
        text_lower = sentence.lower()
        
        for keyword in atmosphere_keywords:
            if keyword in text_lower:
                confidence += 0.4
                entities.append(keyword)
        
        for adj in mood_adjectives:
            if adj in text_lower:
                confidence += 0.3
                entities.append(adj)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —É–∫–∞–∑–∞—Ç–µ–ª–µ–π
        time_patterns = ['–±—ã–ª–æ —É—Ç—Ä–æ–º', '–±—ã–ª –¥–µ–Ω—å', '–Ω–∞—Å—Ç—É–ø–∏–ª –≤–µ—á–µ—Ä', '–ø—Ä–∏—à–ª–∞ –Ω–æ—á—å']
        for pattern in time_patterns:
            if pattern in text_lower:
                confidence += 0.5
        
        if confidence >= 0.3:
            return {
                "confidence": min(1.0, confidence),
                "entities": list(set(entities))
            }
        
        return None
    
    def _extract_object_description(self, doc, sentence: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤."""
        object_keywords = [
            '–º–µ—á', '–∫–∏–Ω–∂–∞–ª', '–ª—É–∫', '—Å—Ç—Ä–µ–ª–∞', '—â–∏—Ç', '–¥–æ—Å–ø–µ—Ö–∏', '—à–ª–µ–º',
            '–∫–æ–ª—å—Ü–æ', '–∞–º—É–ª–µ—Ç', '–æ–∂–µ—Ä–µ–ª—å–µ', '–ø–æ—Å–æ—Ö', '–∂–µ–∑–ª', '–∫–Ω–∏–≥–∞',
            '—Å—Ç–æ–ª', '—Å—Ç—É–ª', '–∫—Ä–æ–≤–∞—Ç—å', '—à–∫–∞—Ñ', '—Å—É–Ω–¥—É–∫', '–∑–µ—Ä–∫–∞–ª–æ',
            '–ª–æ—à–∞–¥—å', '–ø–æ–≤–æ–∑–∫–∞', '–∫–æ—Ä–∞–±–ª—å', '–ª–æ–¥–∫–∞'
        ]
        
        material_adjectives = [
            '–∑–æ–ª–æ—Ç–æ–π', '—Å–µ—Ä–µ–±—Ä—è–Ω—ã–π', '–º–µ–¥–Ω—ã–π', '–∂–µ–ª–µ–∑–Ω—ã–π', '—Å—Ç–∞–ª—å–Ω–æ–π',
            '–¥–µ—Ä–µ–≤—è–Ω–Ω—ã–π', '–∫–∞–º–µ–Ω–Ω—ã–π', '–∫–æ–∂–∞–Ω—ã–π', '—à—ë–ª–∫–æ–≤—ã–π', '–±–∞—Ä—Ö–∞—Ç–Ω—ã–π'
        ]
        
        confidence = 0.0
        entities = []
        
        text_lower = sentence.lower()
        
        for keyword in object_keywords:
            if keyword in text_lower:
                confidence += 0.4
                entities.append(keyword)
        
        for adj in material_adjectives:
            if adj in text_lower:
                confidence += 0.3
        
        # –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
        for token in doc:
            if token.pos_ == 'NOUN' and len(token.text) > 3:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —á–µ–ª–æ–≤–µ–∫–æ–º –∏–ª–∏ –º–µ—Å—Ç–æ–º
                if not any(person_word in token.text.lower() for person_word in ['—á–µ–ª–æ–≤–µ–∫', '–ª—é–¥']):
                    confidence += 0.1
        
        if confidence >= 0.4:
            return {
                "confidence": min(1.0, confidence),
                "entities": list(set(entities))
            }
        
        return None
    
    def _extract_action_description(self, doc, sentence: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π."""
        action_keywords = [
            '–±–∏—Ç–≤–∞', '—Å—Ä–∞–∂–µ–Ω–∏–µ', '–±–æ–π', '–≤–æ–π–Ω–∞', '–¥—Ä–∞–∫–∞',
            '—Ü–µ—Ä–µ–º–æ–Ω–∏—è', '—Ä–∏—Ç—É–∞–ª', '–ø—Ä–∞–∑–¥–Ω–∏–∫', '—Å–≤–∞–¥—å–±–∞', '–ø–æ—Ö–æ—Ä–æ–Ω—ã',
            '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ', '–ø–æ—Ö–æ–¥', '–ø–æ–±–µ–≥', '–ø–æ–≥–æ–Ω—è', '–æ—Ö–æ—Ç–∞'
        ]
        
        action_verbs = [
            '—Å—Ä–∞–∂–∞—Ç—å—Å—è', '–±–∏—Ç—å—Å—è', '–≤–æ–µ–≤–∞—Ç—å', '–¥—Ä–∞—Ç—å—Å—è',
            '–∏–¥—Ç–∏', '–µ—Ö–∞—Ç—å', '–ª–µ—Ç–µ—Ç—å', '–±–µ–∂–∞—Ç—å', '–ø—Ä—ã–≥–∞—Ç—å',
            '–≥–æ–≤–æ—Ä–∏—Ç—å', '–∫—Ä–∏—á–∞—Ç—å', '—à–µ–ø—Ç–∞—Ç—å', '–ø–µ—Ç—å'
        ]
        
        confidence = 0.0
        entities = []
        
        text_lower = sentence.lower()
        
        for keyword in action_keywords:
            if keyword in text_lower:
                confidence += 0.5
                entities.append(keyword)
        
        for verb in action_verbs:
            if verb in text_lower:
                confidence += 0.2
        
        # –ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≥–æ–ª–æ–≤ –¥–µ–π—Å—Ç–≤–∏—è
        action_verb_count = 0
        for token in doc:
            if token.pos_ == 'VERB':
                action_verb_count += 1
        
        if action_verb_count >= 2:
            confidence += 0.3
        
        if confidence >= 0.4:
            return {
                "confidence": min(1.0, confidence),
                "entities": list(set(entities))
            }
        
        return None


async def process_book_descriptions(book_id: str, db) -> dict:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∫–Ω–∏–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –ë–î.
    
    Args:
        book_id: ID –∫–Ω–∏–≥–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    from uuid import UUID
    from sqlalchemy import select
    from sqlalchemy.orm import selectinload
    from ..models.book import Book
    from ..models.description import Description
    
    descriptions_count = 0
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–Ω–∏–≥—É —Å –≥–ª–∞–≤–∞–º–∏
        result = await db.execute(
            select(Book)
            .options(selectinload(Book.chapters))
            .where(Book.id == UUID(book_id))
        )
        book = result.scalar_one_or_none()
        
        if not book:
            return {"error": "Book not found", "total_descriptions": 0}
            
        print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {book.title}")
        print(f"   –ì–ª–∞–≤: {len(book.chapters)}")
        
        for chapter in book.chapters:
            if chapter.is_description_parsed:
                print(f"   ‚è≠Ô∏è  –ì–ª–∞–≤–∞ {chapter.chapter_number} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
                continue
                
            print(f"   üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥–ª–∞–≤—É {chapter.chapter_number}: {chapter.title}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é NLP
            try:
                descriptions_data = nlp_processor.extract_descriptions_from_text(
                    text=chapter.content,
                    chapter_id=str(chapter.id)
                )
                
                print(f"      –ù–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π: {len(descriptions_data)}")
                
                # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç—ã –æ–ø–∏—Å–∞–Ω–∏–π
                for desc_data in descriptions_data:
                    description = Description(
                        chapter_id=chapter.id,
                        type=desc_data["type"],
                        content=desc_data["content"],
                        context=desc_data.get("context", ""),
                        confidence_score=desc_data["confidence_score"],
                        priority_score=desc_data["priority_score"],
                        position_in_chapter=desc_data.get("position_in_chapter", 0),
                        word_count=len(desc_data["content"].split()),
                        entities_mentioned=", ".join(desc_data.get("entities_mentioned", [])),
                        emotional_tone=desc_data.get("emotional_tone", "neutral"),
                        complexity_level=desc_data.get("complexity_level", "medium"),
                        is_suitable_for_generation=desc_data.get("confidence_score", 0) >= 0.3
                    )
                    db.add(description)
                    descriptions_count += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≥–ª–∞–≤—ã
                chapter.is_description_parsed = True
                chapter.descriptions_found = len(descriptions_data)
                chapter.parsing_progress = 100.0
                
                print(f"      ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(descriptions_data)} –æ–ø–∏—Å–∞–Ω–∏–π")
                
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
                continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        await db.commit()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–Ω–∏–≥–∏
        book.is_parsed = True
        await db.commit()
        
        return {
            "success": True,
            "total_descriptions": descriptions_count,
            "book_title": book.title
        }
        
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–Ω–∏–≥–∏ {book_id}: {str(e)}")
        await db.rollback()
        return {
            "error": str(e),
            "total_descriptions": 0
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
nlp_processor = NLPProcessor()
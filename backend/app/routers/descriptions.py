"""
API —Ä–æ—É—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –≤ –∫–Ω–∏–≥–∞—Ö BookReader AI.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç endpoints –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è–º–∏:
- –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –≥–ª–∞–≤—ã
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Å –ø–æ–º–æ—â—å—é LLM (LangExtract)
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—è–º
"""

import asyncio

from fastapi import APIRouter, HTTPException, Depends, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from typing import Dict, Any, List
from uuid import UUID
from datetime import datetime

from ..core.database import get_database_session
from ..core.auth import get_current_active_user
from ..core.cache import cache_manager
from ..core.exceptions import (
    ChapterNotFoundException,
    BookNotFoundException,
)
from ..services.book import book_service
from ..services.langextract_processor import langextract_processor
from ..models.user import User
from ..models.description import Description, DescriptionType
from ..models.chapter import Chapter
from ..schemas.responses.descriptions import (
    ChapterDescriptionsResponse,
    ChapterMinimalInfo,
    NLPAnalysisResult,
    BatchDescriptionsRequest,
    BatchDescriptionsResponse,
    ChapterDescriptionsResult,
)
from ..schemas.responses import DescriptionResponse, DescriptionListResponse

from loguru import logger


router = APIRouter()


@router.get(
    "/{book_id}/chapters/{chapter_number}/descriptions",
    response_model=ChapterDescriptionsResponse,
    summary="Get descriptions from chapter",
    description="Returns all extracted descriptions from a specific chapter"
)
async def get_chapter_descriptions(
    book_id: UUID,
    chapter_number: int,
    extract_new: bool = False,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_database_session),
) -> ChapterDescriptionsResponse:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥–ª–∞–≤—ã –∫–Ω–∏–≥–∏.

    –ú–æ–∂–µ—Ç –ª–∏–±–æ –≤–µ—Ä–Ω—É—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ –ë–î, –ª–∏–±–æ
    –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–æ–≤—ã–π LLM –∞–Ω–∞–ª–∏–∑ –≥–ª–∞–≤—ã (extract_new=True).

    Args:
        book_id: ID –∫–Ω–∏–≥–∏
        chapter_number: –ù–æ–º–µ—Ä –≥–ª–∞–≤—ã (1-indexed)
        extract_new: –ò–∑–≤–ª–µ—á—å –Ω–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (–ø–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å –≥–ª–∞–≤—É)
        current_user: –¢–µ–∫—É—â–∏–π –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

    Returns:
        ChapterDescriptionsResponse: –ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤—ã —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
    """
    # –ü–æ–ª—É—á–∞–µ–º –∫–Ω–∏–≥—É
    book = await book_service.get_book_by_id(
        db=db, book_id=book_id, user_id=current_user.id
    )

    if not book:
        raise BookNotFoundException(book_id)

    # –ò—â–µ–º –≥–ª–∞–≤—É
    chapter = None
    for c in book.chapters:
        if c.chapter_number == chapter_number:
            chapter = c
            break

    if not chapter:
        raise ChapterNotFoundException(chapter_number, book_id)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–µ –ø–∞—Ä—Å–∏–º –∏—Ö)
    # P1.1 OPTIMIZATION: Use cached method from Chapter model
    is_service_page = chapter.check_is_service_page()

    # Cache the result if not already cached
    if chapter.is_service_page is None:
        chapter.is_service_page = is_service_page
        await db.commit()
        logger.debug(f"üìù Cached is_service_page={is_service_page} for chapter {chapter.id}")

    if is_service_page:
        # Return empty result for service pages
        chapter_info = ChapterMinimalInfo(
            id=chapter.id,
            number=chapter.chapter_number,
            title=chapter.title or f"–ì–ª–∞–≤–∞ {chapter.chapter_number}",
            word_count=chapter.word_count,
        )
        return ChapterDescriptionsResponse(
            chapter_info=chapter_info,
            nlp_analysis=NLPAnalysisResult(
                total_descriptions=0,
                by_type={},
                descriptions=[],
            ),
        )

    # –ï—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –Ω–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    if extract_new:
        if not langextract_processor.is_available():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="LLM processor unavailable. Check GOOGLE_API_KEY.",
            )

        # DISTRIBUTED LOCK: Prevent parallel LLM extraction for same chapter
        # This prevents duplicate API calls and data corruption
        lock_key = f"llm_extract_lock:chapter:{chapter.id}"
        lock_acquired = await cache_manager.acquire_lock(
            lock_key, ttl=120  # 2 minutes timeout for LLM extraction
        )

        if not lock_acquired:
            logger.info(
                f"üîí LLM extraction already in progress for chapter {chapter.id}, "
                "returning existing descriptions"
            )
            # Return existing descriptions without re-extraction
            # (another request is already processing this chapter)
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail={
                    "message": "Description extraction already in progress for this chapter",
                    "retry_after_seconds": 15,
                    "chapter_id": str(chapter.id),
                }
            )

        try:
            logger.info(f"üîÑ Starting LLM extraction for chapter {chapter.id}")

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–π –≥–ª–∞–≤—ã
            old_descriptions = (
                (
                    await db.execute(
                        select(Description).where(Description.chapter_id == chapter.id)
                    )
                )
                .scalars()
                .all()
            )

            for old_desc in old_descriptions:
                await db.delete(old_desc)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ LLM
            # TIMEOUT PROTECTION (P0.3): Prevent infinite hangs on LLM API issues
            LLM_EXTRACTION_TIMEOUT = 30.0  # seconds (API response + processing)
            try:
                result = await asyncio.wait_for(
                    langextract_processor.extract_descriptions(chapter.content),
                    timeout=LLM_EXTRACTION_TIMEOUT
                )
            except asyncio.TimeoutError:
                logger.error(
                    f"‚è±Ô∏è LLM extraction timeout ({LLM_EXTRACTION_TIMEOUT}s) for "
                    f"chapter {chapter.id}"
                )
                raise HTTPException(
                    status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                    detail={
                        "message": "Description extraction timed out. Please try again.",
                        "chapter_id": str(chapter.id),
                        "timeout_seconds": LLM_EXTRACTION_TIMEOUT,
                    }
                )

            # ProcessingResult has 'descriptions' list directly, not 'success' attr
            descriptions_data = result.descriptions if result.descriptions else []

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤ –±–∞–∑–µ
            position = 0
            for desc_data in descriptions_data:
                desc_dict = desc_data.to_dict() if hasattr(desc_data, 'to_dict') else desc_data

                # Map string type to enum
                type_str = desc_dict.get("type", "location")
                try:
                    desc_type = DescriptionType(type_str)
                except ValueError:
                    desc_type = DescriptionType.LOCATION

                new_description = Description(
                    chapter_id=chapter.id,
                    type=desc_type,
                    content=desc_dict.get("content", ""),
                    confidence_score=desc_dict.get("confidence_score", 0.8),
                    priority_score=desc_dict.get("priority_score", 0.5),
                    entities_mentioned=",".join(desc_dict.get("entities_mentioned", [])),
                    position_in_chapter=position,
                    word_count=desc_dict.get("word_count", len(desc_dict.get("content", "").split())),
                )
                position += 1
                db.add(new_description)

            # Update chapter stats
            chapter.descriptions_found = len(descriptions_data)
            chapter.is_description_parsed = True
            chapter.parsed_at = datetime.utcnow()

            await db.commit()
            await db.refresh(chapter)

            logger.info(
                f"‚úÖ LLM extraction complete for chapter {chapter.id}: "
                f"{len(descriptions_data)} descriptions"
            )

            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–π –≥–ª–∞–≤—ã (–Ω–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è)
            invalidate_key = f"descriptions:book:{book_id}:chapter:{chapter_number}"
            await cache_manager.delete(invalidate_key)
            logger.debug(f"üóëÔ∏è Invalidated cache: {invalidate_key}")

        finally:
            # Always release the lock
            await cache_manager.release_lock(lock_key)

    # Redis cache key –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π –≥–ª–∞–≤—ã
    cache_key = f"descriptions:book:{book_id}:chapter:{chapter_number}"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï extract_new)
    if not extract_new:
        cached_response = await cache_manager.get(cache_key)
        if cached_response:
            logger.debug(f"üéØ Redis cache HIT for chapter {chapter_number} descriptions")
            # Reconstruct response from cached data
            try:
                return ChapterDescriptionsResponse(**cached_response)
            except Exception as e:
                logger.warning(f"Failed to parse cached descriptions: {e}")
                # Continue to DB fetch if cache is corrupted

    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–π –≥–ª–∞–≤—ã
    descriptions_result = await db.execute(
        select(Description)
        .where(Description.chapter_id == chapter.id)
        .order_by(Description.position_in_chapter)
    )
    descriptions = descriptions_result.scalars().all()

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    chapter_info = ChapterMinimalInfo(
        id=chapter.id,
        number=chapter.chapter_number,  # Field name is 'number', not 'chapter_number'
        title=chapter.title or f"–ì–ª–∞–≤–∞ {chapter.chapter_number}",
        word_count=chapter.word_count,
    )

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
    by_type: Dict[str, int] = {}
    desc_responses: List[DescriptionResponse] = []

    for desc in descriptions:
        type_value = desc.type.value if desc.type else "location"
        by_type[type_value] = by_type.get(type_value, 0) + 1

        desc_responses.append(DescriptionResponse(
            id=desc.id,
            chapter_id=desc.chapter_id,
            type=desc.type,
            content=desc.content,
            context=desc.context or "",
            confidence_score=desc.confidence_score,
            priority_score=desc.priority_score,
            position_in_chapter=desc.position_in_chapter,
            word_count=desc.word_count,
            is_suitable_for_generation=desc.is_suitable_for_generation,
            image_generated=desc.image_generated,
            entities_mentioned=desc.entities_mentioned or "",
            created_at=desc.created_at,
            updated_at=desc.updated_at,
        ))

    analysis_result = NLPAnalysisResult(
        total_descriptions=len(descriptions),
        by_type=by_type,
        descriptions=desc_responses,
    )

    response = ChapterDescriptionsResponse(
        chapter_info=chapter_info,  # Fixed: was 'chapter'
        nlp_analysis=analysis_result,  # Fixed: was 'analysis'
    )

    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (TTL 1 hour)
    # –ö—ç—à–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è (–Ω–µ –∫—ç—à–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
    if len(descriptions) > 0:
        try:
            await cache_manager.set(
                cache_key,
                response.model_dump(mode='json'),
                ttl=3600  # 1 hour
            )
            logger.debug(f"üíæ Cached {len(descriptions)} descriptions for chapter {chapter_number}")
        except Exception as e:
            logger.warning(f"Failed to cache descriptions: {e}")

    return response


@router.get(
    "/descriptions/{description_id}",
    response_model=DescriptionResponse,
    summary="Get description by ID",
    description="Returns a specific description by its ID"
)
async def get_description(
    description_id: UUID,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_database_session),
) -> DescriptionResponse:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ ID.

    Args:
        description_id: ID –æ–ø–∏—Å–∞–Ω–∏—è
        current_user: –¢–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

    Returns:
        DescriptionResponse: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–ø–∏—Å–∞–Ω–∏–∏
    """
    from ..models.book import Book

    # Get description with access check
    result = await db.execute(
        select(Description)
        .join(Chapter, Description.chapter_id == Chapter.id)
        .join(Book, Chapter.book_id == Book.id)
        .where(Description.id == description_id)
        .where(Book.user_id == current_user.id)
    )
    description = result.scalar_one_or_none()

    if not description:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Description not found or access denied",
        )

    return DescriptionResponse(
        id=description.id,
        chapter_id=description.chapter_id,
        type=description.type,
        content=description.content,
        context=description.context or "",
        confidence_score=description.confidence_score,
        priority_score=description.priority_score,
        position_in_chapter=description.position_in_chapter,
        word_count=description.word_count,
        is_suitable_for_generation=description.is_suitable_for_generation,
        image_generated=description.image_generated,
        entities_mentioned=description.entities_mentioned or "",
        created_at=description.created_at,
        updated_at=description.updated_at,
    )


# ============================================================================
# BATCH API (Phase 3 - 2025-12-25)
# ============================================================================


async def _get_chapter_descriptions_internal(
    db: AsyncSession,
    book_id: UUID,
    chapter_number: int,
    chapters: list,
) -> ChapterDescriptionsResponse:
    """
    Internal helper to get descriptions for a single chapter.
    Used by both single and batch endpoints.

    Note: Does NOT trigger LLM extraction - only returns existing descriptions.
    """
    # Find chapter
    chapter = None
    for c in chapters:
        if c.chapter_number == chapter_number:
            chapter = c
            break

    if not chapter:
        raise ChapterNotFoundException(chapter_number, book_id)

    # P1.1 OPTIMIZATION: Use cached method from Chapter model
    is_service_page = chapter.check_is_service_page()

    chapter_info = ChapterMinimalInfo(
        id=chapter.id,
        number=chapter.chapter_number,
        title=chapter.title or f"–ì–ª–∞–≤–∞ {chapter.chapter_number}",
        word_count=chapter.word_count,
    )

    if is_service_page:
        return ChapterDescriptionsResponse(
            chapter_info=chapter_info,
            nlp_analysis=NLPAnalysisResult(
                total_descriptions=0,
                by_type={},
                descriptions=[],
            ),
        )

    # Get descriptions from DB
    descriptions_result = await db.execute(
        select(Description)
        .where(Description.chapter_id == chapter.id)
        .order_by(Description.position_in_chapter)
    )
    descriptions = descriptions_result.scalars().all()

    # Build response
    by_type: Dict[str, int] = {}
    desc_responses: List[DescriptionResponse] = []

    for desc in descriptions:
        type_value = desc.type.value if desc.type else "location"
        by_type[type_value] = by_type.get(type_value, 0) + 1

        desc_responses.append(DescriptionResponse(
            id=desc.id,
            chapter_id=desc.chapter_id,
            type=desc.type,
            content=desc.content,
            context=desc.context or "",
            confidence_score=desc.confidence_score,
            priority_score=desc.priority_score,
            position_in_chapter=desc.position_in_chapter,
            word_count=desc.word_count,
            is_suitable_for_generation=desc.is_suitable_for_generation,
            image_generated=desc.image_generated,
            entities_mentioned=desc.entities_mentioned or "",
            created_at=desc.created_at,
            updated_at=desc.updated_at,
        ))

    return ChapterDescriptionsResponse(
        chapter_info=chapter_info,
        nlp_analysis=NLPAnalysisResult(
            total_descriptions=len(descriptions),
            by_type=by_type,
            descriptions=desc_responses,
        ),
    )


@router.post(
    "/{book_id}/chapters/batch",
    response_model=BatchDescriptionsResponse,
    summary="Get descriptions for multiple chapters",
    description="Returns existing descriptions for multiple chapters in a single request. "
                "Does NOT trigger LLM extraction - use individual endpoint with extract_new=true for that."
)
async def get_batch_descriptions(
    book_id: UUID,
    request: BatchDescriptionsRequest,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_database_session),
) -> BatchDescriptionsResponse:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥–ª–∞–≤ –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º.

    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è prefetching - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è N –≥–ª–∞–≤ –∑–∞ –æ–¥–∏–Ω HTTP –∑–∞–ø—Ä–æ—Å
    –≤–º–µ—Å—Ç–æ N –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ù–ï –∑–∞–ø—É—Å–∫–∞–µ—Ç LLM extraction.

    OPTIMIZED (2025-12-25): Fixed N+1 query problem.
    Previously: N separate queries for N chapters.
    Now: 1 batch query for all chapters + in-memory grouping.

    Args:
        book_id: ID –∫–Ω–∏–≥–∏
        request: –°–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ –≥–ª–∞–≤
        current_user: –¢–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

    Returns:
        BatchDescriptionsResponse: –û–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö –≥–ª–∞–≤
    """
    logger.info(
        f"üì¶ Batch descriptions request: book={book_id}, "
        f"chapters={request.chapter_numbers}"
    )

    # Get book with access check
    book = await book_service.get_book_by_id(
        db=db, book_id=book_id, user_id=current_user.id
    )

    if not book:
        raise BookNotFoundException(book_id)

    # Build chapter lookup map
    chapters_by_number = {c.chapter_number: c for c in book.chapters}

    # Track chapters that need is_service_page cached
    chapters_to_cache: List[Chapter] = []

    # Phase 1: Check Redis cache for all chapters
    results: List[ChapterDescriptionsResult] = []
    chapters_to_fetch: List[int] = []  # Chapters not in cache
    chapter_ids_to_fetch: List[UUID] = []  # Their DB IDs

    for chapter_number in request.chapter_numbers:
        cache_key = f"descriptions:book:{book_id}:chapter:{chapter_number}"
        cached_response = await cache_manager.get(cache_key)

        if cached_response:
            logger.debug(f"üéØ Batch: Redis HIT for chapter {chapter_number}")
            try:
                chapter_data = ChapterDescriptionsResponse(**cached_response)
                results.append(ChapterDescriptionsResult(
                    chapter_number=chapter_number,
                    success=True,
                    data=chapter_data,
                ))
                continue
            except Exception:
                pass  # Fall through to DB fetch

        # Check if chapter exists
        chapter = chapters_by_number.get(chapter_number)
        if not chapter:
            results.append(ChapterDescriptionsResult(
                chapter_number=chapter_number,
                success=False,
                error=f"Chapter {chapter_number} not found",
            ))
            continue

        # P1.1 OPTIMIZATION: Use cached method from Chapter model
        is_service_page = chapter.check_is_service_page()

        # Track for batch caching
        if chapter.is_service_page is None:
            chapter.is_service_page = is_service_page
            chapters_to_cache.append(chapter)

        if is_service_page:
            # Return empty result for service pages
            chapter_info = ChapterMinimalInfo(
                id=chapter.id,
                number=chapter.chapter_number,
                title=chapter.title or f"–ì–ª–∞–≤–∞ {chapter.chapter_number}",
                word_count=chapter.word_count,
            )
            results.append(ChapterDescriptionsResult(
                chapter_number=chapter_number,
                success=True,
                data=ChapterDescriptionsResponse(
                    chapter_info=chapter_info,
                    nlp_analysis=NLPAnalysisResult(
                        total_descriptions=0,
                        by_type={},
                        descriptions=[],
                    ),
                ),
            ))
            continue

        # Need to fetch from DB
        chapters_to_fetch.append(chapter_number)
        chapter_ids_to_fetch.append(chapter.id)

    # Phase 2: BATCH LOAD all descriptions in ONE query (N+1 FIX)
    descriptions_by_chapter: Dict[UUID, List[Description]] = {}

    if chapter_ids_to_fetch:
        logger.debug(
            f"üóÑÔ∏è Batch: Loading descriptions for {len(chapter_ids_to_fetch)} chapters "
            "in single query"
        )
        descriptions_result = await db.execute(
            select(Description)
            .where(Description.chapter_id.in_(chapter_ids_to_fetch))
            .order_by(Description.chapter_id, Description.position_in_chapter)
        )
        all_descriptions = descriptions_result.scalars().all()

        # Group by chapter_id
        for desc in all_descriptions:
            if desc.chapter_id not in descriptions_by_chapter:
                descriptions_by_chapter[desc.chapter_id] = []
            descriptions_by_chapter[desc.chapter_id].append(desc)

        logger.debug(
            f"üóÑÔ∏è Batch: Loaded {len(all_descriptions)} descriptions for "
            f"{len(descriptions_by_chapter)} chapters"
        )

    # Phase 3: Build responses for fetched chapters
    for chapter_number in chapters_to_fetch:
        chapter = chapters_by_number[chapter_number]
        descriptions = descriptions_by_chapter.get(chapter.id, [])

        # Build chapter info
        chapter_info = ChapterMinimalInfo(
            id=chapter.id,
            number=chapter.chapter_number,
            title=chapter.title or f"–ì–ª–∞–≤–∞ {chapter.chapter_number}",
            word_count=chapter.word_count,
        )

        # Build descriptions response
        by_type: Dict[str, int] = {}
        desc_responses: List[DescriptionResponse] = []

        for desc in descriptions:
            type_value = desc.type.value if desc.type else "location"
            by_type[type_value] = by_type.get(type_value, 0) + 1

            desc_responses.append(DescriptionResponse(
                id=desc.id,
                chapter_id=desc.chapter_id,
                type=desc.type,
                content=desc.content,
                context=desc.context or "",
                confidence_score=desc.confidence_score,
                priority_score=desc.priority_score,
                position_in_chapter=desc.position_in_chapter,
                word_count=desc.word_count,
                is_suitable_for_generation=desc.is_suitable_for_generation,
                image_generated=desc.image_generated,
                entities_mentioned=desc.entities_mentioned or "",
                created_at=desc.created_at,
                updated_at=desc.updated_at,
            ))

        chapter_data = ChapterDescriptionsResponse(
            chapter_info=chapter_info,
            nlp_analysis=NLPAnalysisResult(
                total_descriptions=len(descriptions),
                by_type=by_type,
                descriptions=desc_responses,
            ),
        )

        results.append(ChapterDescriptionsResult(
            chapter_number=chapter_number,
            success=True,
            data=chapter_data,
        ))

        # Cache the result if non-empty
        if len(descriptions) > 0:
            try:
                cache_key = f"descriptions:book:{book_id}:chapter:{chapter_number}"
                await cache_manager.set(
                    cache_key,
                    chapter_data.model_dump(mode='json'),
                    ttl=3600
                )
            except Exception:
                pass  # Ignore cache errors

    # Sort results by original chapter order
    chapter_order = {num: idx for idx, num in enumerate(request.chapter_numbers)}
    results.sort(key=lambda r: chapter_order.get(r.chapter_number, 999))

    # Calculate totals
    total_descriptions = sum(
        r.data.nlp_analysis.total_descriptions
        for r in results
        if r.success and r.data
    )
    success_count = sum(1 for r in results if r.success)

    # P1.1: Batch commit is_service_page cache updates
    if chapters_to_cache:
        await db.commit()
        logger.debug(f"üìù Cached is_service_page for {len(chapters_to_cache)} chapters")

    logger.info(
        f"‚úÖ Batch complete: {success_count}/{len(request.chapter_numbers)} chapters, "
        f"{total_descriptions} total descriptions (optimized: 1 DB query)"
    )

    return BatchDescriptionsResponse(
        book_id=book_id,
        chapters=results,
        total_requested=len(request.chapter_numbers),
        total_success=success_count,
        total_descriptions=total_descriptions,
    )
